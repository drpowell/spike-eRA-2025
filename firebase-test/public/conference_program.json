[
    {
        "day": "Monday",
        "time": "08:30 – 16:00",
        "location": "N/A",
        "session_chair": "N/A",
        "title": "Registration Open | Plaza P6-8 Concourse",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Monday",
        "time": "09:00 – 10:30",
        "location": "Plaza P6",
        "session_chair": "N/A",
        "title": "Fundamentals of Accelerated Computing with CUDA Python",
        "authors": "Johan Barthelemy",
        "url": "https://conference.eresearch.edu.au/fundamentals-of-accelerated-computing-with-cuda-python/",
        "details": "Dr Johan Barthelemy1, Dr Wei Fang1, Dr Gabriel Noaje1\n1Nvidia, Australia\nAfter his PhD in Applied Mathematics at the University of Namur (Belgium), Dr. Johan Barthélemy joined the SMART Infrastructure Facility of the University of Wollongong (Australia) where he was a Senior Lecturer and the head of the Digital Living Lab researching AIoT solutions for smart cities and environmental monitoring in extreme conditions such as Antarctica. Passionate about applied AI and how to accelerate it, he is now part of the Strategic Researchers Engagement team at NVIDIA, helping scientists to build the next generation of AIs.\nDr. Wei Fang joined NVIDIA in 2021 as a Solutions Architect for High-Performance Computing (HPC) and Artificial Intelligence (AI). In this role, he actively engages with NVIDIA customers across the Asia Pacific South region, focusing on the Higher Education & Research and Supercomputing sectors. Dr. Fang is dedicated to promoting the adoption of NVIDIA's latest technologies to deliver large-scale HPC and AI solutions, driving innovation, and enhancing research productivity. Before his tenure at NVIDIA, Dr. Fang served as an HPC Specialist at Intersect Australia from 2013 to 2021, where he managed Intersect’s HPC services and delivered cutting-edge HPC and cloud solutions to Australian universities. He also worked as a Cloud Specialist at the National Computational Infrastructure (NCI) in 2013 and as a Satellite Ground-station Software Engineer at Geoscience Australia in 2012, where he developed scientific platforms and software solutions. Prior to that, he contributed to the private sector by developing software and algorithms for telecom network optimization. Dr. Fang earned his Ph.D. in Electrical Engineering from Nanyang Technological University, Singapore, in 2004.\nGabriel Noaje has more than 15 years of experience in accelerated computing solutions for High Performance Computing and Artificial Intelligence. He has performed a variety of roles both in the enterprise and public sector that allowed him to manage, design and work on state of the art HPC solutions across a broad range of industries and domains. In his current role at NVIDIA, Gabriel is spearheading the development of HPC business in Asia Pacific.\nPrior to joining NVIDIA, he was a Senior Solutions Architect with SGI and HPE where he has worked closely with major supercomputing centers in APAC. Previously, he was a Senior Computational Scientist at A*STAR Computational Resource Centre in Singapore (A*CRC) supporting users with deploying their applications on GPUs and large HPC systems. Gabriel holds a PhD in Computer Sciences from the University of Reims Champagne-Ardenne, France and a BSc and MSc in Computer Sciences from the Polytechnic University of Bucharest, Romania.\nThis workshop explores how to use Numba—the just-in-time, type-specializing Python function compiler—to accelerate Python programs to run on massively parallel NVIDIA GPUs. You will learn how to: Use Numba to compile CUDA kernels from NumPy universal functions (ufuncs); Use Numba to create and launch custom CUDA kernels; Apply key GPU memory management techniques.\nAt the conclusion of the workshop, you will be able to use Numba to compile and launch CUDA kernels to accelerate your Python applications on NVIDIA GPUs. You will have an understanding of the fundamental tools and techniques for GPU-accelerated Python applications with CUDA and Numba:\n• GPU-accelerate NumPy ufuncs with a few lines of code.\n• Configure code parallelization using the CUDA thread hierarchy.\n• Write custom CUDA device kernels for maximum performance and flexibility.\n• Use memory coalescing and on-device shared memory to increase CUDA kernel bandwidth.\nThe following topics and technologies will be covered:\n• CUDA Python with Numba;\n• CUDA programming general practices."
    },
    {
        "day": "Monday",
        "time": "09:00 – 10:30",
        "location": "Plaza P7",
        "session_chair": "N/A",
        "title": "Solving Data Management Challenges with Globus – from Zero to Hero in one day",
        "authors": "Alex Ip, Kyle Chard, Steele Cooke, Greg D’Arcy, Chris Myers",
        "url": "https://conference.eresearch.edu.au/solving-data-management-challenges-with-globus-from-zero-to-hero-in-one-day/",
        "details": "Mr Alex Ip1, Mr Greg D’Arcy1, Mr Chris Myers1, Mr Steele Cooke1,A/Prof Kyle Chard2\n1Aarnet Pty Ltd, Chatswood, Australia2University of Chicago/Argonne National Laboratory, USA\nKyle Chardis a Research Associate Professor in the Department of Computer Science at the University of Chicago. He also holds a joint appointment at Argonne National Laboratory. He co-leads the Globus Labs research group, which focuses on a broad range of research problems in data-intensive computing and research data management.\nSteele Cookeis a Software Developer and collaborates on multiple research projects across the sector, with a focus on developing and refining proof of concepts and taking solutions from initial design through to implementation.\nGreg D’Arcyhelps build and manage digital tools that enable researchers to work more effectively, engaging with teams to understand user requirements and translating those ideas into practical solutions.\nAlex Iphas worked across multiple sectors over several decades, including manufacturing, software development, data engineering, and eResearch infrastructure development, bringing this experience to diverse research domains.\nChris Myersspecialises in information security and eResearch infrastructure, focusing on cyber security standards, advanced networking, and emerging technologies, and has supported Globus and related technologies in Australia for almost 20 years.\nJoin us for a hands-on workshop designed for system administrators looking to harness the power of Globus for efficient research data transfer and automation.\nElevate your institution’s research computing efficiency and productivity with expert insights and peer networking opportunities. Each session will provide practical exercises and an open discussion exploring current use cases.\nThe workshop will be delivered as two self-contained 3-hour sessions. You can attend both sessions or just choose the one that’s right for you.\nMorning Session: Introduction to Globus\nThe morning session assumes no prior knowledge of Globus.\nAfternoon Session: Advanced Globus Topics\nThe afternoon session assumes familiarity with Globus concepts and endpoint configuration (as covered in the morning session)."
    },
    {
        "day": "Monday",
        "time": "09:00 – 10:30",
        "location": "Plaza P8",
        "session_chair": "N/A",
        "title": "IBM Storage Scale UserGroup, sponsored by IBM TechXchange",
        "authors": "Spaces are limited. Don’t miss out.",
        "url": "https://conference.eresearch.edu.au/ibm-storage-scale-usergroup-sponsored-by-ibm-techxchange/",
        "details": "As organisations continue to restructure their data resources to modernise and capitalise on the opportunities presented by artificial intelligence (AI), they face significant challenges around the unprecedented rates of data volumes, distribution and varieties.\nTo overcome these challenges, organisations require scalable, distributed file and object storage systems to ensure data accessibility across geographically distributed applications, services, and devices. Join the IBM Storage Scale UserGroup, co-located witheResearch Australasia 2025, and;\nSpaces are limited. Don’t miss out."
    },
    {
        "day": "Monday",
        "time": "10:30 – 11:00",
        "location": "Plaza P6",
        "session_chair": "N/A",
        "title": "Morning Tea | Plaza P6 – P8 Concourse",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Monday",
        "time": "11:00 – 12:30",
        "location": "Plaza P6",
        "session_chair": "N/A",
        "title": "Fundamentals of Accelerated Computing with CUDA Python continued",
        "authors": "Johan Barthelemy",
        "url": "https://conference.eresearch.edu.au/fundamentals-of-accelerated-computing-with-cuda-python/",
        "details": "Dr Johan Barthelemy1, Dr Wei Fang1, Dr Gabriel Noaje1\n1Nvidia, Australia\nAfter his PhD in Applied Mathematics at the University of Namur (Belgium), Dr. Johan Barthélemy joined the SMART Infrastructure Facility of the University of Wollongong (Australia) where he was a Senior Lecturer and the head of the Digital Living Lab researching AIoT solutions for smart cities and environmental monitoring in extreme conditions such as Antarctica. Passionate about applied AI and how to accelerate it, he is now part of the Strategic Researchers Engagement team at NVIDIA, helping scientists to build the next generation of AIs.\nDr. Wei Fang joined NVIDIA in 2021 as a Solutions Architect for High-Performance Computing (HPC) and Artificial Intelligence (AI). In this role, he actively engages with NVIDIA customers across the Asia Pacific South region, focusing on the Higher Education & Research and Supercomputing sectors. Dr. Fang is dedicated to promoting the adoption of NVIDIA's latest technologies to deliver large-scale HPC and AI solutions, driving innovation, and enhancing research productivity. Before his tenure at NVIDIA, Dr. Fang served as an HPC Specialist at Intersect Australia from 2013 to 2021, where he managed Intersect’s HPC services and delivered cutting-edge HPC and cloud solutions to Australian universities. He also worked as a Cloud Specialist at the National Computational Infrastructure (NCI) in 2013 and as a Satellite Ground-station Software Engineer at Geoscience Australia in 2012, where he developed scientific platforms and software solutions. Prior to that, he contributed to the private sector by developing software and algorithms for telecom network optimization. Dr. Fang earned his Ph.D. in Electrical Engineering from Nanyang Technological University, Singapore, in 2004.\nGabriel Noaje has more than 15 years of experience in accelerated computing solutions for High Performance Computing and Artificial Intelligence. He has performed a variety of roles both in the enterprise and public sector that allowed him to manage, design and work on state of the art HPC solutions across a broad range of industries and domains. In his current role at NVIDIA, Gabriel is spearheading the development of HPC business in Asia Pacific.\nPrior to joining NVIDIA, he was a Senior Solutions Architect with SGI and HPE where he has worked closely with major supercomputing centers in APAC. Previously, he was a Senior Computational Scientist at A*STAR Computational Resource Centre in Singapore (A*CRC) supporting users with deploying their applications on GPUs and large HPC systems. Gabriel holds a PhD in Computer Sciences from the University of Reims Champagne-Ardenne, France and a BSc and MSc in Computer Sciences from the Polytechnic University of Bucharest, Romania.\nThis workshop explores how to use Numba—the just-in-time, type-specializing Python function compiler—to accelerate Python programs to run on massively parallel NVIDIA GPUs. You will learn how to: Use Numba to compile CUDA kernels from NumPy universal functions (ufuncs); Use Numba to create and launch custom CUDA kernels; Apply key GPU memory management techniques.\nAt the conclusion of the workshop, you will be able to use Numba to compile and launch CUDA kernels to accelerate your Python applications on NVIDIA GPUs. You will have an understanding of the fundamental tools and techniques for GPU-accelerated Python applications with CUDA and Numba:\n• GPU-accelerate NumPy ufuncs with a few lines of code.\n• Configure code parallelization using the CUDA thread hierarchy.\n• Write custom CUDA device kernels for maximum performance and flexibility.\n• Use memory coalescing and on-device shared memory to increase CUDA kernel bandwidth.\nThe following topics and technologies will be covered:\n• CUDA Python with Numba;\n• CUDA programming general practices."
    },
    {
        "day": "Monday",
        "time": "11:00 – 12:30",
        "location": "Plaza P7",
        "session_chair": "N/A",
        "title": "Solving Data Management Challenges with Globus – from Zero to Hero in one day continued",
        "authors": "Alex Ip, Kyle Chard, Steele Cooke, Greg D’Arcy, Chris Myers",
        "url": "https://conference.eresearch.edu.au/solving-data-management-challenges-with-globus-from-zero-to-hero-in-one-day/",
        "details": "Mr Alex Ip1, Mr Greg D’Arcy1, Mr Chris Myers1, Mr Steele Cooke1,A/Prof Kyle Chard2\n1Aarnet Pty Ltd, Chatswood, Australia2University of Chicago/Argonne National Laboratory, USA\nKyle Chardis a Research Associate Professor in the Department of Computer Science at the University of Chicago. He also holds a joint appointment at Argonne National Laboratory. He co-leads the Globus Labs research group, which focuses on a broad range of research problems in data-intensive computing and research data management.\nSteele Cookeis a Software Developer and collaborates on multiple research projects across the sector, with a focus on developing and refining proof of concepts and taking solutions from initial design through to implementation.\nGreg D’Arcyhelps build and manage digital tools that enable researchers to work more effectively, engaging with teams to understand user requirements and translating those ideas into practical solutions.\nAlex Iphas worked across multiple sectors over several decades, including manufacturing, software development, data engineering, and eResearch infrastructure development, bringing this experience to diverse research domains.\nChris Myersspecialises in information security and eResearch infrastructure, focusing on cyber security standards, advanced networking, and emerging technologies, and has supported Globus and related technologies in Australia for almost 20 years.\nJoin us for a hands-on workshop designed for system administrators looking to harness the power of Globus for efficient research data transfer and automation.\nElevate your institution’s research computing efficiency and productivity with expert insights and peer networking opportunities. Each session will provide practical exercises and an open discussion exploring current use cases.\nThe workshop will be delivered as two self-contained 3-hour sessions. You can attend both sessions or just choose the one that’s right for you.\nMorning Session: Introduction to Globus\nThe morning session assumes no prior knowledge of Globus.\nAfternoon Session: Advanced Globus Topics\nThe afternoon session assumes familiarity with Globus concepts and endpoint configuration (as covered in the morning session)."
    },
    {
        "day": "Monday",
        "time": "11:00 – 12:30",
        "location": "Plaza P8",
        "session_chair": "N/A",
        "title": "IBM Storage Scale UserGroup, sponsored by IBM TechXchange",
        "authors": "Spaces are limited. Don’t miss out.",
        "url": "https://conference.eresearch.edu.au/ibm-storage-scale-usergroup-sponsored-by-ibm-techxchange/",
        "details": "As organisations continue to restructure their data resources to modernise and capitalise on the opportunities presented by artificial intelligence (AI), they face significant challenges around the unprecedented rates of data volumes, distribution and varieties.\nTo overcome these challenges, organisations require scalable, distributed file and object storage systems to ensure data accessibility across geographically distributed applications, services, and devices. Join the IBM Storage Scale UserGroup, co-located witheResearch Australasia 2025, and;\nSpaces are limited. Don’t miss out."
    },
    {
        "day": "Monday",
        "time": "12:30 – 13:30",
        "location": "Plaza P6",
        "session_chair": "N/A",
        "title": "Lunch | Plaza P6 – P8 Concourse",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Monday",
        "time": "13:30 – 15:00",
        "location": "Plaza P6",
        "session_chair": "N/A",
        "title": "Fundamentals of Accelerated Computing with CUDA Python continued",
        "authors": "Johan Barthelemy",
        "url": "https://conference.eresearch.edu.au/fundamentals-of-accelerated-computing-with-cuda-python/",
        "details": "Dr Johan Barthelemy1, Dr Wei Fang1, Dr Gabriel Noaje1\n1Nvidia, Australia\nAfter his PhD in Applied Mathematics at the University of Namur (Belgium), Dr. Johan Barthélemy joined the SMART Infrastructure Facility of the University of Wollongong (Australia) where he was a Senior Lecturer and the head of the Digital Living Lab researching AIoT solutions for smart cities and environmental monitoring in extreme conditions such as Antarctica. Passionate about applied AI and how to accelerate it, he is now part of the Strategic Researchers Engagement team at NVIDIA, helping scientists to build the next generation of AIs.\nDr. Wei Fang joined NVIDIA in 2021 as a Solutions Architect for High-Performance Computing (HPC) and Artificial Intelligence (AI). In this role, he actively engages with NVIDIA customers across the Asia Pacific South region, focusing on the Higher Education & Research and Supercomputing sectors. Dr. Fang is dedicated to promoting the adoption of NVIDIA's latest technologies to deliver large-scale HPC and AI solutions, driving innovation, and enhancing research productivity. Before his tenure at NVIDIA, Dr. Fang served as an HPC Specialist at Intersect Australia from 2013 to 2021, where he managed Intersect’s HPC services and delivered cutting-edge HPC and cloud solutions to Australian universities. He also worked as a Cloud Specialist at the National Computational Infrastructure (NCI) in 2013 and as a Satellite Ground-station Software Engineer at Geoscience Australia in 2012, where he developed scientific platforms and software solutions. Prior to that, he contributed to the private sector by developing software and algorithms for telecom network optimization. Dr. Fang earned his Ph.D. in Electrical Engineering from Nanyang Technological University, Singapore, in 2004.\nGabriel Noaje has more than 15 years of experience in accelerated computing solutions for High Performance Computing and Artificial Intelligence. He has performed a variety of roles both in the enterprise and public sector that allowed him to manage, design and work on state of the art HPC solutions across a broad range of industries and domains. In his current role at NVIDIA, Gabriel is spearheading the development of HPC business in Asia Pacific.\nPrior to joining NVIDIA, he was a Senior Solutions Architect with SGI and HPE where he has worked closely with major supercomputing centers in APAC. Previously, he was a Senior Computational Scientist at A*STAR Computational Resource Centre in Singapore (A*CRC) supporting users with deploying their applications on GPUs and large HPC systems. Gabriel holds a PhD in Computer Sciences from the University of Reims Champagne-Ardenne, France and a BSc and MSc in Computer Sciences from the Polytechnic University of Bucharest, Romania.\nThis workshop explores how to use Numba—the just-in-time, type-specializing Python function compiler—to accelerate Python programs to run on massively parallel NVIDIA GPUs. You will learn how to: Use Numba to compile CUDA kernels from NumPy universal functions (ufuncs); Use Numba to create and launch custom CUDA kernels; Apply key GPU memory management techniques.\nAt the conclusion of the workshop, you will be able to use Numba to compile and launch CUDA kernels to accelerate your Python applications on NVIDIA GPUs. You will have an understanding of the fundamental tools and techniques for GPU-accelerated Python applications with CUDA and Numba:\n• GPU-accelerate NumPy ufuncs with a few lines of code.\n• Configure code parallelization using the CUDA thread hierarchy.\n• Write custom CUDA device kernels for maximum performance and flexibility.\n• Use memory coalescing and on-device shared memory to increase CUDA kernel bandwidth.\nThe following topics and technologies will be covered:\n• CUDA Python with Numba;\n• CUDA programming general practices."
    },
    {
        "day": "Monday",
        "time": "13:30 – 15:00",
        "location": "Plaza P7",
        "session_chair": "N/A",
        "title": "Solving Data Management Challenges with Globus – from Zero to Hero in one day continued",
        "authors": "Alex Ip, Kyle Chard, Steele Cooke, Greg D’Arcy, Chris Myers",
        "url": "https://conference.eresearch.edu.au/solving-data-management-challenges-with-globus-from-zero-to-hero-in-one-day/",
        "details": "Mr Alex Ip1, Mr Greg D’Arcy1, Mr Chris Myers1, Mr Steele Cooke1,A/Prof Kyle Chard2\n1Aarnet Pty Ltd, Chatswood, Australia2University of Chicago/Argonne National Laboratory, USA\nKyle Chardis a Research Associate Professor in the Department of Computer Science at the University of Chicago. He also holds a joint appointment at Argonne National Laboratory. He co-leads the Globus Labs research group, which focuses on a broad range of research problems in data-intensive computing and research data management.\nSteele Cookeis a Software Developer and collaborates on multiple research projects across the sector, with a focus on developing and refining proof of concepts and taking solutions from initial design through to implementation.\nGreg D’Arcyhelps build and manage digital tools that enable researchers to work more effectively, engaging with teams to understand user requirements and translating those ideas into practical solutions.\nAlex Iphas worked across multiple sectors over several decades, including manufacturing, software development, data engineering, and eResearch infrastructure development, bringing this experience to diverse research domains.\nChris Myersspecialises in information security and eResearch infrastructure, focusing on cyber security standards, advanced networking, and emerging technologies, and has supported Globus and related technologies in Australia for almost 20 years.\nJoin us for a hands-on workshop designed for system administrators looking to harness the power of Globus for efficient research data transfer and automation.\nElevate your institution’s research computing efficiency and productivity with expert insights and peer networking opportunities. Each session will provide practical exercises and an open discussion exploring current use cases.\nThe workshop will be delivered as two self-contained 3-hour sessions. You can attend both sessions or just choose the one that’s right for you.\nMorning Session: Introduction to Globus\nThe morning session assumes no prior knowledge of Globus.\nAfternoon Session: Advanced Globus Topics\nThe afternoon session assumes familiarity with Globus concepts and endpoint configuration (as covered in the morning session)."
    },
    {
        "day": "Monday",
        "time": "13:30 – 15:00",
        "location": "Plaza P8",
        "session_chair": "N/A",
        "title": "IBM Storage Scale UserGroup, sponsored by IBM TechXchange",
        "authors": "Spaces are limited. Don’t miss out.",
        "url": "https://conference.eresearch.edu.au/ibm-storage-scale-usergroup-sponsored-by-ibm-techxchange/",
        "details": "As organisations continue to restructure their data resources to modernise and capitalise on the opportunities presented by artificial intelligence (AI), they face significant challenges around the unprecedented rates of data volumes, distribution and varieties.\nTo overcome these challenges, organisations require scalable, distributed file and object storage systems to ensure data accessibility across geographically distributed applications, services, and devices. Join the IBM Storage Scale UserGroup, co-located witheResearch Australasia 2025, and;\nSpaces are limited. Don’t miss out."
    },
    {
        "day": "Monday",
        "time": "13:30 – 15:00",
        "location": "Plaza P9",
        "session_chair": "N/A",
        "title": "AWS Workshop Part 1: Build a ‘Virtual Fugaku’ in AWS.",
        "authors": "Registrations are free, but numbers are limited. Register your place when registering for the conference.",
        "url": "https://conference.eresearch.edu.au/build-a-virtual-fugaku-in-aws/",
        "details": "This 1.5 hour workshop will focus on the portability of high-performance computing (HPC) applications across different computational environments, with a practical emphasis on using the “Virtual Fugaku” platform developed by the RIKEN Centre for Computational Science (R-CCS) in Japan. Virtual Fugaku enables researchers to reproduce the rich software environment of the Fugaku supercomputer within cloud platforms like AWS, delivering a consistent and standardized toolchain for HPC application development and execution. By leveraging Apptainer (formerly Singularity) containers, the Virtual Fugaku environment offers an accessible pathway to deploy and run complex scientific workloads in public or private cloud, or on-premises supercomputers, without the need to reengineer code for each system. This session provides participants hands-on experience building their own HPC cluster in AWS, building and running Apptainer containers, including the Virtual Fugaku environment."
    },
    {
        "day": "Monday",
        "time": "15:00 – 15:30",
        "location": "Plaza P6",
        "session_chair": "N/A",
        "title": "Afternoon Tea | Plaza P6 – P8 Concourse",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Monday",
        "time": "15:30 – 17:00",
        "location": "Plaza P6",
        "session_chair": "N/A",
        "title": "Fundamentals of Accelerated Computing with CUDA Python continued",
        "authors": "Johan Barthelemy",
        "url": "https://conference.eresearch.edu.au/fundamentals-of-accelerated-computing-with-cuda-python/",
        "details": "Dr Johan Barthelemy1, Dr Wei Fang1, Dr Gabriel Noaje1\n1Nvidia, Australia\nAfter his PhD in Applied Mathematics at the University of Namur (Belgium), Dr. Johan Barthélemy joined the SMART Infrastructure Facility of the University of Wollongong (Australia) where he was a Senior Lecturer and the head of the Digital Living Lab researching AIoT solutions for smart cities and environmental monitoring in extreme conditions such as Antarctica. Passionate about applied AI and how to accelerate it, he is now part of the Strategic Researchers Engagement team at NVIDIA, helping scientists to build the next generation of AIs.\nDr. Wei Fang joined NVIDIA in 2021 as a Solutions Architect for High-Performance Computing (HPC) and Artificial Intelligence (AI). In this role, he actively engages with NVIDIA customers across the Asia Pacific South region, focusing on the Higher Education & Research and Supercomputing sectors. Dr. Fang is dedicated to promoting the adoption of NVIDIA's latest technologies to deliver large-scale HPC and AI solutions, driving innovation, and enhancing research productivity. Before his tenure at NVIDIA, Dr. Fang served as an HPC Specialist at Intersect Australia from 2013 to 2021, where he managed Intersect’s HPC services and delivered cutting-edge HPC and cloud solutions to Australian universities. He also worked as a Cloud Specialist at the National Computational Infrastructure (NCI) in 2013 and as a Satellite Ground-station Software Engineer at Geoscience Australia in 2012, where he developed scientific platforms and software solutions. Prior to that, he contributed to the private sector by developing software and algorithms for telecom network optimization. Dr. Fang earned his Ph.D. in Electrical Engineering from Nanyang Technological University, Singapore, in 2004.\nGabriel Noaje has more than 15 years of experience in accelerated computing solutions for High Performance Computing and Artificial Intelligence. He has performed a variety of roles both in the enterprise and public sector that allowed him to manage, design and work on state of the art HPC solutions across a broad range of industries and domains. In his current role at NVIDIA, Gabriel is spearheading the development of HPC business in Asia Pacific.\nPrior to joining NVIDIA, he was a Senior Solutions Architect with SGI and HPE where he has worked closely with major supercomputing centers in APAC. Previously, he was a Senior Computational Scientist at A*STAR Computational Resource Centre in Singapore (A*CRC) supporting users with deploying their applications on GPUs and large HPC systems. Gabriel holds a PhD in Computer Sciences from the University of Reims Champagne-Ardenne, France and a BSc and MSc in Computer Sciences from the Polytechnic University of Bucharest, Romania.\nThis workshop explores how to use Numba—the just-in-time, type-specializing Python function compiler—to accelerate Python programs to run on massively parallel NVIDIA GPUs. You will learn how to: Use Numba to compile CUDA kernels from NumPy universal functions (ufuncs); Use Numba to create and launch custom CUDA kernels; Apply key GPU memory management techniques.\nAt the conclusion of the workshop, you will be able to use Numba to compile and launch CUDA kernels to accelerate your Python applications on NVIDIA GPUs. You will have an understanding of the fundamental tools and techniques for GPU-accelerated Python applications with CUDA and Numba:\n• GPU-accelerate NumPy ufuncs with a few lines of code.\n• Configure code parallelization using the CUDA thread hierarchy.\n• Write custom CUDA device kernels for maximum performance and flexibility.\n• Use memory coalescing and on-device shared memory to increase CUDA kernel bandwidth.\nThe following topics and technologies will be covered:\n• CUDA Python with Numba;\n• CUDA programming general practices."
    },
    {
        "day": "Monday",
        "time": "15:30 – 17:00",
        "location": "Plaza P7",
        "session_chair": "N/A",
        "title": "Solving Data Management Challenges with Globus – from Zero to Hero in one day continued",
        "authors": "Alex Ip, Kyle Chard, Steele Cooke, Greg D’Arcy, Chris Myers",
        "url": "https://conference.eresearch.edu.au/solving-data-management-challenges-with-globus-from-zero-to-hero-in-one-day/",
        "details": "Mr Alex Ip1, Mr Greg D’Arcy1, Mr Chris Myers1, Mr Steele Cooke1,A/Prof Kyle Chard2\n1Aarnet Pty Ltd, Chatswood, Australia2University of Chicago/Argonne National Laboratory, USA\nKyle Chardis a Research Associate Professor in the Department of Computer Science at the University of Chicago. He also holds a joint appointment at Argonne National Laboratory. He co-leads the Globus Labs research group, which focuses on a broad range of research problems in data-intensive computing and research data management.\nSteele Cookeis a Software Developer and collaborates on multiple research projects across the sector, with a focus on developing and refining proof of concepts and taking solutions from initial design through to implementation.\nGreg D’Arcyhelps build and manage digital tools that enable researchers to work more effectively, engaging with teams to understand user requirements and translating those ideas into practical solutions.\nAlex Iphas worked across multiple sectors over several decades, including manufacturing, software development, data engineering, and eResearch infrastructure development, bringing this experience to diverse research domains.\nChris Myersspecialises in information security and eResearch infrastructure, focusing on cyber security standards, advanced networking, and emerging technologies, and has supported Globus and related technologies in Australia for almost 20 years.\nJoin us for a hands-on workshop designed for system administrators looking to harness the power of Globus for efficient research data transfer and automation.\nElevate your institution’s research computing efficiency and productivity with expert insights and peer networking opportunities. Each session will provide practical exercises and an open discussion exploring current use cases.\nThe workshop will be delivered as two self-contained 3-hour sessions. You can attend both sessions or just choose the one that’s right for you.\nMorning Session: Introduction to Globus\nThe morning session assumes no prior knowledge of Globus.\nAfternoon Session: Advanced Globus Topics\nThe afternoon session assumes familiarity with Globus concepts and endpoint configuration (as covered in the morning session)."
    },
    {
        "day": "Monday",
        "time": "15:30 – 17:00",
        "location": "Plaza P8",
        "session_chair": "N/A",
        "title": "IBM Storage Scale UserGroup, sponsored by IBM TechXchange",
        "authors": "Spaces are limited. Don’t miss out.",
        "url": "https://conference.eresearch.edu.au/ibm-storage-scale-usergroup-sponsored-by-ibm-techxchange/",
        "details": "As organisations continue to restructure their data resources to modernise and capitalise on the opportunities presented by artificial intelligence (AI), they face significant challenges around the unprecedented rates of data volumes, distribution and varieties.\nTo overcome these challenges, organisations require scalable, distributed file and object storage systems to ensure data accessibility across geographically distributed applications, services, and devices. Join the IBM Storage Scale UserGroup, co-located witheResearch Australasia 2025, and;\nSpaces are limited. Don’t miss out."
    },
    {
        "day": "Monday",
        "time": "15:30 – 17:00",
        "location": "Plaza P9",
        "session_chair": "N/A",
        "title": "AWS Workshop Part 2: Vibe-coding with Amazon Q Developer!",
        "authors": "Registrations are free, but numbers are limited. Register your place when registering for the conference.",
        "url": "https://conference.eresearch.edu.au/vibe-coding-with-amazon-q-developer/",
        "details": "This workshop will introduce the participants to Agentic development techniques. You will get access to industry leading tools that accelerate development in the hands-on lab. The facilitators will demonstrate how to get the most out of this emerging tech, then provide guidance as participants build their own application. Come with your own blue-sky idea, and bring it to life. Or just work through the guided examples. The choice is yours!\nYou’ll be supplied with a cloud based dev environment, with Amazon Q CLI using the latest Claude models from Anthropic for Agentic spec based development. HTML applications, chatbots and document analysis app examples are provided to kick start your Agentic development journey."
    },
    {
        "day": "Monday",
        "time": "17:30 – 19:00",
        "location": "Plaza P6",
        "session_chair": "N/A",
        "title": "Computer Animated Film session – Boulevard Auditorium",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Tuesday",
        "time": "07:45 – 17:30",
        "location": "N/A",
        "session_chair": "N/A",
        "title": "Registration Open | Boulevard Auditorium Registration Desk",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Tuesday",
        "time": "07:45 – 08:30",
        "location": "N/A",
        "session_chair": "N/A",
        "title": "Networking Breakfast, Exhibition & Poster Viewing | Boulevard Foyer Exhibition AreaMake the most of your conference experience by attending the breakfast on the first day, an inclusive opportunity for all participants to casually break the ice, share ideas, and build essential professional connections before the start of formal sessions.",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Tuesday",
        "time": "08:30 – 08:45",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "Welcome to Country",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Tuesday",
        "time": "08:45 – 09:00",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "WelcomeNatasha Simons, Luc Betbeder-Matibet, Co-Chair, 2025 eResearch Australasia Conference",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Tuesday",
        "time": "09:00 – 09:10",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "Prof. Paul Bonnington",
        "authors": ", Pro-Vice-Chancellor (Research Infrastructure), The University of Queensland",
        "url": "https://conference.eresearch.edu.au/speaker/professor-paul-bonnington/",
        "details": "Pro-Vice-Chancellor (Research Infrastructure), The University of Queensland\nProfessor Paul Bonnington joined UQ in 2022 as Pro-Vice Chancellor (Research and Infrastructure).\nPaul has decades of experience within research infrastructure in the tertiary sector, providing researchers with access to major computing, software, and analytics capability.\nPrior to joining, Paul was Director of eResearch at Monash University. Paul led a multi-disciplinary centre of over 50 professionals in eResearch, data and computational, providing expertise in computing, visualisation and data capabilities for numerous research areas.\nUnder Paul’s direction, the Monash eResearch Centre hosted federally-funded national eResearch infrastructure worth more than $27 million, for specialised high-performance computing, research cloud services and data storage and data management, underpinning the data-centric research of over 4,000 researchers. He also led numerous state and national research infrastructure initiatives while at Monash.\nAt the University of Auckland, Paul established New Zealand’s first eResearch program.\nPaul’s major research interests lie in graph-theory, medical AI, and developing advanced computational solutions to combinatorial problems.Paul has also served on numerous state and national boards in the research and research infrastructure areas, including the current Australian Earth-System Simulator National Research Infrastructure (ACCESS-NRI) Board."
    },
    {
        "day": "Tuesday",
        "time": "09:10-09:50",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "The exploration of Mars through Science, technology,data,and supporting infrastructure",
        "authors": ", Executive Dean, School of Engineering, Edith Cowan University",
        "url": "https://conference.eresearch.edu.au/the-exploration-of-mars-through-science-technology-data-and-supporting-infrastructure/",
        "details": "Paulo will provide an insider perspective of the exploration of Mars with NASA Mars Exploration Rover project. He will describe the science, technology, data gathered and details on the infrastructure supportive the deep space exploration. He will present discoveries that transformed our understanding about the evolution of our Solar System.\nProfessor Paulo de Souza is the Executive Dean of the School of Engineering. A disrupter and transformational advocate for impactful research outcomes and real-world educational delivery, Paulo has spent the last two decades bridging science, industry and society, with the application of engineering to solve unique problems: spanning the critical struggle of our fragile ecosystems, improving industrial processes to the discovery of habitable environments on the surface of Mars. Paulo transformed industrial, research and academic portfolios under his leadership while accelerating performance across all aspects of the business. Prior to joining ECU, Paulo was Dean (Research) at Griffith University, Science Director and Chief Research Scientist at CSIRO, Collaborating Scientist with NASA’s Mars Exploration Rover program and worked in consultancy, mining and steelmaking industry in South America and Europe. He co-authored the series of papers identified at the 2004 Breakthrough of the Year by Science Magazine. Paulo is a Fellow of the Australian Computer Society, a Fellow of the Governance Institute of Australia, a Professional Fellow of Engineers Australia and a member of the ARC College of Experts. Paulo is a Physicist with a master’s in mechanical engineering from UFES (Brazil) and has a Doctoral degree (Dr rer. nat.) from Johannes Gutenberg Universität in Mainz."
    },
    {
        "day": "Tuesday",
        "time": "09:50-10:30",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "Trusted Data Repositories as an Essential Element of the Research Enterprise",
        "authors": ", Director, World Data System-International Technology Office",
        "url": "https://conference.eresearch.edu.au/trusted-data-repositories-as-an-essential-element-of-the-research-enterprise/",
        "details": "Presenter / Author:Reyna Broadhurst\nThe World Data System (WDS), initially created via the International Science Council in 2008, builds on a strong and proven legacy of predecessor World Data Centers established to ensure long-term stewardship, curation, archiving, and dissemination of data. Today, the WDS has a membership of over 145 trusted data repositories and data-focussed organizations. The mission of WDS is to enhance the capabilities, impact, and sustainability of its member data repositories and data services by creating trusted communities of scientific data repositories, strengthening the scientific enterprise throughout the entire lifecycle of data and all related components, creating first-class data that feeds first-class research output, and advocating for accessible data and transparent and reproducible science.\nRepositories, an essential element of the research enterprise, must demonstrate continued relevance to the research communities they serve. In this presentation, we will explore how the WDS and repositories are mobilizing to increase their AI-readiness and to support federated systems, data spaces and down-stream applications. While demonstrating the value of trusted data repositories in the provision of FAIR and machine-actionable data as part of these interconnected systems, we recognize that many challenges and gaps are not yet solved. Thus, we also will highlight areas for improvement, feature promising initiatives, and emphasize the importance of coordinated and collective action."
    },
    {
        "day": "Tuesday",
        "time": "10:30 – 11:00",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "Morning Tea, Exhibition, Poster Viewing | Boulevard Foyer",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Tuesday",
        "time": "11:00 – 11:20",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet",
        "title": "WEHI’s Spatial Omics Data Analytics (SODA) Hub: an environment for spatial analysis from data generation to analysis and beyond",
        "authors": "Julie Iskander",
        "url": "https://conference.eresearch.edu.au/wehis-spatial-omics-data-analytics-soda-hub-an-environment-for-spatial-analysis-from-data-generation-to-analysis-and-beyond/",
        "details": "Dr Julie Iskander1, Mr Michael Milton1\n1WEHI, Melbourne/Parkville, Australia\nJulie Iskander leads the Research Computing Platform at WEHI, where she brings her background in software engineering, computational biology, and biomedical research to drive infrastructure development and support scalable, data-driven science. Her team helps researchers bridge the gap between scientific questions and computational solutions, with growing emphasis on making AI tools and cloud platforms accessible across disciplines.\nSpatial omics has the potential to enable precision diagnostics due to the wealth of information obtained from combining bioimaging, transcriptomic and proteomic technologies. Management and analysis of spatial data has remained a challenge, with large data volumes, numerous technologies and lack of standardised processing pipelines adding to the complexity. The WEHI Spatial Omics Data Analytics (SODA) Hub has been established to provide streamlined environment for users of spatial technology at the institute, from data and metadata management to analysis pipelines.\nIn this presentation, we will outline how the SODA-Hub will support spatial research at the institute by handling data ingestion and metadata collection, streamlining the movement of large data sets and organising data in a way that is FAIR (findable, accessible, interoperable, and reusable). The OME Remote Objects System (OMERO) image management system serves as the foundational platform of the environment and will provide spatial researchers with the ability to view, analyse and organise their spatial data. To streamline analysis, SODA-Hub will provide a suite of pipelines, including cell segmentation and downstream tools for the MERSCOPE, Xenium and COMET platforms, implemented in performant Nextflow pipelines and accessible via a user-friendly web-interface provided by Seqera Platform. This streamlining of spatial research at the institute will lead to better research outcomes, and ultimately better health outcomes for patients."
    },
    {
        "day": "Tuesday",
        "time": "11:00 – 11:20",
        "location": "Boulevard B1",
        "session_chair": "Vanessa Crosby",
        "title": "Unlocking Collaboration: How Swinburne Leverages Research Link Data and PIDs for Strategic Partnerships",
        "authors": "Amir Aryani",
        "url": "https://conference.eresearch.edu.au/unlocking-collaboration-how-swinburne-leverages-research-link-data-and-pids-for-strategic-partnerships/",
        "details": "A/Prof. Amir Aryani1, Dr Thomas Shafee1, Shana Chong1, Avijit Dey1\n1Swinburne University of Technology, Hawthorn, Australia\nAmir Aryani is an Associate Professor at Swinburne University of Technology and leads the Social Data Analytics Lab. His research focuses on using data analytics to address complex health and social issues.\nThe ability to identify and understand internal and external collaboration networks presents an ongoing challenge for many research institutions. This is particularly true for analysing broader industry networks, communicating the impact of industry collaborative efforts, and enhancing the visibility of applied research outcomes.\nIn this presentation, we'll demonstrate how Swinburne University of Technology is leveraging ARDC Research Link Australia (RLA) data to map Swinburne research capabilities and partnerships. The RLA provides a foundational database to explore these collaborations by leveraging the persistent identifiers networks and linking ORCID to other PIDs like ROR, DOI, and RAID. We've enriched this data by infusing it with our internal Swinburne records and further enhanced and connected it using an open-source generative AI. Specifically, GenAI is used to clean and connect otherwise siloed datasets, enabling us to create insightful visualisations of Swinburne's extensive collaboration network.\nWe will demonstrate interactive data visualisations from the Swinburne School of Business, Law and Entrepreneurship, and showcase how this information enables us to explore Swinburne's thought leadership, innovation and industry partnerships. The presentation includes the School areas of societal impact including climate resilience, tackling bullying in sport, decarbonisation, sustainable tourism and poverty alleviation. Finally, we will discuss the valuable role of ORCID and ROR in creating this new capability for Swinburne, and discuss the information gaps, and potential future work in this area."
    },
    {
        "day": "Tuesday",
        "time": "11:00 – 11:20",
        "location": "Boulevard B2",
        "session_chair": "Marlies Hankel",
        "title": "The Nectar National Node: A New Foundation for Scalable, Secure, andCollaborativeResearch",
        "authors": "Kaushik Ramesh, Bryce Torcello",
        "url": "https://conference.eresearch.edu.au/the-nectar-national-node-a-new-foundation-for-scalable-secure-and-collaborative-research/",
        "details": "Mr Kaushik Ramesh1, Mr Bryce Torcello1\n1Australian Research Data Commons, Melbourne, Australia\nKaushik Ramesh is a Senior Cloud DevOps Engineer with a strong background in cloud infrastructure, automation, and agile delivery. With deep expertise in managing OpenStack environments, Kaushik leads initiatives to streamline cloud operations and improve service reliability. They play a key role in project planning, sprint coordination, and workflow automation, serving as both a technical leader and Scrum Master.\nBryce Torcello is a DevOps engineer at ARDC, bringing valuable experience from his previous role managing high-performance computing infrastructure at Peter MacCallum Cancer Centre. He works on the Nectar cloud, Australia's federated OpenStack research cloud platform serving thousands of researchers nationwide. His hands-on experience in both HPC environments and cloud infrastructure management provides unique insights into the practical challenges and opportunities facing research computing today. Having worked on both sides of the infrastructure divide, Torcello offers a pragmatic perspective on evolving research computing needs and the operational realities of running large-scale scientific infrastructure.\nEstablished in 2012, the Australian Research Data Commons (ARDC) Nectar Research Cloud has become a cornerstone of Australia's digital research infrastructure. As an open-source, distributed and federated cloud platform spanning multiple institutions, Nectar has supported nearly 6,700 research projects and attracted over 29,000 users. It continues to empower researchers across traditional and emerging domains such as climate science, genomics and artificial intelligence.\nThe Nectar National Node is managed and operated by ARDC in partnership with AARNet, which hosts the infrastructure. Already operational and supporting researchers across Australia, the Node provides a range of cloud-based services including Virtual Machines, Volume Storage, interactive platforms such as JupyterHub and BinderHub, Virtual Desktops and Kubernetes-based Container Orchestration, all delivered within an enterprise-grade architecture built for performance, reliability and security. With a strong focus on service quality and resilience, and drawing on AARNet’s cybersecurity expertise, the Node is actively pursuing ISO27001 certification to align with international standards for information security and service management. This supports national research platforms, sensitive data workflows and trusted research environments in fields such as health and medical research.\nThis presentation will demonstrate how the National Node enhances Nectar’s distributed model and offers a scalable, secure foundation for research cloud infrastructure. Attendees will gain insights into how the Node helps accelerate time to results and supports the growth of data-intensive projects such as large-scale genomic analysis and multi-institution research collaboration.\nAs Australia's research landscape becomes more complex and data-driven, the National Node ensures equitable access to world-class resources for researchers nationwide."
    },
    {
        "day": "Tuesday",
        "time": "11:00 – 11:20",
        "location": "Boulevard B3",
        "session_chair": "Reyna Broadhurst",
        "title": "Delivering enduring infrastructure: the Planet Research Data Commons",
        "authors": "Kerry Levett, Jonathan Smillie",
        "url": "https://conference.eresearch.edu.au/delivering-enduring-infrastructure-the-planet-research-data-commons/",
        "details": "Mrs. Kerry Levett1, Mr Jonathan Smillie2, Mr Hamish Holewa2\n1Australian Research Data Commons, Australia,2Australian Research Data Commons, Australia\nKerry was an environmental science researcher before moving into data management and research infrastructure. She works to bridge the gap between the users and the builders of digital research infrastructure to create platforms that will transform research.\nThe national earth and environmental science data landscape is rich, diverse and complex; spanning multiple sectors, jurisdictions and data modalities. In order to effectively address multiple national environmental priorities, there is a critical need for digital research infrastructure that supports integrated and seamless national-scale research based on this data.\nThe ARDC’s Planet Research Data Commons (Planet RDC) provides digital research infrastructure for earth and environmental sciences. Through co-design, strategic investment, informatics and computing services, coordination and expertise, the ARDC (Australian Research Data Commons) is enabling collaboration between research, government and infrastructure providers to deliver an enduring ecosystem of data infrastructure.\nThe projects run by our partners are developing data infrastructure and policies to enable FAIR, trusted data and analytics that can interoperate with other FAIR data infrastructures.\nThe projects come together in the Planet Architecture Working Group (AWG) to solve technical and systems challenges. The AWG is a forum for sharing systems and architecture expertise, technical best practice across projects, and emerging challenges. The group collectively conducts technical assessments of critical technologies, ensures alignment of solutions with the Planet RDC program strategy and objectives, and ensures the delivered infrastructure is interoperable and enduring.\nProjects work to a systems Reference Model and a modular Reference Architecture suite which addresses key architectural concerns via a collection of architectural patterns developed in independent but aligned activities.\nThis talk will provide an overview of the Planet RDC infrastructure, the role of the Planet Architecture Working Group, and introduce the framing Reference Model and Architectures."
    },
    {
        "day": "Tuesday",
        "time": "11:00 – 11:20",
        "location": "Boulevard Room",
        "session_chair": "Ellen Lyrtzis",
        "title": "Global patterns of research data storage and management",
        "authors": "Tom Honeyman",
        "url": "https://conference.eresearch.edu.au/global-patterns-of-research-data-storage-and-management/",
        "details": "Dr Tom Honeyman1, TBC (confirming participation from UQ, UniMelb, Monash, Sydney) TBC1\n1Unsw, Sydney, Australia\nTom is a Project Manager, helping to deliver the Research Data Experience program. Previously, he worked as the Solution Architect for the HASS and Indigenous Research Data Commons at the ARDC, and before that was program manager for the ARDC's Research Software program.\nAs part of the work UNSW’s Research Data Experience program, we sought to understand what constitutes best practice in research data storage and management by undertaking a series of case studies to profile universities in the global top 50, as well as domain research institutes and national science bodies.\nWhat emerged was a patchwork of best practice features, spread across all case studies.\nIn this presentation we will show some of the patterns that emerged consistently in the profiles, but also how some local settings had led to different storage and management patterns emerging across the globe. This is especially the case with differences in policy, and relative access to alternative national level infrastructures, which we will profile as part of this talk. Overall, gathering these cases has given us a greater sense at UNSW what in aggregate might constitute apparent best practice."
    },
    {
        "day": "Tuesday",
        "time": "11:20 – 11:40",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet",
        "title": "Transforming Chemical Research Workflows with Semantic and FAIR Data Infrastructures",
        "authors": "Oliver Koepler",
        "url": "https://conference.eresearch.edu.au/transforming-chemical-research-workflows-with-semantic-and-fair-data-infrastructures/",
        "details": "Dr Oliver Koepler1, Dr Felix Bach2, Dr Johannes Liermann6, Prof Sonja Herres-Pawlis5, Dr Nicole Jung7, Dr Steffen Neumann3, Matthias Razum2, Prof Christoph Steinbeck4\n1TIB – Leibniz Information Centre for Science and Technology, Hannover, Deutschland,2FIZ Karlsruhe, Karlsruhe, Deutschland,3Leibniz Institute of Plant Biochemistry (IPB), Halle, Deutschland,4Friedrich-Schiller University Jena, Jena, Deutschland,5RWTH Aachen University, Aachen, Deutschland,6Johannes-Gutenberg University Mainz, Mainz, Deutschland,7Karlsruhe Institute of Technology (KIT), Karlsruhe, Deutschland\nOliver Koepler is Head of the Lab Linked Scientific Knowledge at the Leibniz Center for Science and Technology, TIB in Germany. His work centers on the development of semantic services and infrastructures for research data. It also includes the development, curation and standardisation of ontologies. He is one of the Spokespersons of the NFDI4Chem Consortium for chemistry in the German National Research Data Infrastructure. In his role as Spokesperson of the NFDI Section Metadata, Terminologies and Provenance and as a member of the European Open Science Cloud (EOSC) Technical and Semantic Interoperability Task Force he is interested in the topic of semantic interoperability across disciplines, frameworks and beyond. Oliver is an organic chemist by training, but continued his career in the last 20 years in the area of digital libraries, data science and research data infrastructures.\nNFDI4Chem, part of Germany’s National Research Data Infrastructure (NFDI), is transforming data workflows in chemistry by enabling semantic, machine-actionable research data through integrated services and community engagement. Central to its efforts are standards such as ontologies and metadata schemas, which underpin the entire data lifecycle. The consortium curates a collection of chemistry-specific ontologies and organises the annual international Ontologies4Chem Workshop to foster harmonisation across the chemistry ontology community. NFDI4Chem provides a Terminology Service (TS) to support seamless embedding and use of ontologies in tools like Electronic Laboratory Notebooks (ELNs) and data repositories enabling semantic annotation of chemical entities, reactions, and experimental conditions thus providing full provenance of data generation. ELNs are the core tools in supporting data and metadata creation. They allows researchers to capture, analyse, and document data directly in the lab and to transfer it into a federation of chemistry repositories. These repositories, endorsed by over 30 journals, facilitate publication, long-term archiving and reuse. To promote standardisation in data and metadata, NFDI4Chem developed Minimum Information for Chemical Investigations (MIChI), offering guidelines, data models, and checklists. These are used to ensure data quality across services and are implemented in metadata schemas tailored for chemistry. The metadata of the repository federation is harvested and indexed by the NFDI4Chem Search Service, currently covering around 140,000 datasets. In summary, NFDI4Chem enables semantic workflows from data creation to reuse, paving the way for machine-actionable data and the creation of chemistry knowledge graphs to support future research and AI applications."
    },
    {
        "day": "Tuesday",
        "time": "11:20 – 11:40",
        "location": "Boulevard B1",
        "session_chair": "Vanessa Crosby",
        "title": "RAiD: the purpose built persistent identifier for projects",
        "authors": "Matthias Liffers",
        "url": "https://conference.eresearch.edu.au/raid-the-purpose-built-persistent-identifier-for-projects/",
        "details": "Mr Matthias Liffers1\n1Australian Research Data Commons, Perth, Australia\nMatthias has worked at the interface of information management and information technology for the best part of two decades. As Product Manager (RAiD), he spearheads the development of the global RAiD system.\nPersistent identifiers (PIDs), such as DOI and ORCID, are widely used to unambiguously refer to entities in the research ecosystem, such as publications, grants, datasets, researchers and other contributors, and organisations. PIDs enable qualified references between these entities. These qualified references build a picture of global research called the research or PID graph. This graph can be used by research organisations to automate the collection of information about research.\nUntil now, the existence of the fundamental unit of research – the project – could only be inferred from grants and publications. RAiD (Research Activity Identifier; ISO 23527:2022) is the missing link in the PID graph and was developed by the Australian Research Data Commons (ARDC) to uniquely and unambiguously identify projects. RAiD provides a more complete picture of global research.\nThis lightning talk will introduce the problem RAiD was engineered to solve, discuss how RAiD fits into the PID graph, and show how research organisations in Australia and Aotearoa New Zealand can access the ARDC RAiD Service."
    },
    {
        "day": "Tuesday",
        "time": "11:20 – 11:40",
        "location": "Boulevard B2",
        "session_chair": "Marlies Hankel",
        "title": "A Cloud-based Grid Computing Site",
        "authors": "Jonathan Woithe",
        "url": "https://conference.eresearch.edu.au/a-cloud-based-grid-computing-site/",
        "details": "Dr Jonathan Woithe1, Dr David Dossett2, Dr Marcus Ebert3, Prof Paul Jackson1, Prof Martin Sevior2\n1University Of Adelaide, Australia,2University of Melbourne, Australia,3University of Victoria, Canada\nJonathan obtained his PhD in physics at the University of Adelaide, and has worked extensively on the development of Atmospheric radar systems in the years since. He has been an active member in the Australian Open Source community, including involvement with Linux Australia. Most recently he has taken on a role at the University of Adelaide in the High Energy Physics group, overseeing a local compute cluster and the development of the Australian Grid Computing site to support the ATLAS and Belle II projects.\nhttps://orcid.org/0009-0008-2836-1162\nExperimental high energy particle physics research has significant compute and storage needs. Compute capacity is required both to process raw collision data from experiments attached to particle accelerators, and to analyse and model those collisions. In the case of the ATLAS experiment attached to the European Large Hadron Collider, this involves processing more than 25 petabytes of data per week. In addition, experiments such as Belle II (attached to the Japanese SuperKEKB accelerator) and ATLAS each require tens of petabytes of storage per year to hold raw data, reconstructions and analysis products for later reference or reuse. The necessary high performance computing infrastructure to support this work is provided by over 150 distributed sites around the world which collectively form the global computing grid.\nWith cloud-based resources becoming ubiquitous, using such infrastructure to create a grid computing site has become attractive from economic, logistic and technical management perspectives. After outlining the technical requirements of a grid computing site, we will describe the Australian cloud-based implementation employed for the \"AU-Melbourne\" site which currently provides resources for the ATLAS and Belle II experiments."
    },
    {
        "day": "Tuesday",
        "time": "11:20 – 11:40",
        "location": "Boulevard B3",
        "session_chair": "Reyna Broadhurst",
        "title": "Uniting Research Infrastructures for Solid Earth Science: From Shared Vision to Global Action",
        "authors": "Helen Glaves",
        "url": "https://conference.eresearch.edu.au/uniting-research-infrastructures-for-solid-earth-science-from-shared-vision-to-global-action/",
        "details": "Carmela Freda1,5, Rebecca Bendick2, Tim Rawling3, Elisabetta D'Anastasio4, Helen Glaves6,1, Gaetano Festa7, Rebecca Farringhton3, Mrs. Federica Tanlongo1, Rossana Paciello5,1, Jan Michalek8,1, Daniele Bailo5,1, Otto Lange9,1, Daniela Mercurio1, Elizabeth  Abbott4, Jonathan  Hanson4, Lesley Wyborn10, Massimo Cocco1,5\n1EPOS ERIC, Roma, Italy,2EarthScope Consortium, Washington DC, USA,3Auscope, Melbourne, Australia,4GNS NZ, Avalon, New Zealand,5Istituto Nazionale di Geofisica e Vulcanologia (INGV), Roma, Italy,6British Geological Survey (BGS), Nottingham, United Kingdom,7Università di Napoli \"Federico II\", Naples, Italy,8Universitetet i Bergen , Bergen, Norway,9Universiteit Utrecht, Utrecht, The Netherlands,10Australian National University, Canberra, Australia\nHelen Glaves is Senior Data Scientist at the British Geological Survey. With a background in geology and information technology, she has led major initiatives in marine data interoperability and open science, including the Ocean Data Interoperability Platform and the Research Data Alliance. A strong advocate for FAIR data, she received the European Geosciences Union (EGU) Ian McHarg Medal in 2016 and served as EGU President from 2021 to 2023. Glaves is also an Editor for the Earth and Space Science journal published by the American Geophysical Union.\nhttps://orcid.org/0000-0001-8179-4444\nUnderstanding the complex processes shaping our planet requires global collaboration and robust, interoperable Research Infrastructures. Solid Earth science depends on access to petabyte-scale, multidisciplinary data that spans borders and domains. The European Plate Observing System (EPOS), AuScope (Australia), EarthScope (USA), and GNS Science (New Zealand) are leading this effort through a shared commitment to open science, FAIR data practices, and global equity.\nTheir collective vision is to move from collaboration to federating regional Research Infrastructures into a cohesive global system that enables seamless access to high-quality data and services, and involving other similar infrastructures in Africa, Asia and Latin America.\nTurning this vision into reality demands the alignment not only of technical standards, but also legal frameworks, supported by sustainable investment in digital infrastructure and capacity building, and the engagement of policy actors play a critical role in fostering the infrastructure strategies, simplifying regulations, and supporting workforce development. Major challenges include ensuring interoperability across fragmented data standards, navigating diverse regulatory environments, securing long-term funding, and promoting inclusivity—particularly for under-resourced regions and Indigenous communities.\nThis global initiative champions the transformative power of shared infrastructure—not only to accelerate scientific discovery, but to translate knowledge into action that strengthens societal resilience, advances sustainability, supports the UN Sustainable Development Goals, and reinforces science diplomacy in an era of environmental and geopolitical urgency."
    },
    {
        "day": "Tuesday",
        "time": "11:20 – 11:40",
        "location": "Boulevard Room",
        "session_chair": "Ellen Lyrtzis",
        "title": "Transforming Research Data Experience at UNSW",
        "authors": "Rizwan Sathakkathulla, Mohammad Islam",
        "url": "https://conference.eresearch.edu.au/transforming-research-data-experience-at-unsw/",
        "details": "Mr Rizwan Sathakkathulla1, Mr Mohammad Islam1\n1UNSW, Kensington, Australia\nRizwan works at UNSW as an Enterprise Architect and is actively involved in various research initiatives, including data, compute, training and areas such as ethics, grants, graduate and HDR research programs. As part of a strategic initiative to streamline platform silos, enhance our security posture, manage data growth and address gaps in the current architecture, governance, data management and policy, the Research Data Experience Program was added to our roadmap.\nMohammad Islam works at UNSW as the Head of Research Solutions at UNSW and oversees a range of research portfolio initiatives. One of his key deliverables is the Research Data Experience Program, which plays a critical role in helping the University address the significant growth in research data and associated costs. This program also aims to streamline platform silos, strengthen the security posture, and close existing gaps in architecture, governance, data management, and policy—making it a strategic priority on our roadmap.\nThis presentation will walk through the current state of UNSW’s research data environment, where data is growing 30% every year, stored across disconnected systems with weak compliance and rising costs. We’ll highlight the pain points like legacy technology, lack of scalability, resilience and inefficiencies that slow down researchers and increase risks.\nWe’ll also share insights from a sector, Gartner and industry scan, showing advancements by peer institutions in the area of research data platform. This sets the stage for our conceptual target state: a unified, secure platform blending on-premises and cloud storage, with efficient data tools and a Trusted Research Environment (TRE) to streamline collaboration.\nFinally, we’ll outline the expected outcomes like lower costs, better compliance, and a smoother experience for researchers. By fixing siloed platforms, automating workflows, and aligning with best practices, this program will future-proof UNSW’s research data for long-term success."
    },
    {
        "day": "Tuesday",
        "time": "11:40 – 12:00",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet",
        "title": "Image Processing Portal (IPP): Scalable web-based microscopy data analysis – Containerization, Orchestration, and HPC migration",
        "authors": "Nishanthi Dasanayaka, Mark Endrei",
        "url": "https://conference.eresearch.edu.au/image-processing-portal-ipp-scalable-web-based-microscopy-data-analysis-containerization-orchestration-and-hpc-migration/",
        "details": "Dr Nishanthi Dasanayaka1, Dr Mark Endrei1, Dr Nicholas Condon2, Dr James Springfield2\n1Research Computing Center, The University of Queensland, Brisbane, Australia,2Institute for Molecular Bioscience, The University of Queensland, Brisbane, Australia\nNishanthi Dasanayaka is a Research Software Engineer at the Research Computing Centre, The University of Queensland, with nearly three years of experience in eResearch portal development. She holds a PhD in Computer Science from Queensland University of Technology, Brisbane. With a background in system architecture design and algorithm development, her work focuses on integrating advanced web technologies and computational methods to develop scalable eResearch platforms that support UQ researchers.\nMark Endrei is a senior principal research software engineer at the Research Computing Centre, The University of Queensland, Australia. He also has more than 20 years of experience in IT industry, working with large corporations both nationally and internationally. He has a PhD from The University of Queensland and a Bachelor of Engineering Degree (H1) in Computer Systems Engineering from RMIT University.\nModern light-sheet microscopy techniques produce terabytes of high-resolution, time-lapse imaging data per experiment, posing major challenges in data management and analysis. To address this, we developed the Image Processing Portal (IPP)- a web-based platform enabling researchers to process microscopy datasets using organizational High-Performance Computing (HPC) resources without requiring command-line skills.\nDeveloped in partnership with the Research Computing Centre’s Metropolitan Data Caching Infrastructure (MeDICI) at the University of Queensland (UQ), IPP simplifies GPU-accelerated image analysis tasks such as deconvolution, file conversion, batch processing, deskewing, and image transformations such as Z-projection. Users can interact with HPC file systems, run macros from shared repositories, perform deconvolution through an integrated metadata extraction pipeline, and soon, automatic 3D stitching of Teravoxel-sized tiled microscopy images through tools such as TeraStitcher -all via an intuitive web interface.\nRecent enhancements add significant new functional and infrastructure improvements. Deployment of the IPP portal was modernized from Docker Compose to Kubernetes, providing improved scalability, reliability, and efficiency. We also review migrating the IPP app from the now retired Wiener supercomputer to UQ’s next-generation Bunya supercomputer, enabling faster processing, improved GPU access, and integration with interactive visual computing via onDemand.\nThese developments transform IPP into a robust, scalable solution for large-scale image analysis, accessible to non-specialist users and adaptable to evolving research needs. IPP now offers researchers a user-friendly, HPC-powered image processing, lowering HPC and cloud computing technical barriers."
    },
    {
        "day": "Tuesday",
        "time": "11:40 – 12:00",
        "location": "Boulevard B1",
        "session_chair": "Vanessa Crosby",
        "title": "Persistent Identifier-Driven Infrastructure for National Research Discovery: Behind the Scenes of Research Link Australia and Research Data Australia.",
        "authors": "Enming Zhang, Shikha Chawla",
        "url": "https://conference.eresearch.edu.au/persistent-identifier-driven-infrastructure-for-national-research-discovery-behind-the-scenes-of-research-link-australia-and-research-data-australia/",
        "details": "Ms Enming Zhang, Ms Shikha Chawla\nEnming is a highly accomplished Business Analyst with a strong track record in leveraging evolving digital capabilities to deliver innovative and impactful solutions across the research and data infrastructure landscape. With deep expertise in Agile delivery, Enming is adept at bridging the gap between business needs and technical capabilities, fostering strong collaboration between diverse stakeholder groups and IT teams. Her strategic insight and analytical rigour have consistently driven successful project outcomes, ensuring alignment with organisational priorities, compliance standards, and long-term digital transformation goals. Enming’s leadership and experience make her a trusted advisor in navigating complex, data-centric initiatives.\nShikha is an experienced Business Analyst with deep expertise in emerging technologies, data governance, and the delivery of innovative, data-driven solutions in complex research and digital infrastructure environments. With a strong command of Agile methodologies, she excels at fostering cross-functional collaboration between stakeholders and IT teams to translate strategic objectives into actionable outcomes. Shikha is passionate about enabling continuous improvement, ensuring data integrity, and aligning project deliverables with organisational policy and compliance frameworks. Her work consistently bridges the gap between technical implementation and business value, supporting the development of scalable, sustainable digital research ecosystems.\nResearch Data Australia (RDA) and Research Link Australia (RLA) are national discovery services supporting the Australian research sector. At the heart of both platforms lies a robust infrastructure built on persistent identifiers (PIDs) and a dynamic metadata index that enables rich, interlinked discovery experiences.\nThis presentation explores the technical design and implementation underpinning RDA and RLA, focusing on how PIDs such as DOIs, ORCIDs, and RAiDs form the backbone of a continuously evolving PID graph. This graph enables contextual connections between datasets, publications, researchers, institutions, and projects, ensuring both discoverability and provenance.\nIn parallel, a highly tuned metadata indexing pipeline supports responsive and scalable search capabilities across millions of records. We discuss the challenges of harmonising heterogeneous metadata, strategies for graph enrichment, and approaches to real-time updates and synchronisation between the PID graph and search index.\nBy presenting lessons learned and opportunities ahead, this talk provides insight into how national-scale infrastructure can leverage identifiers to support research visibility, reuse, and trust."
    },
    {
        "day": "Tuesday",
        "time": "11:40 – 12:00",
        "location": "Boulevard B2",
        "session_chair": "Marlies Hankel",
        "title": "Finding sustainable and user accepted solutions to the space and number of files needs of software environments",
        "authors": "Finding sustainable and user accepted solutions to the space and number of files needs of software environments",
        "url": "https://conference.eresearch.edu.au/finding-sustainable-and-user-accepted-solutions-to-the-space-and-number-of-files-needs-of-software-environments/",
        "details": "Dr Marlies Hankel1, Miss Shelly Harris1\n1The University of Queensland, Brisbane, Australia\nDr Marlies Hankel has a master in mathematics and a PhD in computational chemistry. She moved to The University of Queensland in 2004 and has held positions in the Australian Institute for Bioengineering and Nanotechnology and School of Mathematics as a senior research fellow and lecturer. Her research was in energy storage materials, and she taught numerical methods. She built her first high performance computing cluster in 2004 and has built 4 clusters over the years. She enjoys teaching and helping others and finally moved full time to the Research Computing Centre (RCC) in 2021. Her role in RCC is skills training and support of users and the development of operational procedures.\nSituation\nSoftware environments (SE), installed via conda, python or pip, provide a convenient way for users to install software for life sciences and artificial intelligence research domains. However, some SEs include many dependencies, databases or models which can quickly run into millions of files and terabytes of data. In a high-performance computing (HPC) environment available space is limited and the number of files and space for user data are constrained by quotas. Therefore, many users, especially those new to HPC, run into trouble with file system quotas.\nTask\nDuring this Birds of a Feather (BoF) we will discuss the type of user spaces provided on different HPC facilities and the common problems users encounter with these when installing SEs. Our goal is to identify what current measures are used and how these could be combined or improved to provide a sustainable way forward for users and HPC providers.\nAction\nWe will discuss commonly employed solutions such as increasing user quotas, provision of centrally installed SEs, databases, and models, and user education. Participants will give an overview of their setup and outline the barriers they face. We will then discuss how common measures can be improved and better utilised.\nResult\nThe key outcome of this BoF is to find sustainable solutions to the space and file problem of SEs that would be more readily adopted by users."
    },
    {
        "day": "Tuesday",
        "time": "11:40 – 12:00",
        "location": "Boulevard B3",
        "session_chair": "Reyna Broadhurst",
        "title": "Towards a Coherent and Coordinated Spectrum of Data Stewardship and Sharing Services",
        "authors": "Towards a Coherent and Coordinated Spectrum of Data Stewardship and Sharing Services",
        "url": "https://conference.eresearch.edu.au/towards-a-coherent-and-coordinated-spectrum-of-data-stewardship-and-sharing-services/",
        "details": "Mrs. Reyna Jenkyns1, Helen Glaves2\n1World Data System, Victoria, Canada,2British Geological Survey, United Kingdom\nReyna Jenkyns is currently the Associate Director for the World Data System International Technology Office, hosted by Ocean Networks Canada (ONC) at the University of Victoria. Prior to the WDS, she worked for ONC from 2009 to 2023, most recently as the Data Stewardship Manager. In that time, she has contributed to many data community committees and working groups. She received her Bachelor of Mathematics from the University of Waterloo, and her Master of Science from the University of Victoria.\nHelen Glaves is a Senior Data Scientist at the British Geological Survey (BGS), with more than 30 years’ experience in marine geoscience and geoinformatics. Her current role focuses on the development and implementation of research infrastructures, which includes acting as Director of the Integrated Core Services (ICS-C) for the European Plate Observing System (EPOS). She is actively involved in a number of national and international initiatives addressing various aspects of open science and data stewardship, including acting as co-chair for the In Situ Data subgroup and a member of the Data and Knowledge Working Group within the Group on Earth Observations (GEO). She is also an editor for the American Geophysical Union (AGU) journal Earth & Space Science and chair of the AGU Charles S. Falkenberg Award committee.\nMany initiatives and networks exist that share some common goals with respect to data curation, sharing, archiving and preservation. These initiatives take many forms with numerous points of intersection and overlap: data repositories (disciplinary, generalist, institutional), research institutions, collaborative environments (e.g., data spaces, digital twins), technical service providers, federated data systems, data commons, registries and more. In an increasingly challenging global landscape for scientific research data stewardship, there is a growing need to work collaboratively to address common goals and maximize potential benefits of pooling our shared global resources.\nThis BoF aims to bring together global, regional and national initiatives that are working towards common goals with respect to data stewardship, and identify those commonalities where greater collaboration, coordination and combining of resources will reduce duplication of effort, avoid divergence in practices and policies and potentially maximize the benefits of open sharing and reuse of data, products and services. Furthermore, to ensure a robust global data ecosystem, we aim to develop a better understanding of the distinct value-added roles and interdependencies between initiatives.\nSeveral examples will be given of existing initiatives that are already actively promoting open sharing and reuse of data that are aligned with widely accepted policies and practices including adoption of recognized principles such as FAIR, CARE and TRUST. Four lightning talks will feature relevant initiatives and networks: Kelsey Druken (ACCESS-NRI), Helen Glaves (GEO), Mark Rehbein (Australian Ocean Data Network), and Angus Nixon (AuScope Geochemistry Network). Participants will be encouraged to engage in the discussion and contribute examples of both the challenges encountered and successful collaborations, illustrating potential opportunities for future cooperations that can make use of dwindling resources available for data stewardship."
    },
    {
        "day": "Tuesday",
        "time": "11:40 – 12:00",
        "location": "Boulevard Room",
        "session_chair": "Ellen Lyrtzis",
        "title": "From Awareness to Application: Exploring Researchers’ Skills Needs for Using Digital Research Infrastructure",
        "authors": "From Awareness to Application: Exploring Researchers’ Skills Needs for Using Digital Research Infrastructure",
        "url": "https://conference.eresearch.edu.au/from-awareness-to-application-exploring-researchers-skills-needs-for-using-digital-research-infrastructure/",
        "details": "Miss Ellen Lyrtzis1, Mrs. Kathryn Unsworth1, Dr Amany Gouda-Vossos1, Mrs. Sarah Thomas2\n1Australian Research Data Commons (ARDC), Australia,2Australian Access Federation, Australia\nDr Amany Gouda-Vossos: With expertise in molecular sciences, evolutionary biology, behavioural sciences, and health and science education, Amany employs an evidence-based approach to build the capabilities of project partners. Her passion to foster a strong learning culture was cultivated through her experience across university and corporate settings.\nKathryn Unsworth: Kathryn leads the Skilled Workforce Development program, which drives a nationally coordinated approach to skills initiatives and targets areas of the workforce that conduct, underpin and enable data-intensive research. She also guides the ARDC’s leadership in skills uplift and capability transfer across the Australian research sector.\nSarah Thomas: An enthusiastic and resolute community engagement manager, with demonstrated expertise in research and innovation ecosystems. Sarah is committed to using her skills in relationship management, communication, program design, delivery and evaluation to deliver positive outcomes for the community.\nEllen Lyrtzis: With a background in health and medical research and project management, Ellen supports the ARDC’s vision of advancing data-driven research by building national capability, fostering collaboration, and promoting best practices in data, infrastructure, platforms, and skills development.\nThe increasing complexity and scale of the digital research infrastructure (DRI) landscape, results in the need for researchers to develop a broad and evolving set of digital research skills. These skills should include both foundational competencies and emerging capabilities that align with the changing digital research environment. Domain expertise alone is no longer enough, researchers must also be proficient in digital tools, data management, and computational methods to fully leverage DRI.\nSignificant skills and knowledge gaps persist, as many researchers struggle with limited awareness of which digital tools and methods are relevant to their work, compounded by insufficient skills development and training opportunities, and unmapped learning pathways.\nTo ground discussions, we’ll draw on participant insights and research contexts, examining them through the lens of the ARDC Digital Research Capabilities and Skills Framework to identify the critical digital skills most relevant to the effective use of DRI. This examination will be augmented by a working draft approach to initiating a skills and training gap analysis, focusing primarily on skills required by HDR students and early-career researchers. The aim is to highlight what skill areas are currently addressed through university curricula, external training providers and training specifically delivered by DRI providers. This activity will collectively map the disconnects and opportunities for coordinated support.\nThis BOF brings together those responsible for upskilling researchers to identify challenges and explore solutions that enhance researcher readiness to fully leverage DRI. The insights gathered will inform future training, approaches to skills development, and potential opportunities for collaboration."
    },
    {
        "day": "Tuesday",
        "time": "12:00 – 12:20",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet",
        "title": "Bringing the FAIR software principles to the General Atomic and Molecular Electronic Structure System",
        "authors": "Jorge Luis Galvez Vallejo",
        "url": "https://conference.eresearch.edu.au/bringing-the-fair-software-principles-to-the-general-atomic-and-molecular-electronic-structure-system/",
        "details": "Dr Jorge Luis Galvez Vallejo1\n1National Computational Infrastructure, Canberra, Australia\nI am an High Performance Computing specialist at the National Computational Infrastructure in Canberra, Australia. I am formally trained in quantum computational chemistry but branched out into high performance computing during my PhD and Postdoctoral Fellow appointments. During my PhD I was part of the Exascale Computing Project under the banner of GAMESS under professor Mark Gordon. I played a crucial role in the porting of key routines to GPUs in the LibCChem/GMSHPC program using C++/CUDA/HIP. Upon completing my PhD I came to Canberra to the ANU to work under A. P. Giuseppe Barca, who lead the C++/CUDA efforts of the GAMESS-ECP program. During my time at the ANU I became the lead developer and key maintainer of the EXESS program, a from scratch written electronic structure software that specializes in exploiting massively parallel architectures with GPU accelerators to its full extent. The highlight of my tenure was being a part of the team that was awarded the 2024 Gordon Bell prize by the ACM for our ab initio molecular dynamics simulation that was able to use the entirety of Frontier effectively running at more than 1 Exaflop. I moved to the NCI in May 2025 to seek new challenges in the areas of weather, climate, and geophysics.\nThe General Atomic and Molecular Electronic Structure System (GAMESS) is one of the most widely used quantum chemistry programs in the world and has been in continuous development since the late 70s. GAMESS has been a pioneer in many areas of computational chemistry and software technology.\nGAMESS has always been a free to use, source available program; however, it has lacked key software engineering technologies that have limited it from being FAIR software. In this work, I present a series of innovations introduced to GAMESS to abide by the FAIR principles. First, the build system of GAMESS has been entirely rewritten to use CMake with modularity in mind, being able to select external packages, libraries to use with ease.\nUsing the new CMake infrastructure, I have made GAMESS more accessible by introducing a conda environment for dependency management and a spack recipe for deployment in HPC centres.\nBy reworking the build system, I provide an accessible and reusable GAMESS library which other programs can link to and use. This is made easier with a CMake package included in with the program. The new infrastructure makes it seamless to link external packages to GAMESS.\nAdditionally, a unit-testing system has been introduced to GAMESS for further validation of the program.\nBy introducing these key improvements to GAMESS, software modernization incentives can begin, such as migrating from Fortran77 to modern Fortran constructs to ensure portability in edge computing systems. Finally, a full rework of the documentation is presented."
    },
    {
        "day": "Tuesday",
        "time": "12:00 – 12:20",
        "location": "Boulevard Room",
        "session_chair": "Ellen Lyrtzis",
        "title": "PITSCHI: The Imaging Data Management System at CMM, UQ – Enabling FAIR and Persistent Identifier-Driven Research Data Management",
        "authors": "Rubbiya Ali",
        "url": "https://conference.eresearch.edu.au/pitschi-the-imaging-data-management-system-at-cmm-uq-enabling-fair-and-persistent-identifier-driven-research-data-management/",
        "details": "Ms Rubbiya Ali1, Dr Mark Endrei2, Tom Mason1, Nishanthi Dasanayaka2, Jake Carroll2, Prof. Roger Wepf1\n1Centre For Microscopy And Microanalysis, The University of Queensland., St Lucia, Australia,2Research Computing Centre, The University of Queensland, Queensland, St Lucia, Australia\nDr. Rubbiya Ali is the Data Informatics Manager at the Centre for Microscopy and Microanalysis, The University of Queensland. With a PhD from UQ’s Institute for Molecular Bioscience, her expertise lies in computational image analysis, large-scale microscopy data workflows, and scientific data management. She has developed novel algorithms (3D BLE and RAZA) for 3D edge detection and 3D particle picking in electron tomography, and has played a key role in designing and implementing PITSCHI, a scalable, FAIR-aligned platform that enables deep integration of data capture, storage, and analysis for large-scale imaging workflows.\nModern high-throughput imaging technologies—such as fast CMOS detectors and high-resolution microscopy—routinely generate multi-terabyte datasets that challenge conventional data handling models. At the Centre for Microscopy and Microanalysis (CMM), The University of Queensland (UQ), over 700 researchers from academia and industry access more than 50 instruments across five facilities. For such a complex environment, robust, automated, and FAIR-compliant data infrastructure is essential.\nTo address this need, CMM and UQ’s Research Computing Centre (RCC) developed PITSCHI (Particle Image depoT using Storage CacHing Infrastructure)—a scalable, open-source imaging data platform built on Clowder (NCSA, USA), aligned with the Australian Characterisation Commons at Scale (funded by ARDC in collaboration with Microscopy Australia).\nPITSCHI automates data capture, secure transfer, and ingestion into a searchable, metadata-rich repository. It integrates deeply with UQ’s research ecosystem—including the Research Infrastructure Management System (RIMS), Research Data Manager (RDM), and the MeDiCI high-performance storage fabric—enabling seamless data flow from acquisition to archiving.\nA key innovation is the implementation of Persistent Identifiers (PIDs) at dataset, instrument, and facility levels. This supports traceability, reproducibility, and reusability of data, while enabling automated metadata enrichment and reducing manual curation overhead.\nTo date, PITSCHI has ingested over 635 TB across 7.8 million files, demonstrating its capacity to support long-term, institution-wide data stewardship. Architectural insights, practical lessons from PID deployment, and the design of PITSCHI highlight how a federated, FAIR-aligned infrastructure can effectively support complex research environments."
    },
    {
        "day": "Tuesday",
        "time": "12:20 – 12:40",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet",
        "title": "Minutes-to-Modules: Rapid, Reproducible Software Packaging with Spack and CI for Research HPC on Gadi and Setonix",
        "authors": "Justin Kin Jun Hew",
        "url": "https://conference.eresearch.edu.au/minutes-to-modules-rapid-reproducible-software-packaging-with-spack-and-ci-for-research-hpc-on-gadi-and-setonix/",
        "details": "Mr Justin Kin Jun Hew1, Mr Harshula Jayasuriya1, Mr Tommy Gatti1, Dr Aidan Heerdegen, Mr Lawrence Bird1, Dr Mike Tetley1\n1Australia’s Climate Simulator (ACCESS-NRI), Australian National University, Canberra, Australia, Acton, Australia\nJustin Kin Jun Hew is a Research Software Engineer at ACCESS-NRI and a Research Officer in the Department of Nuclear Physics, working at the intersection of computational physics and climate modeling. His research spans ice sheet dynamics, high-performance computing, and turbulence in magnetohydrodynamics. Justin has contributed to open-source scientific software and he was the 2024 Bok Honours scholar in astrophysics and physics at the Australian National University.\nORCiD:https://orcid.org/0000-0002-5238-6115\nScientific discovery often stalls at the software-installation step: every dependency resolved by hand is an hour not spent on analysis. At ACCESS-NRI we turned this bottleneck into a two-hour sprint by coupling Spack’s declarative package manager with a lightweight, test-first recipe pattern. Using the Ice-sheet and Sea-level System Model (ISSM) as a case study, we demonstrate how a single, readable Python class now reproduces a 50-library HPC build—MPI, PETSc, Triangle, wrappers, examples and all—across Gadi, Setonix and GitHub Actions CI.\nThis presentation showcases three accelerators that turn software installation into a sprint. First, a template-driven scaffolding tool generates a fully-formed package.py—complete with variants and configure_args()—so researchers tweak only the science-specific components instead of wrestling with boilerplate. Second, a lightweight dependency-introspection utility scans modules already loaded and converts them into precise Spack constraints, slashing trial-and-error cycles. Finally, treating CI as documentation, GitHub Actions builds every commit in both Docker and on HPC nodes, caches resulting artefacts and generates build logs that double as living install notes—meaning newcomers can reproduce a working environment in minutes, not days. We report a significant drop in “time-to-first-run” for new team members—and a spike in upstream contributions as external collaborators adopt such templates. Attendees will leave with a copy-paste recipe, a checklist for packaging complex MPI codes, and a roadmap to embed rapid packaging practices in their own projects.\nWhether you maintain legacy Fortran or fresh CUDA kernels, this talk shows how a culture of package early, package often unlocks quicker science and creates calmer researchers."
    },
    {
        "day": "Tuesday",
        "time": "12:20 – 12:40",
        "location": "Boulevard Room",
        "session_chair": "Ellen Lyrtzis",
        "title": "Driving Australia’s Research Data Ecosystem Capability: The National Persistent Identifier (PID) Strategy and Research Policy",
        "authors": "Linda O’Brien, Lee Harris",
        "url": "https://conference.eresearch.edu.au/driving-australias-research-data-ecosystem-capability-the-national-persistent-identifier-pid-strategy-and-research-policy/",
        "details": "Prof. Linda O'Brien1, Lee Harris, Clare Nulley-Valdes, Natasha Simons\n1National PID Strategy Advisory Group, Australia\nLinda O’Brien is a highly experienced senior executive, Board director and consultant who has successfully delivered operational, transformational and strategic initiatives within the academic and community sectors. She provided strategic consulting to the ARDC in the development of the National Persistent Identifier Strategy, having been previous Chair of the ORCID Board. Her passion is to unlock the power of data to create public value, in research and education and across government and the community sectors, driving innovation, productivity improvements and improved policy outcomes.\nLee Harris has over 20 years of experience in the Australian Public Service with including over 14 years in research programs and policy including NCRIS. Key achievements include the 2015 Research Infrastructure Review, 2016 NRI Roadmap and the 2018 and 2020 Research Infrastructure Investment Plans. Joining the ARC in 2020, he worked on the ERA EI Review and preparations for ERA 2023. Since their discontinuation, his work has focussed on the development of a new approach to national research evaluation, improving the ARCs approach to Open Access and the use of Persistent Identifiers to inform insights into the Australian research sector.\nResearch evaluation at the National and program level is increasingly looking for ways to increase the quality and availability of data about the research ecosystem. Persistent Identifiers (PIDs) are powerful tools to link research elements and accelerate Australian research quality, efficiency and impact. However, for PIDs to achieve their potential there must be collective buy in and action from the research sector. The National PID Strategy and Implementation Road Map has provided a clear narrative that highlights the benefits of PIDs to multiple stakeholders both within government and the university sector.\nThe availability and quality of data about research elements is critical for funding institutions to evaluate research outcomes and demonstrate return on investment. Current practices in Australian research mean that data is often limited, may not be accessible, and may be of a low quality or missing key information. As policy makers, funders can drive change in the research sector by establishing funding requirements that influence institutions and individuals. The National PID Strategy has provided opportunities to engage across the sector, providing insights into potential actions and challenges in the research data ecosystem space. It has provided a common language, key priorities, and clear aims to guide policy development and PID use."
    },
    {
        "day": "Tuesday",
        "time": "12:40 – 14:10",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet",
        "title": "Lunch, Exhibition, Poster Viewing | Boulevard Foyer",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Tuesday",
        "time": "12:50 – 13:50",
        "location": "Boulevard B1",
        "session_chair": "Luc Betbeder-Matibet",
        "title": "Lunchtime Networking BoF Session – Charting a path towards a sustainable community of people who work on Research Software",
        "authors": "Peter Marendy",
        "url": "https://conference.eresearch.edu.au/charting-a-path-towards-a-sustainable-community-of-people-who-work-on-research-software/",
        "details": "Rowland Mosbergen1, Dr. Manodeep SInha2, Mercedes Randell3\n1WEHI, Australia,2Sorsery Consulting, Australia,3NeSI, New Zealand\nRowland has 25 years of experience as a generalist in highly complex environments in the not-for-profit, private, and research sectors and is a Research Software Engineer since 2010.\nOver his career he has delivered over $230 million of value in business improvements, recruited 4 multi-disciplinary teams and grown 2 more, and has a strong sales and marketing background. Building an inclusive vision and creating buy-in with marginalised stakeholders within complex environments is his specialty.\nPlease join us to raise awareness within eResearch of the community of people who work on Research Software, the Research Software Engineer Australia New Zealand (RSE-AUNZ) community, and to understand the needs of the community.\nThis BoF would be of interest to people who:\n– Develop research software and are worried about career progression,\n– Want to make the research software more visible,\n– Streamline and scale research workflows but feel undervalued,\n– Balance researcher needs with good software practices, and\n– Make research software easier to use.\nIt would also be useful to those who manage RSEs or who struggle to recruit or maintain RSEs within their project lifecycles, and those who would like to help sustain this community.\nObjectives would be to solicit feedback and discussion about the following topics:\n– What are the needs of the community and is the RSE-AUNZ meeting those needs?\n– What can we do to get more active community participation? Eg. proactively sharing knowledge, joining the steering committee, or asking for help?\n– Do we need to apply for grants to help sustain the community, as per the US-RSE?\n– How can we ensure greater equity and inclusion within the RSE-AUNZ community to better support and represent all members?"
    },
    {
        "day": "Tuesday",
        "time": "14:10 – 14:30",
        "location": "Boulevard Auditorium",
        "session_chair": "Robert Shen",
        "title": "BridgingtheGap: A Research-Ready AI/ML Infrastructure on the Nectar Research Cloud",
        "authors": "Glen Charlton",
        "url": "https://conference.eresearch.edu.au/bridging-the-gap-a-research-ready-ai-ml-infrastructure-on-the-nectar-research-cloud/",
        "details": "Mr Glen Charlton1, Long Le1, Dr Jiaxin Fan1, Dr Anastasios Papaioannou2, Andy Botting3, Meirian Lovelace-Tozer3, Ben Chiu3\n1Intersect Australia, Sydney, Australia,2University of Technology Sydney, Sydney, Australia,3Australian Research Data Commons, Australia\nGlen is the Lead Data Scientist for the Advanced Analytics & AI (3AI) Platform at Intersect Australia. The 3AI team empowers researchers with data science and AI through hands-on support and the transfer of knowledge to improve the efficiency and increase the capability of researchers to conduct novel and practically relevant research. Glen actively promotes the novel and responsible adoption of AI for both research and operational purposes within members, partners and internally within Intersect.\nBackground\nResearchers and scientists are increasingly using programming languages for data processing, visualisation, and analysis. Advancement in machine learning (ML) and artificial intelligence (AI) helps accelerate the process of analysing complex research data and conducting experiments, leading to the discovery of hidden patterns in the data. In collaboration with the Australian Research Data Commons (ARDC), the Advanced Analytics and AI (3AI) Platform at Intersect has aimed to reduce the technical barriers and provide a research-ready AI/ML infrastructure on the Nectar Research Cloud, allowing researchers to focus on their science.\nMethod/Actions\nThe modular design provides a pre-configured Python environment, DataOps, MLOps, and system monitoring tools like Apache Airflow, MLflow, Prometheus, and Grafana. Interactive software such as RStudio and JupyterLab are included. Researchers can customise their environment with specialised Python installations optimised for tasks like computer vision or generative AI.\nResults\nThe initiative offers pre-configured, optimized environments, simplifying cloud computing for AI/ML. Researchers can quickly and efficiently begin their AI/ML journey and utilise the available resources (e.g. Nectar’s Graphical Processing Unit (GPU) Reservation System). The platform provides essential tools for data analysis, workflow orchestration, and model development within a user-friendly framework, ensuring optimal performance.\nConclusion\nThis collaborative effort empowers researchers to leverage AI/ML tools by providing a user-friendly, documented environment. It aims to accelerate AI/ML research on the Nectar Research Cloud, enhancing the Australian research landscape by providing accessible and powerful computational resources."
    },
    {
        "day": "Tuesday",
        "time": "14:10 – 14:30",
        "location": "Boulevard B1",
        "session_chair": "Olivia Davies",
        "title": "The AuScope Open Science Ar-Ar Data Pipeline: From Instrument to Earthbank",
        "authors": "Bryant Ware",
        "url": "https://conference.eresearch.edu.au/the-auscope-open-science-ar-ar-data-pipeline-from-instrument-to-earthbank/",
        "details": "Dr Bryant Ware, Dr Hayden Dalton2, Ms Ashley Savelkouls2, Dr Angus Nixon3\n1Curtin University, Perth, Australia,2The University of Melbourne, Melbourne, Australia,3The University of Adelaide, Adelaide, Australia\nBryant Ware is a Research Fellow at the John de Laeter Centre, Curtin University, Perth, Western Australia, and the coordinator of the AuScope Geochemistry laboratory Network (AGN). He leads a national team of analytical geochemists to enhance the capability and capacity of geochemistry laboratories across Australia. His work focuses on advancing geochemical data management, international best data reporting practices, and laboratory workflow developments. Within the AGN he oversee’s key initiatives within AGN, including the development of the AuScope EarthBank platform, outreach efforts, and fostering a FAIR (Findable, Accessible, Interoperable, Reusable) geochemistry data ecosystem. My strategic objectives include improving laboratory capabilities, developing new analytical techniques and data management workflows, standardising geochemistry data, supporting user access, and promoting best practices in data management.\nAs the volume of geochemical data grows, driven by advanced instrumentation, improved digital infrastructure, and a global push toward Open Science, there is an increasing need for clear community guidelines, standardised reporting practices, and shared vocabularies. The ⁴⁰Ar/³⁹Ar geochronology technique, widely used to determine the timing of geological events throughout Earth’s history, generates complex datasets that require detailed documentation and correction before they can be interpreted or reused. For meaningful reuse, these datasets often need recalculating or recalibrating using updated reference and/or decay constant values. While the ⁴⁰Ar/³⁹Ar geochronology community has developed recommended reporting formats (e.g., Schaen et al. 2021), adoption can be difficult in practice. A key barrier is the lack of simple, accessible digital tools to move data from laboratories into structured formats that are consistent, discoverable, and easy to share. Existing systems often rely on manual data entry and fail to capture core outputs from analytical instruments. To address this, two labs in the AuScope Geochemistry Network are implementing: (1) open-source, automated data capture tools (e.g., Pychron) that allow direct upload from instruments to trusted platforms like the AuScope Data Repository and EarthBank platform, and (2) streamlined workflows for collecting essential metadata from researchers and analytical facilities without adding extra steps to already complex workflows. These initiatives aim to make ⁴⁰Ar/³⁹Ar data more accessible, reusable, and aligned with Open Science practices. By improving digital infrastructure at the point of data generation, this work supports more transparent, efficient, and collaborative geoscience research."
    },
    {
        "day": "Tuesday",
        "time": "14:10 – 14:30",
        "location": "Boulevard B2",
        "session_chair": "Darya Vanichkina",
        "title": "Developing a Responsible Use of Generative AI Module for Researchers",
        "authors": "Shannon Taylor",
        "url": "https://conference.eresearch.edu.au/developing-a-responsible-use-of-generative-ai-module-for-researchers/",
        "details": "Dr Shannon Taylor1\n1The University of Sydney, Camperdown, Australia\nShannon Taylor is a Research Data Consultant at The University of Sydney. Shannon engages with researchers, professional and faculty staff to assist with research data concerns. She also produces content and resources to provide uplift for researchers.\nAs generative AI tools become increasingly embedded in research workflows and innovative approaches, universities face growing pressure to support researchers in their ethical and responsible use of generative AI (Gen AI) tools. In response to rising uncertainty and numerous inquiries from researchers, a project was initiated to develop an interactive training module tailored specifically to the research community. The module focuses on responsible Gen AI use across disciplines, aiming to foster informed decision-making through a non-prescriptive, reflective approach rather than enforcing simple compliance.\nTo ensure the module was relevant and effective, a program of research and outreach was undertaken to explore existing AI use and perceptions. The objective was to strike a balance between highlighting opportunities and addressing risks. Stakeholder consultations and focus groups helped uncover knowledge gaps, areas of uncertainty, and real-world concerns. Researchers consistently expressed a need for practical, non-technical guidance to navigate legal, ethical, and reputational risks. Feedback was gathered through three iterative rounds involving both Higher Degree by Research students and academic staff.\nKey outcomes of the project included not only the development of the training module, but also the creation of practical institutional resources and guidelines identified as necessary by participants.\nThis presentation will outline the development process, challenges, and strategies for creating effective, researcher-led training in response to emerging technologies. The model presented offers a framework that can be adapted by other institutions in similar contexts."
    },
    {
        "day": "Tuesday",
        "time": "14:10 – 14:30",
        "location": "Boulevard B3",
        "session_chair": "Michael Lynch",
        "title": "From Register to Real-World Use: Enhancing the Vocabulary Register for Search, Access, and Integration",
        "authors": "Rana Raad",
        "url": "https://conference.eresearch.edu.au/from-register-to-real-world-use-enhancing-the-vocabulary-register-for-search-access-and-integration/",
        "details": "Mrs. Rana Raad1\n1Geoscience Australia, Symonston, Australia\nRana Raad is a Data Catalogue Officer at Geoscience Australia (GA) with a background in geology and a Master’s degree in Earth and Environmental Sciences. She began her career in exploration and mining before transitioning into data management, where she now focuses on enhancing the accessibility and usability of geoscientific data. Rana manages metadata quality and standards implementation within the Enterprise Catalogue, provides expert advice on metadata creation and maintenance, and supports the development of GA's metadata profile. She also oversees the organisation’s vocabulary register and supports data cataloguing and publishing workflows.\nControlled vocabularies are critical for ensuring consistent, searchable, and interoperable metadata and data across digital platforms. As part of ongoing enhancements to our vocabulary register at Geoscience Australia, we are publishing our vocabularies using an existing data catalogue process and assigning persistent identifiers (PIDs) to each concept. These PIDs now enable hyperlinks across our data portals; allowing users to access definitions of concepts directly in context. This integration supports data interpretation, improves usability, and reinforces adherence to the FAIR principles (Findable, Accessible, Interoperable, Reusable).\nWe also recognised the need to review and update the vocabulary register to include high-level themes that facilitate some faceted searches, making it easier for users to find relevant concepts by science domain. Additionally, we identified the potential of enhancing the register by including visual image examples alongside concept definitions.\nSubstantial improvements have already been made, and ongoing development remains essential to ensure terms are clearly understood and consistently applied by researchers, data managers, and other stakeholders. By enhancing the accessibility and usability of our vocabulary register, we are promoting accurate data use and strengthening trust in the information presented throughout our systems."
    },
    {
        "day": "Tuesday",
        "time": "14:10 – 14:30",
        "location": "Boulevard Room",
        "session_chair": "Gin Tan",
        "title": "Shallow budgets, deep impact: Repurposing REDCap for digital intervention research",
        "authors": "Shaun Grady",
        "url": "https://conference.eresearch.edu.au/shallow-budgets-deep-impact-repurposing-redcap-for-digital-intervention-research/",
        "details": "Mr Shaun Grady1\n1Hunter Medical Research Institute, New Lambton Heights, Australia\nShaun Grady is a Senior Health Informatician within the Data Management & Health Informatics stream at Hunter Medical Research Institute and manages the Data Sciences Research Training Program. Shaun is originally a Radiation Therapist by training with specific research interests in digital health delivery, clinical process improvement and utilisation of health data.\nBackground\nDigital tools are integral to the delivery of research and health interventions, however many researchers face both budgetary and technical barriers that limit their ability to either or adopt or create their own software for this purpose. This presentation outlines how REDCap can be repurposed to deliver high-quality digital interventions at scale in a research context. Originally designed for health data collection, REDCap offers powerful features such as automated survey delivery, branching logic, and secure participant tracking, making it an ideal platform for both research and behavioural health programs.\nActions and Outcomes\nDrawing on examples from multiple studies, we demonstrate how REDCap has been adapted to deliver interactive interventions, engaging educational content, and tailored messaging to research participants, without the need for custom software development. We also discuss strategies for overcoming common limitations, including user interface constraints and mobile responsiveness, by integrating REDCap with simple external tools.\nThis session will be particularly valuable for research teams, health service and infrastructure managers, and digital health developers working in resource-constrained environments. Attendees will leave with practical implementation tips and a renewed perspective on how to extract maximum value from existing infrastructure to support digital health equity, scale, and sustainability.\nConclusion\nBy reimagining tools already within reach, we can reduce development time and cost, while maintaining all the rigour, security, and participant engagement required in research."
    },
    {
        "day": "Tuesday",
        "time": "14:30-14:50",
        "location": "Boulevard Auditorium",
        "session_chair": "Robert Shen",
        "title": "Securely Share Your Bare-Metal HPC and AI Cluster without Virtualisation: Modern Multi-tenancy Solutions",
        "authors": "Werner Scholz",
        "url": "https://conference.eresearch.edu.au/securely-share-your-bare-metal-hpc-and-ai-cluster-without-virtualisation-modern-multi-tenancy-solutions/",
        "details": "Dr Werner Scholz1\n1XENON Systems, Springvale, Australia\nDr. Werner Scholz is CTO and Head of R&D at XENON Systems, a consultancy, solutions and services provider for High Performance Computing, Deep Learning, Artificial Intelligence, data storage and data management solutions.\nWerner is leading a team of dedicated, experienced, and highly skilled solutions architects and engineers working on systems for some of the largest supercomputing, AI, cloud, and research institutes in Australia and the APAC region.\nWerner graduated with Master's and PhD degrees in Physics from the Vienna Univ. of Technology, Austria, where he developed a widely used MPI parallel open-source simulation package for magnetic materials.\nBefore joining XENON Systems, Werner led a team of engineers at Seagate Technology in the US where he developed Heat Assisted Magnetic Recording technologies for next generation hard disk drives and managed Seagate's HPC infrastructure.\nCompute clusters provide efficient solutions for High Performance Computing (HPC) workloads, Artificial Intelligence (AI), and cloud services. The largest supercomputers, AI platforms, and cloud services in the world are all designed as clusters with thousands of compute nodes, fast networks, and centralised management, which allows relatively small teams of engineers to manage and maintain them. These clusters are designed as multi-user systems for hundreds of users and thousands of concurrent jobs. However, securely separating users/projects/tenants and workloads and accommodating workloads of different security classification is a challenge. Typically, this is solved using virtualisation solutions, but they incur significant overheads, performance penalties, and configuration challenges.\nIn this presentation, we will discuss alternative multi-tenancy solutions for bare-metal HPC and AI Clusters which provide the security of hard network segregation while maintaining full bare-metal performance. In addition to the compute nodes and Ethernet and Infiniband network fabrics, appropriate storage systems also support multiple tenants, which results in complete compute-network-storage cluster infrastructures, which can be securely shared between different teams and organisations while maintaining strict separation of the tenants."
    },
    {
        "day": "Tuesday",
        "time": "14:30-14:50",
        "location": "Boulevard B1",
        "session_chair": "Olivia Davies",
        "title": "How and where are you preserving your raw machine observable data for future science?",
        "authors": "How and where are you preserving your raw machine observable data for future science?",
        "url": "https://conference.eresearch.edu.au/how-and-where-are-you-preserving-your-raw-machine-observable-data-for-future-science/",
        "details": "Dr Angus Nixon1,2, Dr Bryant Ware2,3, Prof Lesley Wyborn4,5\n1Department of Earth Science, The University of Adelaide, Adelaide, Australia,2AuScope Geochemistry Network, Australia,3John de Laeter Center, Curtin University, Bentley, Australia,4Australian National University, Canberra, Australia,5Australian Research Data Commons, Australia\nAngus completed a PhD in Geology examining the low-temperature evolution of northern Australia and the relationship between plate boundary tectonics and structural and chemical signatures in the continental interior. He subsequently joined the AuScope Geochemistry Network (AGN) and is currently an EarthBank Fellow at the University of Adelaide, and co-chair of the OneGeochemistry initiative. His work includes establishing standards for areas including fission track thermochronology, inorganic geochemistry, beta-decay geochronology systems, as well as integrating data infrastructure with research community needs and workflows.\nPertinent in a wide range of scientific disciplines is the advent of ‘next generation’ instrumentation routinely collecting data at up to 16 times current data volumes. These data stem from a wide array of data types (including images), acquisition methods, analytical scales, sample or observed materials, and research applications. While many disciplines are developing best practice reporting schema and vocabularies for the data outputs, these are typically restricted to processed data most suitable for end user interaction, application and publication. Primary Observable Datasets (PODs), the fundamental observations and analyses underpinning the published data, generally remain inaccessible and unlinked to the published outcome, limiting opportunities for future repurposing or recalculation with evolving techniques and knowledge. This session will explore how frequently PODs remain uncaptured across disciplines, what forms of PODs are generated, and if existing systems for capturing PODs may inform or be translated to other domains. We invite a diverse audience from across the research ecosystem, from researchers who utilise the final data products (are there instances that PODs data has been required, yet unavailable/lost?), analytical facilities which generate data, databases and repositories, and research or data communities and infrastructure to attend and contribute in discussions. The session will be conducted as an open-floor discussion with attendees led by representatives of AuScope, Atlas of Living Australia, and the Australian Plant Phenomics Network. Key takeaways will inform data infrastructures on the state of PODs storage, necessity and complexity across disciplines, and how this may be addressed to benefit the community."
    },
    {
        "day": "Tuesday",
        "time": "14:30-14:50",
        "location": "Boulevard B2",
        "session_chair": "Darya Vanichkina",
        "title": "Driving AI adoption in research",
        "authors": "Driving AI adoption in research",
        "url": "https://conference.eresearch.edu.au/driving-ai-adoption-in-research/",
        "details": "Dr Darya Vanichkina1, Dr Glen Charlton2, Dr Kyle Hemming3, Dr Benjamin Goudey4, Dr Anastasios Papaioannou5, Dr Julie Iskander6, Dr Patrick Tung7\n1Sydney Informatics Hub (SIH), Core Research Facilities, University of Sydney, Camperdown, Australia,2Advanced Analytics & AI (3AI) Platform, Intersect Australia, Sydney, Australia,3University of Auckland, Auckland, New Zealand,4Australian BioCommons, University of Melbourne, Parkville, Australia,5eResearch Platforms and Services, University of Technology Sydney, Ultimo, Australia,6Walter and Eliza Hall Institute (WEHI), Parkville, Australia,7Research Technology Services, University of New South Wales, Randwick, Australia\n​​Darya Vanichkina PhD SFHEA is the Data Science & AI Group Lead at the Sydney Informatics Hub, a University of Sydney Core Research Facility dedicated to enabling excellence in data and compute-intensive research. Darya leads a team that delivers consultancy services and training to boost research outcomes and funding, accelerate projects, and foster partnerships with industry and government. In 2025, her team is championing AI adoption for research across the University’s faculties and affiliates.\nhttps://orcid.org/0000-0002-0406-164X\nDr Glen Charlton is the Lead Data Scientist for the Advanced Analytics & AI Platform at Intersect Australia. The team empowers researchers with data science and AI through hands-on support and knowledge transfer to improve efficiency and increase capability of researchers to conduct novel, practically relevant research. Glen actively promotes novel and responsible adoption of AI for both research and operational purposes within members, partners and internally within Intersect.\nhttps://orcid.org/0000-0002-1482-9720\nDr. Kyle Hemming is a Senior eResearch Engagement Specialist at the University of Auckland, supporting researchers in data science, data management, and responsible AI adoption. With a decade of quantitative research experience and eight years supporting researchers, he is passionate about improving research outcomes. His interests also include reproducible research, strategic planning, and stakeholder engagement.\nhttps://orcid.org/0000-0001-5913-3981\nDr Benjamin Goudey is AI Technical Lead at Australian BioCommons, a national digital infrastructure capability that provides access to tools, methods, and training for life science researchers. His role focuses on improving national AI capabilities and reducing barriers to adoption across life science research. Ben brings extensive experience from industry and academia in AI application and predictive modelling to a range of biological domains, evidenced by a strong record of publications and patents.\nhttps://orcid.org/0000-0002-2318-985X\nAnastasios Papaioannou is a Senior Manager leading the eResearch Platforms and Services at UTS, where he oversees cloud and high-performance computing, research data storage, training, and the integration of AI in research. He actively works on the adoption of AI by collaborating with UTS colleagues to develop a comprehensive suite of AI guidelines, tools, and training programs for researchers. With a strong background in research, data science, and computational physics, he works closely with academics and HDR students to help them leverage large-scale infrastructure and digital technologies to accelerate research.https://orcid.org/0000-0002-8959-4559\nDr Julie Iskander leads the Research Computing Platform at WEHI, where she brings her background in software engineering, computational biology, and biomedical research to drive infrastructure development and support scalable, data-driven science. Her team helps researchers bridge the gap between scientific questions and computational solutions, with growing emphasis on making AI tools and cloud platforms accessible across disciplines.\nhttps://orcid.org/0000-0002-3426-4376\nDr. Patrick Tung is the AI Imaging Scientist at UNSW’s Research Technology Services. He collaborates across disciplines to apply AI in computer vision, particularly for 3D imaging and materials and medical analysis. With a background in tomography and diffraction, he has held postdoctoral roles in Australia and Europe. His research focuses on deep learning for structural analysis in materials for energy systems and circular economies, contributing to innovations in imaging techniques and computational modeling.\nhttps://orcid.org/0000-0002-2741-3177\nSituation\nArtificial Intelligence (AI) tools are widely available and are transforming how knowledge work is conducted globally, yet AI adoption in research across Australasia remains somewhat limited. Researchers face a landscape of challenges that limit the adoption of AI in the research lifecycle, including concerns around data privacy and sovereignty, ethical oversight, accuracy and reproducibility, bias, intellectual property management and copyright, infrastructure limitations, the environmental impact of large-scale AI models, and more.\nTask\nIn this BoF session, we will discuss these barriers through the lens of real-world experiences from our institutions. We will share case studies that demonstrate how some of these challenges have been addressed through innovative solutions, guidelines, and awareness. We will also highlight how emerging tools and ready-to-use infrastructure can lower the barrier to entry for researchers and provide a safe environment to use and experiment with AI.\nAction\nOur goal is to coordinate a collaborative discussion on how cross-institutional initiatives may benefit wider AI adoption in research. Together, we will identify opportunities for cross-institutional initiatives to accelerate AI literacy, infrastructure readiness, and policy/guidelines alignment across the region.\nResult\nWhether you are an AI enthusiast or a cautious sceptic, this session offers a space to share experiences, exchange ideas, and learn from one another, so we can better support researchers and collectively advance the adoption of AI in research."
    },
    {
        "day": "Tuesday",
        "time": "14:30-14:50",
        "location": "Boulevard B3",
        "session_chair": "Michael Lynch",
        "title": "Sustainable Digital Humanities Collections",
        "authors": "Sustainable Digital Humanities Collections",
        "url": "https://conference.eresearch.edu.au/sustainable-digital-humanities-collections/",
        "details": "Mr Michael Lynch1, Ms Eve Ansell2, Dr Nichola Burton3, Dr Ian McCrabb4, Dr Peter Sefton5\n1The University of Sydney, Sydney, Australia,2Queensland Cyber Infrastructure Foundation, St Lucia, Australia,3Australian Research Data Commons, Caulfield East, Australia,4Systemik Solutions, Sydney, Australia,5The University of Queensland, Brisbane, Australia\nMike Lynch (https://orcid.org/0000-0001-5152-5307) is a data science group lead at the Sydney Informatics Hub, with expertise in research data management, open standards for research data and the application of modern IT development and deployment practices to research software.\nEve Ansell (https://orcid.org/0000-0002-3467-9782) is the HASS Data Science Lead for QCIF. With a background in linguistics and a Graduate Digital Research Fellowship with the Australian Text Analystics Platform, she is a strong advocate for digital tools for humanities researchers.\nNichola Burton (https://orcid.org/0000-0003-4470-4846) is the Programs Architect (HASS and Indigenous Research Data Commons) for the Australian Research Data Commons, with a background in psychology research. She works with the HASS and Indigenous research communities to find out what their data and digital research requirements are and develop infrastructure to meet their needs.\nIan McCrabb is the founder and managing director of Systemik Solutions, a Sydney-based IT consulting group focused on open-source digital humanities platforms and research sites.\nPeter Sefton (https://orcid.org/0000-0002-3545-944X) is an eResearch expert, specialising in software development, research data management and metadata, currently leading the technology and infrastructure team for the Language Research Data Commons project at the University of Queensland.\nDigital platforms and technologies allow humanities researchers to curate new collections and work with archival collections in new and exciting ways. However, keeping these collections available for future researchers is difficult – they often become the hostages of legacy platforms and are dependent on the efforts of small teams in one institution. The ARDC’s Community Data Lab is attempting to foster a cooperative national approach to working with HASS and Indigenous data.\nThis BoF is an opportunity for technologists, librarians and researchers to share the challenges of supporting researchers in the digital humanities in a sustainable way.\nParticipants will be invited to identify collections or platforms in their institutions which would benefit from tools and platforms being developed at a national level, discuss requirements from researchers which are not being currently met – whether these are technical, organisation, or capabilities – discuss how standard vocabularies, schemas and migration practices can help build a sustainable digital humanities commons, and share approaches which have been successful in helping humanities researchers to engage with digital technologies.\nThe session will conclude with a discussion of practical steps we can take which will help participants maintain engagement with their peers in other institutions and in national funding bodies."
    },
    {
        "day": "Tuesday",
        "time": "14:30-14:50",
        "location": "Boulevard Room",
        "session_chair": "Gin Tan",
        "title": "Future-proofing the digital research infrastructure workforce",
        "authors": "Future-proofing the digital research infrastructure workforce",
        "url": "https://conference.eresearch.edu.au/future-proofing-the-digital-research-infrastructure-workforce/",
        "details": "Ms Gin Tan1, Kathryn Unsworth6, Juliana Villa-Ortiz1, Dr. Sara King7, A/Proc Nic Geard2, Dr. Danny Meloncelli3, Dr. Maciej Cytowski4, Dr. Gordon McDonald5\n1Monash University, Australia,2University of Melbourne, Australia,3Queensland Cyber Infrastructure Foundation (QCIF), Australia,4Pawsey Supercomputing Research Centre, Australia,5University of Sydney, Australia,6Australian Research Data Commons (ARDC), Australia,7Australian Academic and Research Network (AARNet), Australia\nGin Tan is the Associate Director of Monash eResearch. She began her career in both enterprise and research computing, eventually finding her true passion in high-performance computing (HPC).Over the past eight years at Monash University, Gin has been instrumental in the development and deployment of a new HPC cluster, and has since grown into a technical leadership role. She enjoys diving deep into the technical aspects of her work while also excelling at translating complex needs into practical, effective solutions.\nDr Sara King is the Training and Engagement Lead for AARNet. She is focused on outreach within the research sector, developing communities of interest around training and skills development, especially infrastructure and digital literacies.\nA/Prof Nic Geard is the Director of the Melbourne Data Analytics Platform at the University of Melbourne, focused on the provision of collaborative, interdisciplinary support for digital and data-led research.\nDr Danny Meloncelli is the Head of Skills Development at QCIF Ltd, overseeing the development and delivery of courses that equip researchers with critical skills in coding, statistics, data management and bioinformatics.\nMaciej Cytowski is a computational scientist and expert in high-performance computing and data science. He specialises in designing and implementing computational strategies for complex scientific problems, particularly in fields like computational physics, computational biology, weather, climate, and artificial intelligence. Currently, he is the Head of Scientific Services at the Pawsey Supercomputing Research Centre in Perth, Western Australia.\nDr Gordon McDonald leads the Informatics Team at the University of Sydney’s Sydney Informatics Hub, which supports researchers through data science, AI, bioinformatics, statistics and HPC services and training. He works at the intersection of technology and strategy, helping build sustainable, future-ready research infrastructure. Gordon is particularly focused on national-scale research tools, and creating teams that can meet the changing demands of data-intensive research together with our NCRIS partners such as the Australian Biocommons and the Australian Plant Phenomics Facility.\nJuliana Villa is the Research Community Specialist at Monash eResearch. She works closely with researchers to understand their digital and infrastructural challenges and to support their evolving needs. With over a decade of experience in project management, customer engagement, and stakeholder relations across both the Australian research and international education sectors, Juliana brings a strategic and people-focused approach to her work. Her expertise includes strategic planning, stakeholder engagement, governance, and cross-sector collaboration.\nKathryn leads the Skilled Workforce Development team at the Australian Research Data Commons (ARDC). Her role involves driving a nationally coordinated approach to digital research skills initiatives, targeting areas of the workforce that conduct, underpin, and enable data-intensive research, including researchers (users of DRI) and the DRI workforce (providers of DRI). She guides and facilitates the ARDC Skills team in identifying, evaluating and scoping skills development opportunities, then planning and implementing their delivery in alignment with and support for the ARDC’s strategic priorities and activities, which encompass the thematic Research Data Commons, national cloud, national information infrastructure and national leadership in Digital Research Infrastructure (DRI) skills.\nHiring and retaining top talent in the digital research infrastructure has never been more challenging. With industry giants offering attractive salaries and benefits, how can we, research infrastructure providers supporting research, compete in the current market? More importantly, how do we build a pipeline of skilled professionals who understand the unique challenges and requirements of digital research infrastructure?\nOutcome 1 of Australia’s National Digital Research Infrastructure (NDRI) Strategy highlights that by 2030, the Australian NDRI system should be supported by structured training frameworks for both researchers and the National Research Infrastructure (NRI) workforce. It calls for a coordinated and consistent approach to help NDRI providers expand training opportunities in relevant digital skills, not only for their users but also for their own staff, as a means to address ongoing skills gaps and workforce shortages.\nThis session proposes a multi-institutional collaboration, bringing together managers, engineers, and trainers working across digital research computing and infrastructure development and delivery. Together, participants will explore practical, scalable strategies for talent development. The aim is to co-design a collaborative and sustainable 12-month action plan to attract, train, and retain the next generation of digital research infrastructure professionals."
    },
    {
        "day": "Tuesday",
        "time": "14:50 – 15:10",
        "location": "Boulevard Room",
        "session_chair": "Gin Tan",
        "title": "Modern Storage for Modern HPC and AI Environments",
        "authors": "Ben Trinder",
        "url": "https://conference.eresearch.edu.au/modern-storage-for-modern-hpc-and-ai-environments/",
        "details": "Ben Trinder1\n1XENON Systems, Springvale, Australia\nBen joined XENON Systems in 2024 as a Solutions Architect. Prior to this, he was with Quantum Storage providing Professional Services across Australia and New Zealand.\nAnd before that, Ben worked in film and television for 20+ years, including 12 years at Endemol Shine – looking after the storage, networks and software used to create shows such as MasterChef, Ninja Warrior, Married at First Sight and many others.\nBen has experience working at vendors, at the customer side as well as in the integrator space, giving him the knowledge and skills to navigate the challenges of implementing solutions to fit budgets, deadlines and future needs.\nWith a strong focus on customer requirements, Ben is an “on the tools” engineer, who is ready to take a solution from initial discussions through design, implementation and ongoing support.\nIn today's modern landscape, the requirements and challenges of adding storage to an HPC environment have evolved.\nNetworks are 800G and beyond. HPC data sets are shared or sourced across the WAN. Environments are stretched from data centres to the cloud.\nTraditional spinning disks with hardware RAID controllers presenting block storage to individual servers may not be suitable for your modern HPC environment.\nStorage demands continue to grow faster than storage capacities – management and archiving is needed. How do you ensure only the right data is using up your valuable storage? Do you implicitly archive all your data using automation, or do you explicitly archive only the data when you choose?\nThis session covers how storage has evolved, with Flash NAND speeds approaching that of system RAM, shared-nothing architectures, Kubernetes-defined storage platforms, archive workflows and more.\nFrom NVMe storage in individual servers, on-premises clustered Flash arrays to software-defined storage that spans multiple DCs, we discuss how the landscape has changed and what technologies can solve these modern challenges."
    },
    {
        "day": "Tuesday",
        "time": "15:10 – 15:30",
        "location": "Boulevard Room",
        "session_chair": "Gin Tan",
        "title": "Mastering the Maze: Simplifying HPC, Kubernetes, and Secure Software Delivery with DevOps, GitOps, and Platform Engineering",
        "authors": "Malcolm Haak",
        "url": "https://conference.eresearch.edu.au/mastering-the-maze-simplifying-hpc-kubernetes-and-secure-software-delivery-with-devops-gitops-and-platform-engineering/",
        "details": "Adrian Torrie1\nPresented by Malcolm Haak\n1XENON Systems, Springvale, Australia\nAdrian is a skilled solutions architect with broad experience building and maintaining data systems from the ground up.\nHe has broad skills across all IT domains, with specific depth in automating data analytics/machine learning/artificial intelligence solutions in hybrid environments, along with the necessary skills for training others in this specialty.\nAdrian’s last role had the primary focus of enabling self-service, via Platform Engineering, for Data Scientists and Data Engineers, allowing rapid iteration of models and their release.\nHe excels in delivering secure solutions for large scale, high performance computational requirements using modern automation approaches.\nThe intricate and complex modern research environments, their interconnected software supply chains, and critical security demands requires more than incremental improvements. This presentation dives into this challenging landscape, first illuminating its multifaceted nature, then detailing how the strategic application of DevOps, GitOps, and Platform Engineering principles distills this complexity into manageable, simplified frameworks. We will demonstrate how these approaches create resilient and agile research infrastructure through automated, reproducible, and auditable management.\nAttendees will gain crucial insights into how these methodologies demystify the complex interplay between advanced infrastructure, software lifecycle management, and robust security. Key focus areas include leveraging GitOps to simplify declarative control over HPC and Kubernetes, strategies for taming software supply chain intricacies for research integrity, and embedding practical, yet comprehensive, security. Discover how Platform Engineering can provide."
    },
    {
        "day": "Tuesday",
        "time": "15:30 – 16:00",
        "location": "Boulevard Auditorium",
        "session_chair": "Robert Shen",
        "title": "Afternoon Tea, Exhibition, Poster Viewing | Boulevard Foyer",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Tuesday",
        "time": "16:00 – 16:20",
        "location": "Boulevard Auditorium",
        "session_chair": "Ai-Lin Soo",
        "title": "Cultivating Data Stewardship in the Australian Grains Sector: Early Outcomes from the DataHarvest Project",
        "authors": "Erin Elstermann",
        "url": "https://conference.eresearch.edu.au/cultivating-data-stewardship-in-the-australian-grains-sector-early-outcomes-from-the-dataharvest-project/",
        "details": "Mrs. Erin Elstermann1, Ms Melanie Dixon1, Mr John Brown1, Dr Fatima Naim1\n1Curtin University, Australia\nErin Elstermann is the Data Management Officer for the DataHarvest Project at Curtin University’s Centre for Crop and Disease Management. With a Master’s in Biotechnology from the University of Western Australia and a background in plant science and industry experience, Erin supports researchers in the Australian grains industry with research, development, and extension (RD&E) data best practices.\nEffective Research Data Management (RDM) is essential in agriculture, where diverse datasets, from field experiment design to soil composition to complex plant physiology and genomics, drive innovation and sustainable practices. In the Australian grains industry, challenges such as inconsistent practices, varying levels of data literacy, and limited organisational support hinder effective RDM. Well-executed RDM can reduce data silos, enhance reproducibility, and facilitate collaboration, helping translate research into practical, scalable outcomes for growers and the industry.\nThe DataHarvest project, funded by the Grains Research and Development Corporation (GRDC), aims to assess the current state of RDM within the Australian grains sector and strengthen its research partners capacity and capability through targeted support and practical training.\nThrough the delivery of national workshops, we have engaged researchers, data managers, and institutional stakeholders to identify gaps and priorities. In response, we developed a suite of practical resources covering Data Management Plans (DMPs), metadata standards, agricultural vocabularies, and data access conditions and licensing. To encourage adoption, we’ve produced step-by-step instructional videos and continue to offer ongoing support to GRDC partners through a dedicated Support Services Centre.\nThis presentation will highlight key DataHarvest activities and outcomes to date, including developed resources, the evolution of DMP standards through compliance reporting, and partner feedback. Findings reveal a fragmented but improving RDM landscape, with growing awareness of best practices and a strong demand for practical, discipline-specific support. The early outcomes of DataHarvest demonstrate that accessible resources and sustained guidance are critical for strengthening data stewardship in agricultural research."
    },
    {
        "day": "Tuesday",
        "time": "16:00 – 16:20",
        "location": "Boulevard B1",
        "session_chair": "Liz Stokes",
        "title": "Architecting Research Data Experiences for Scale: Aligning Infrastructure Intelligence with Data Taxonomy in a Distributed, Resource-Constrained Research Landscape",
        "authors": "Benjamin Wu",
        "url": "https://conference.eresearch.edu.au/architecting-research-data-experiences-for-scale-aligning-infrastructure-intelligence-with-data-taxonomy-in-a-distributed-resource-constrained-research-landscape/",
        "details": "Mr Benjamin Wu1\n1Netapp, North Sydney, Australia\nBen Wu is Executive Architect for Business Consulting at NetApp, where he leads data strategy engagements across research and higher education institutions in Australia and the wider Asia-Pacific region. With a background in systemic optimisation and digital transformation, Ben works at the intersection of research, policy, and infrastructure—helping organisations navigate the complex demands of modern data ecosystems.\nHis expertise lies in enabling institutions to unlock the full value of research data by aligning technology, governance, and practice around frameworks such as DAMA’s DMBOK and various eResearch standards and taxonomies. Ben is particularly focused on delivering relentless optimisation—reducing friction, improving agility, and driving sustainable performance across the full data lifecycle.\nBen’s strategic influence is grounded in practical outcomes. He helps research leaders translate bold ideas—such as portable research infrastructures, intelligent data fabrics, and lifecycle-aware storage—into operational capabilities that empower researchers, data stewards, and institutional IT alike. His architectural approaches are designed to be adaptable, scalable, and deeply attuned to the evolving needs of distributed research models, including the rise of global “pop-up” campuses.\nAs a regular contributor to national eResearch discussions and forums, Ben advocates for coherent, standards-aligned data ecosystems that enhance collaboration, reuse, and long-term value realisation. His work continues to shape how research-intensive organisations design, manage, and scale the digital foundations that underpin scientific discovery.\nAs research leaders respond to increasing funding volatility, many institutions are evolving into distributed ecosystems—extending core capabilities via international partnerships, satellite programs, and aptly-named “pop-up campuses.” These environments demand agility, but often inherit a patchwork of incompatible data practices that limit reuse, add friction, and jeopardise compliance. This presentation addresses how a unified approach to research data architecture—anchored by a national taxonomy—can enable consistency, flexibility, and researcher-centric data experiences across fragmented landscapes.\nFocusing on Australian institutions and their global research presence, the presentation outlines patterns for building adaptive, intelligent infrastructure that reflects the lived workflows of researchers. These patterns support responsive data services that scale with the project, enable portable environments, and maintain lifecycle integrity even when research is transient, collaborative, or globally dispersed.\nThe core insight is that a nationally standardised taxonomy does more than improve metadata—it becomes the backbone for federated services, classification-aware storage, and interoperable data governance. When applied to research storage and metadata systems, this framework allows institutions to project their data capabilities into new environments rapidly, while maintaining coherence with their compliance, performance, and access policies.\nExamples from multi-site deployments will highlight how alignment and infrastructure abstraction together reduce administrative burden, lower onboarding friction, and empower research staff to move fluidly between platforms without sacrificing oversight or performance.\nThe talk concludes with practical recommendations for eResearch professionals and data architects aiming to optimise for a future where research is no longer tied to place, but must remain tied to standards of quality, reuse, and experience."
    },
    {
        "day": "Tuesday",
        "time": "16:00 – 16:20",
        "location": "Boulevard B2",
        "session_chair": "Dianne Brown",
        "title": "Automated Workflows for Building and Testing Climate Models",
        "authors": "Tommy Gatti",
        "url": "https://conference.eresearch.edu.au/automated-workflows-for-building-and-testing-climate-models/",
        "details": "Mr Tommy Gatti1, Mr Harshula Jayasuriya1, Dr Aidan Heerdegen1\n1ACCESS-NRI, Australia\nTommy is a Senior Research Software Engineer for the Model Release Team at ACCESS-NRI. He's a multidisciplinary programmer who enjoys developing CI/CD infrastructure and technical pathfinding in different domains. In previous industry roles he has been a driver developer and member of a third level support team.\nDevelopers are human, and to err is human – and this erring is a barrier to providing reproducible builds and clear provenance chains for climate models and software builds. Often, questions are raised regarding the links between how binaries and components are changed, and where those binaries are located. These essential questions are ripe for addressing not by a human, but by a machine – through a continuous integration and deployment process using a High-Performance Computing (HPC) package manager with provenance in mind.\nUsing GitHub Actions and the build-from-source package manager Spack, we developed generic, automated workflows that validate standardised manifests and creates clear links between model changes (for example, source code, dependencies or build options), and the deployment of models onto HPCs. This workflow enables developers proposing changes via Pull Requests (PRs) to use a standardised manifest to specify versions, configuration and variation on dependencies and libraries of the final binaries. This manifest is used by an instance of Spack on a HPC to cache and deploy software. Post-install, the PR reflects the status of the deployment, information on how to access the binaries, and provides artifacts that enable future installations to be reproducible, replicable and reliable – a full chain of provenance.\nAt ACCESS-NRI, these automated workflows provide a consistent provenance chain for complex software, quicker PR feedback cycles, and leaves our developers to do what they do best. They were developed to support climate models – but the methodology is applicable to a wide range of software."
    },
    {
        "day": "Tuesday",
        "time": "16:00 – 16:20",
        "location": "Boulevard B3",
        "session_chair": "Ria Hamblett",
        "title": "“I Don’t Have Data”: Creating Guidelines for Law Researchers",
        "authors": "Andreas Mertin, Ria Hamblett",
        "url": "https://conference.eresearch.edu.au/i-dont-have-data-creating-guidelines-for-law-researchers/",
        "details": "Mr Andreas Mertin1, Ms Ria Hamblett1\n1University Of Technology Sydney, Sydney, Australia\nAndreas Mertin: Data Management Specialist, UTS Data Governance Team\nORCID:https://orcid.org/0000-0002-5808-4783\nBio: Andreas Mertin has provided RDM support to the UTS research community in numerous roles, from promoting RDM best practice as a Data Librarian, to advising on tools and infrastructure as an eResearch Information Analyst. He is now the Research Data Management Specialist in the Data Analytics and Insights Unit at UTS, where he drives data governance and information management activities in research practice and working to extend the Data Risk Framework across the UTS Research domain.\nRia Hamblett: Senior Specialist, Data Librarian, UTS Library\nORCID:https://orcid.org/0000-0002-3822-9631\nBio: Ria Hamblett has almost a decade of experience as an academic librarian, specialising in research support across all aspects of the research lifecycle. As Data Librarian in the Open Scholarship & Copyright team at UTS Library, she leads RDM training and advocacy efforts, including developing resources, providing consultations, and delivering workshops to research staff and students across all Faculties. She is a core member of the UTS RDM Community of Practice group, which functions as a cross-unit discussion forum for all University units involved in RDM activities, fostering a collaboratively informed stakeholder group.\nWhat does research data management (RDM) look like for legal research, especially when some law academics argue they do not have any data?\nThis collaborative project between UTS Library, the Data Governance Team and the Faculty of Law seeks to discover an answer to that vexing question. Through the creation of RDM guidelines, case studies and workflows, the project seeks to ensure broader institutional RDM requirements are met while providing concrete advice and specific examples tailored for Faculty of Law researchers.\nThe project hosted a series of focus groups with law researchers using different kinds of research materials, from archival and human participant data, through to more traditional textual sources. The aim was to create an open, warts and all, gloves off, yet inclusive collaborative environment where we as RDM professionals could engage in open dialogue with a variety of law researchers about how they view and manage their research materials.\nThough designed with law researchers in mind, the guidelines and their development nonetheless provide valuable insights into broader HASS disciplines approaches to RDM. Come along to hear about the surprising, sometimes frustrating, but always entertaining story of how we cracked the HASS RDM conundrum once and for all. *\n*Results not guaranteed – the needs of your local HASS researchers may vary. Attending this session is not an alternative to meeting with your local HASS researcher."
    },
    {
        "day": "Tuesday",
        "time": "16:00 – 16:20",
        "location": "Boulevard Room",
        "session_chair": "Simon Porter",
        "title": "Advancing the research landscape: a new institutional repository for the University of Otago",
        "authors": "Lisa Chisholm, Mandy Phipps-Green",
        "url": "https://conference.eresearch.edu.au/advancing-the-research-landscape-a-new-institutional-repository-for-the-university-of-otago/",
        "details": "Mrs. Lisa Chisholm1, Mrs. Mandy Phipps-Green1\n1University Of Otago, Dunedin, New Zealand\nLisa Chisholm is the Library Research Services Manager at the University of Otago | Ōtākou Whakaihu Waka. She has over 20 years experience in academic and corporate libraries both in New Zealand and overseas. She is knowledgeable and passionate about Open Scholarship, Research Metrics & Impact, Research Data Management, and Open Access, and is dedicated to supporting both students and academic researchers in these areas.\nhttps://orcid.org/0000-0002-5313-6943\nMandy Phipps-Green is a Research Services Librarian at the University of Otago Library, University of Otago | Ōtākou Whakaihu Waka. Holding a BA(Hons) in Anthropology from the University of Otago, and an MSc in Forensic Science from the University of Auckland, Mandy worked in the field of complex disease genetics for 15 years, before joining the Library in 2022. She is the administrator of OUR Archive, the University's institutional repository of research outputs.\nhttps://orcid.org/0000-0002-1763-9623\nIn 2021, the University of Otago’s existing institutional repository system (DSpace v.5) was assessed as highly bespoke, unintuitive, underutilised, expensive to maintain, and a security risk. With high-level sponsorship by the Deputy Vice-Chancellor of Research and Enterprise, the Library launched a project to select and implement a new cutting-edge repository system, with the goal of elevating Otago’s research visibility in a highly competitive tertiary sector.\nFollowing a rigorous evaluation process, stakeholders selected Esploro, a cloud-based Software as a Service (SaaS) platform from Ex Libris. A small but dedicated project team spearheaded the implementation, involving migration of existing content, and configuration of the new system from October 2023 to March 2024, culminating in a successful launch on 1 May 2024.\nThe transition was transformative: ~15,000 existing research outputs were migrated from the old system, of which ⅔ were student theses and just ⅓ non-thesis research. One-year post-launch, the repository boasts >44,000 outputs – an increase of 191%, with a dramatic increase in non-thesis research (now 75% of total content).\nThis successful increase in research outputs was made possible by key features of Esploro, including Smart Expansion, which imports known past publications using paired researcher ID and DOI information, and Smart Harvesting, which automatically identifies and adds new publications daily, minimising the manual deposits by researchers.\nThis presentation will explore the triumphs and challenges of implementing a new system and the impact on research visibility and accessibility."
    },
    {
        "day": "Tuesday",
        "time": "16:20 – 16:40",
        "location": "Boulevard Auditorium",
        "session_chair": "Ai-Lin Soo",
        "title": "Building Scalable Infrastructure for Ecological Modelling and Wildlife Monitoring in Australia",
        "authors": "Renuka Sharma, Jenna Wraith",
        "url": "https://conference.eresearch.edu.au/building-scalable-infrastructure-for-ecological-modelling-and-wildlife-monitoring-in-australia/",
        "details": "Dr Jenna Wraith1, Dr Renuka Sharma1, Jo Morris2\n1QCIF- Sustainable Futures, Brisbane, Australia,2ARDC, Brisbane, Australia\nDr Jenna Wraith is a conservation scientist and technologist leading the Sustainable Futures Department at QCIF, a not-for-profit organisation advancing digital infrastructure for research. She holds a PhD in plant conservation and specialises in developing national-scale tools for biodiversity research, including species distribution modelling, wildlife monitoring, and the use of artificial intelligence in ecological science. Jenna’s work bridges environmental research and technology, supporting data-driven decision-making for conservation. Based on Yugambeh Country (Gold Coast, Australia), she is passionate about making advanced tools more accessible and building inclusive, collaborative approaches to eResearch.\nAustralia is facing urgent environmental challenges including biodiversity loss, climate change, and increasing pressure on land and ecosystems. Responding to these requires robust infrastructure and collaboration across research, government, and the environmental sector. QCIF Sustainable Futures, which includes the EcoCommons platform and the Wildlife Observatory of Australia (WildObs), provides national-scale infrastructure to support scalable, transparent, and reproducible ecological modelling and wildlife monitoring.\nEcoCommons is a cloud-based platform that empowers researchers, policymakers, and practitioners to build and share ecological models using open-access data and validated, scalable Jupyter notebooks. These workflows enable species distribution modelling, climate change projections, and conservation planning, promoting transparency and accessibility for users of all skill levels. WildObs complements EcoCommons by providing infrastructure for automated analysis of camera trap images using advanced computer vision to detect and identify species. This supports long-term biodiversity monitoring and data requirements for national initiatives such as the Threatened Species Index and environmental reporting.\nThese platforms are built through strong, ongoing collaboration with partners across Australia’s research infrastructure ecosystem such as the Terrestrial Ecosystem Research Network (TERN) and the Atlas of Living Australia (ALA) and align with FAIR data standards and emerging data trends. Together, they combine ecological expertise with digital innovation to enable high-throughput environmental science that strengthens biodiversity reporting, land-use decisions, and climate adaptation strategies. By connecting data, infrastructure, and people, the Sustainable Futures portfolio showcases the value of shared vision and partnership. It equips Australia’s research community to respond to biodiversity and climate challenges and enables scalable, community-driven science."
    },
    {
        "day": "Tuesday",
        "time": "16:20 – 16:40",
        "location": "Boulevard B1",
        "session_chair": "Liz Stokes",
        "title": "Data Versioning: From Principles to Practical Recommendations",
        "authors": "Jens Klump",
        "url": "https://conference.eresearch.edu.au/data-versioning-from-principles-to-practical-recommendations/",
        "details": "Dr Jens Klump1,2, Prof Dr Heinz Pampel2, Dr Mingfang Wu3, Ms Laura Rothfritz2, Ms Dorothea Strecker2, Prof Dr Lesley Wyborn4,5\n1CSIRO, Perth, Australia,2Humboldt Universität zu Berlin, Berlin, Germany,3ARDC, Melbourne, Australia,4Australian National University, Canberra, Australia,5ARDC, Canberra, Australia\nJens Klump is a geochemist by training and Group Leader, Exploration Through Cover in CSIRO Mineral Resources, based in Perth, Western Australia. Jens' work focuses on how information technology can be used to solve geoscience challenges. This includes data in minerals exploration, data capture, and data analysis, automated data and metadata capture, sensor data integration, both in the field and in the laboratory, data processing workflows, and data provenance, but also data analysis by statistical methods, machine learning and artificial intelligence.\nThe data lifecycle—from acquisition to release—is increasingly complex, involving multiple processing stages, research groups, and funding sources. There are also growing concerns about data sovereignty and data governance. This complexity demands a robust conceptual framework to ensure reproducible versioning and linking of any original datasets to their many derivatives.\nTo address this, we initially developed six data versioning principles for digital data artifacts by analysing use cases and adapting the Functional Requirements for Bibliographic Records (FRBR) framework developed around 1995 by IFLA (International Federation of Library Associations and Institutions) for analogue Information Resources. Our six principles established a common language for key concepts and terms: Revision, Release, Granularity, Manifestation, Provenance, and Citation (Klump et al., 2021,https://doi.org/10.5334/dsj-2021-012).\nAs part of a project supported by the Berlin University Alliance, a workshop was held to translate the principles into actionable practices. The workshop, held in June 2024 in Berlin, was attended by 40 experts from information infrastructure institutions with diverse scientific backgrounds (https://zenodo.org/records/13743876). Through this workshop and subsequent RDA community feedback, we refined our work into key recommendations.\nIn this presentation, we will introduce these recommendations to the Australian data community, which need to be embedded in research practices in a consistent way. The recommendations cover:\n– adopting a consistent versioning strategy;\n– considering standardisation initiatives;\n– using persistent identifiers for unique identification of versions;\n– implementing clear and descriptive version labels;\n– ensuring user-friendly version control systems;\n– documenting changes and metadata;\n– communicating versioning practices clearly to stakeholders."
    },
    {
        "day": "Tuesday",
        "time": "16:20 – 16:40",
        "location": "Boulevard B2",
        "session_chair": "Dianne Brown",
        "title": "AI-Ready Research Data in Australia",
        "authors": "AI-Ready Research Data in Australia",
        "url": "https://conference.eresearch.edu.au/ai-ready-research-data-in-australia/",
        "details": "Ms Dianne Brown1, Komathy Padmanabhan, Jacky Cho, Sasenka Abeysooriya\n1Monash University, Australia\nTo be confirmed\nSituation\nAI presents a transformative opportunity for research. In the global race to ride the AI wave, governments, industry and research institutions are making significant investments in AI infrastructure and talent development. However, this momentum often overlooks a critical enabler of AI success: data. Without trustworthy, high-quality, and ethically managed research data, even the most advanced AI systems risk producing misleading or unethical results, undermining the integrity of scientific discovery.\nTask/ Action\nAustralia must ensure its institutional and national research data assets are AI-ready. Opportunities exist in:\nCreation of a repository of curated training and validation datasets, underpinned by well-maintained data catalogues\nCreating data quality standards and practices to ensure AI systems are trained on research data that produces valid, meaningful, and reproducible insights.\nDefining researcher’s role (and training them) as data stewards who ensure ethically curated, well-documented, and fit for training AI-ready data that also uphold other data principles such as CARE Creation of storage and infrastructure that considers long-term data access, versioning, and scalability.\nManaging AI-specific research data risks—such as bias, misuse, security, IP and privacy. This will be central in the responsible and ethical use of AI in research and maintaining research’s ongoing social license.\nResult\nA chance for a discussion around the evolution of research data governance, Indigenous data sovereignty, research ethics and integrity processes to account for these challenges and the new requirements for provenance, documentation of data use, persistent identifiers, consent models and other accountability measures to support researchers navigate this changing landscape."
    },
    {
        "day": "Tuesday",
        "time": "16:20 – 16:40",
        "location": "Boulevard B3",
        "session_chair": "Ria Hamblett",
        "title": "“I Don’t Have Data”: Navigating Research Data Management Support for Humanities and Social Sciences Disciplines",
        "authors": "“I Don’t Have Data”: Navigating Research Data Management Support for Humanities and Social Sciences Disciplines",
        "url": "https://conference.eresearch.edu.au/i-dont-have-data-2-0-navigating-research-data-management-support-for-humanities-and-social-sciences-disciplines/",
        "details": "Ms Ria Hamblett1, Mr Andreas Mertin1, Dr Shannon Taylor2, Dr Cameron Fong2, Mr John Brown4, Ms Alexis Tindall3\n1University of Technology Sydney, Sydney, Australia,2University of Sydney, Sydney, Australia,3University of Adelaide, Adelaide, Australia,4Curtin University Library, Perth, Australia\nShannon Taylor, Research Data Consultant, University of Sydney\nORCID:https://orcid.org/0000-0002-3449-1878\nBio: Shannon Taylor is a Research Data Consultant at The University of Sydney. Shannon engages with researchers, professional and faculty staff to assist with research data concerns, as well as produce and develop content. Shannon has recently developed innovative research data management approaches for the non-STEM field, leveraging her previous experience as a STEM researcher.\nCameron Fong, Research Data Consulting Lead, University of Sydney\nORCID:https://orcid.org/0000-0002-4558-3700\nBio: Cameron Fong engages with researchers, faculty, professional staff, and students to promote, improve, develop, and implement RDM best practices. This varies from developing resources, providing support and training for the research community, working with individual researchers to develop tailored solutions, to collaborating with peers at different institutions.\nRia Hamblett, Senior Specialist, Data Librarian, UTS Library\nORCID:https://orcid.org/0000-0002-3822-9631\nBio: Ria Hamblett has almost a decade of experience as an academic librarian, specialising in research support across all aspects of the research lifecycle. As Data Librarian in the Open Scholarship & Copyright team at UTS Library, she leads RDM training and advocacy efforts, including developing resources, providing consultations, and delivering workshops to research staff and students across all Faculties. She is a core member of the UTS RDM Community of Practice group, which functions as a cross-unit discussion forum for all University units involved in RDM activities, fostering a collaboratively informed stakeholder group.\nAndreas Mertin, Data Management Specialist, UTS Data Governance Team\nORCID:https://orcid.org/0000-0002-5808-4783\nBio: Andreas Mertin has provided RDM support to the UTS research community in numerous roles, from promoting RDM best practice as a data librarian, to advising on tools and infrastructure as an eResearch Information Analyst. He is now the Research Data Management Specialist in the Data Analytics and Insights Unit at UTS, where he drives data governance and information management activities in research practice and working to extend the Data Risk Framework across the UTS Research domain.\nAlexis Tindall, Manager, Digital Stewardship, University of Adelaide Library\nORCID:https://orcid.org/0000-0002-1888-6693\nBio: Alexis Tindall is Manager, Digital Stewardship at the University of Adelaide Library, leading the Library’s support for research data management and digital preservation. Before this, she worked in enabling digital research in the humanities, arts and social sciences, and has a background in project management and digitisation in museums.\nJohn Brown; Coordinator, Research Services, Curtin University Library\nORCID:https://orcid.org/0000-0002-6118-577X\nBio: John Brown works at Curtin University Library providing data management support for researchers.\nResearch Data Management (RDM) has traditionally been largely STEM-centric, presenting unique challenges for RDM professionals working with Humanities and Social Sciences (HASS) researchers. This session will delve into these challenges and discuss strategies for effective RDM engagement and support within HASS disciplines.\nWe will highlight the significant differences in professional drivers for RDM between HASS and STEM fields; and explore why standard RDM terminology (including terms such as ‘data’, ‘validate’, and ‘reproduce’) often does not resonate with HASS researchers. Emphasising the importance of listening to and understanding HASS perspectives, we will discuss how RDM policies and procedures should aim to be more inclusive, reflecting the diverse research landscapes across our institutions.\nThis session is designed for RDM support professionals working with HASS researchers, as well as governance personnel involved in developing RDM policies and procedures. It will feature short presentations from the University of Sydney, University of Technology Sydney, University of Adelaide, and Curtin University sharing insights from their RDM experiences and projects within HASS disciplines. This will be followed by an open forum discussion, for attendees to share their own experiences and pose questions.\nKey takeaways will include strategies for addressing discipline-specific challenges; engagement strategies for facilitating open discussions with researchers across disciplines; uplifting RDM practices within HASS; and shared best practices and successes.\nThis session aims to foster a mutually beneficial approach to RDM, aligning institutional objectives with the unique needs of the HASS community."
    },
    {
        "day": "Tuesday",
        "time": "16:20 – 16:40",
        "location": "Boulevard Room",
        "session_chair": "Simon Porter",
        "title": "Building a National Persistent Identifier Toolkit to Enhance Research Quality, Provenance, and Impact",
        "authors": "Building a National Persistent Identifier Toolkit to Enhance Research Quality, Provenance, and Impact",
        "url": "https://conference.eresearch.edu.au/building-a-national-persistent-identifier-toolkit-to-enhance-research-quality-provenance-and-impact/",
        "details": "Mr Simon Porter1, Ms Natasha Simons2, Dr Lyle Winton2, Mr Matthias Liffers2, Adjunct Professor Linda O'Brien\n1Digital Science, London, United Kingdom,2ARDC, Australia\nSimon Porter (https://orcid.org/0000-0002-6151-8423) is VP of Research Futures at Digital Science, and a member of the ORCID Board. He has forged a career transforming university practices in how data about research is used, both from administrative and eResearch perspectives. As well as making key contributions to research information visualization, he is well known for his advocacy of Research Profiling Systems and their capability to create new opportunities for researchers.\nNatasha Simons (https://orcid.org/0000-0003-0635-1998) is Director, National Coordination, for the Australian Research Data Commons (ARDC). She leads a large, talented team of Program Managers, Product Managers and Subject Matter Experts contributing to deliver ARDC's strategic Research Data Commons initiatives. Based at the University of Queensland in Brisbane, Natasha is passionate about bringing out the best in people and creating high performing teams. She has a high international profile in the area of research data infrastructure, particularly persistent identifiers, and enjoys collaborating to solve common data challenges Natasha holds a number of positions on international Boards, Advisory and Working Groups.\nLyle Winton (https://orcid.org/0000-0002-3049-1221) is ARDC's Program Manager for Research Link Australia. He has over 20 years of experience in research infrastructure having worked within universities and on state, national, and international initiatives. Lyle has a research background in experimental physics involving large-scale collaborations, with significant challenges in sharing knowledge, big data and large scale computing.\nMatthias Liffers (https://orcid.org/0000-0002-3639-2080) has worked at the intersection of information management and information technology for nearly two decades. As Product Manager Persistent Identifier Services at the ARDC, he works with Australian research organisations to make sense of their place in the global research ecosystem.\nLinda O'Brien (https://orcid.org/0000-0002-1477-8652) is a highly experienced senior executive, Board director and consultant who has successfully delivered operational, transformational and strategic initiatives within the academic and community sectors.\nThis BoF session aims to bring together practitioners and stakeholders across the research ecosystem to discuss the development and implementation of a national benchmarking toolkit designed to accelerate the adoption of persistent identifiers (PIDs) in Australia. The session will explore the shared responsibility required to achieve the goals of the Australian National PID Strategy and how the toolkit supports measurable progress toward improved research quality, provenance, and impact.\nParticipants will engage with the toolkit in the context of the five strategic objectives of the strategy: (1) enhancing the FAIRness of research inputs, (2) increasing discoverability and reuse, (3) improving reproducibility and reducing administrative burden, (4) supporting impact assessment through metadata, and (5) enabling capability mapping.\nThis session is designed for research infrastructure professionals, funders, institutional leaders, and digital policy makers. It will include a brief presentation followed by structured small-group discussions, each focused on one strategic objective. Participants will share challenges, practices, and opportunities for improving PID adoption and alignment within their own contexts.\nAttendees will leave with a deeper understanding of the benchmarking framework, ideas for advancing local and national PID efforts, and connections to peers working toward similar goals. The session will also explore how this toolkit can foster collective accountability and support a national community of practice around persistent identifiers."
    },
    {
        "day": "Tuesday",
        "time": "16:40 – 17:00",
        "location": "Boulevard Auditorium",
        "session_chair": "Ai-Lin Soo",
        "title": "From risk to research asset: DMPs with purpose",
        "authors": "Philippa Frame",
        "url": "https://conference.eresearch.edu.au/from-risk-to-research-asset-dmps-with-purpose/",
        "details": "Mrs. Philippa Frame1, Ms Yvette Wyborn2, Mr Marvin van Prooijen1, Mr Ryan Bennett2,Ms Paola Pettit2\n1Office for Scholarly Communication, QUT Library, Queensland University of Technology, Brisbane, Australia,2eResearch, Queensland University of Technology, Brisbane, Australia\nPhilippa Frame is a Research Data Librarian in the Office for Scholarly Communication at QUT, where she leads the Library’s research data management service to support staff and Higher Degree Research students to manage their data across the entire research lifecycle, ensuring its usability for current and future research. Her responsibilities include providing training and advice on the creation and completion of data management plans, facilitating access to purchased and secondary research datasets, and recommending suitable data storage solutions tailored to individual needs. Philippa also assists researchers to publish their research data through Research Data Finder, QUT’s research data repository.\nData Management Plans (DMPs) are often perceived as compliance checkboxes—onerous, misunderstood, and underutilised. At QUT, we are shifting this narrative. Recognising that robust data management practices are foundational to impactful research, we embarked on a strategic transformation to position DMPs as research-enabling tools that improve efficiency, integrity, and institutional data governance.\nThe DMPonline Replacement Project was a critical initiative within a cross-departmental program aimed at strengthening infrastructure and support systems for high-quality research. It addressed longstanding challenges with the legacy system, which was an unsupported, high-risk platform with low user engagement. A fragmented experience with unclear processes and duplicate efforts between the existing application and a Research Data Management + Primary Materials (RDM+PM) Checklist led to researcher confusion and poor-quality outputs that risked non-compliance with institutional policy and funding body requirements.\nTo address these issues, QUT developed an integrated platform that combines the functions of both tools, enhances accessibility, and introduces automated provisioning of data storage at the point of plan creation. This solution reduces technical risk and administrative burden while empowering researchers with the right tools. By investing in purpose-built infrastructure and user-centric design, this project re-framed DMPs as more than administrative overhead. The new solution simplifies planning, improves accessibility, and introduces a Research Project ID (RPID) to support data management best practice.\nThis presentation shares insights from the redesign and implementation process, highlights outcomes aligned with strategic research goals, and offers a practical, replicable model for other institutions."
    },
    {
        "day": "Tuesday",
        "time": "16:40 – 17:00",
        "location": "Boulevard Room",
        "session_chair": "Simon Porter",
        "title": "A Strategic Community Roadmap for an Australian FAIR Vocabulary Ecosystem",
        "authors": "Kheeran Dharmawardena",
        "url": "https://conference.eresearch.edu.au/a-strategic-community-roadmap-for-an-australian-fair-vocabulary-ecosystem/",
        "details": "Dr Megan Wong2, Donald Hobern3, Dr Steven McEachern4, Professor Douglas Boyle5, Dr Natalia Atkins6, Kheeran Dharmawardena7, Dr Lesley Wyborn1,8\n1NCI, Australian National University, Canberra, Australia,2Federation University, Mt Helen, Australia,3University of Adelaide, Adelaide, Australia,4University of Essex, Wivenhoe Park, Australia,5Integrated Marine Observing System (IMOS), Hobart, Australia,6The University Melbourne, Melbourne, Australia,7Cytrax, Melbourne, Australia,8AuScope Ltd, Melbourne, Australia\nMegan is a Research Associate in eResearch (PhD – Soil Ecology) with a background in science and education. Her research interests are in enabling sustainable information management and knowledge transfer, and its application to environmental management. Megan's current work focuses on helping ensure agricultural and natural resource management data, information and knowledge is globally available to researchers, government agencies, municipalities and the public, with a focus on making data understandable by both humans and machines (semantic interoperability).\nVocabularies are the anchor points that humans and machines rely on for effective and efficient data processing. Vocabularies encompass controlled vocabularies, taxonomies, thesauri, ontologies and metadata schemas, and form part of an ecosystem that includes the people, resources, standards, tools, platforms, policies, and practices that make them accessible and useful for researchers.\nCurrently in Australia the vocabulary ecosystem is fragmented and not effectively coordinated. Many existing vocabularies are not findable, let alone accessible, interoperable and reusable by machines (i.e., not FAIR), limiting the uptake of machine-learning and artificial intelligence technologies to leverage existing data for new insights and to model complex systems at scale.\nTo address these issues, in 2022 a Vocabulary Workshop was held, sponsored by the Australian Data Archive, the Australian Research Data Commons and CODATA. A proposal for a Community Roadmap emerged that focused on what was required to make vocabularies machine-actionable (FAIR) and well-governed. Regular community consultations were held through BoFs at eResearch Australasia in 2022, 2023 and 2024, and the AeRO eResearch Newsletter.\nThe resultant '2025 Strategic Community Roadmap for an Australian FAIR Vocabulary Ecosystem' aims to facilitate wider adoption and broader community engagement for machine-actionable vocabularies, with a focus on the social and technical support required to overcome current data interoperability challenges. Fifty-seven recommendations are presented organised around four strategic themes, highlighting their importance and urgency for implementation.\nThis presentation will offer an overview of the roadmap and will provide feedback to the community on their valuable contributions at the 2022-2024 eResearch Australasia Conferences."
    },
    {
        "day": "Tuesday",
        "time": "17:00 – 17:20",
        "location": "Boulevard Auditorium",
        "session_chair": "Ai-Lin Soo",
        "title": "DMP Engagement Recipe",
        "authors": "Yvette Wharton",
        "url": "https://conference.eresearch.edu.au/dmp-engagement-recipe/",
        "details": "Ms Yvette Wharton1, Dr Sarah Hopkins1, Laura Armstrong1\n1The University of Auckland, New Zealand\nYvette Wharton works at the Centre for eResearch, Waipapa Taumata Rau, University of Auckland. She is the Research Data Management Programme Lead, working on the Secure Research Environment and machine-actionable Data Management Planning initiatives.http://orcid.org/0000-0002-6689-8840\nEnabling good research data management (RDM) practice is increasingly important given the intersection of data ethics, security and use of new technologies. To do this, universities undertake maturity assessments and develop roadmaps to provide efficient and practical training, connected tools for data management planning (DMPing), infrastructure, and support services. These enable researchers to undertake responsible research data management that align with policy and regulatory requirements whilst supporting university administrative needs.\nWaipapa Taumata Rau, the University of Auckland, used a co-design approach to select ReDBox as our Data Management Planning tool. Following a pilot, we are engaging with the research community to raise awareness and encourage the use of our newly launched online DMP tool. In this talk, we will share our engagement approach and experiences, including the outreach activities undertaken and the resources we have developed as we transition to a DMP Service."
    },
    {
        "day": "Tuesday",
        "time": "17:00 – 17:20",
        "location": "Boulevard Room",
        "session_chair": "Simon Porter",
        "title": "Domain-specific vocabulary discovery facilities in Australia: GeoVoc & EnviroVoc",
        "authors": "Nicholas Car",
        "url": "https://conference.eresearch.edu.au/domain-specific-vocabulary-discovery-facilities-in-australia-geovoc-envirovoc/",
        "details": "Dr Nicholas Car1\n1KurrawongAI, Brisbane, Australia\nNicholas worked for a decade in informatics research at Australian universities and government research organisations – CSIRO. His PhD focussed on decision modelling using Semantic Web ontologies for irrigation.\nHe has worked in international Semantic Web standards development in the areas of data profiling and spatial data.\nSince 2018, Nicholas has worked in the private sector supplying commercial and free open source Knowledge Graph-based products and services.\nIn 2022, Nicholas founded KurrawongAI to focus on product-independent consulting.\nAs of 2025, Australian research units, government, and the private sector have published Semantic Web vocabularies for over a decade. Publication and use is growing but perhaps slowly. Discussions with producers and users indicate that difficult vocabulary discovery is a barrier to increased growth. Reducing it could improve use, via reuse, and publication, by vocab extension.\nNo single technical facility meets all vocabulary-related needs of publisher and user communities. In the geology/resources domain, at least five government bodies (e.g. Geoscience Australia (GA), the Government Geoscience Information Committee (GGIC), and state Geological Surveys), the ARDC’s Research Vocabularies Australia (RVA), and several international organisations publish domain-specific vocabs, thus all must be searched for relevant content.\nWe have tried to introduce cross-system search into vocab tools widely used in the geology/resources domain but this has proved difficult for operators as each must maintain information about all other organisation’s facilities in the sector to provide effective search.\nTo overcome this, we have produced Australian geology/resources domain and environmental domain-specific vocabulary discovery portals within which we maintain access to all known sector-specific and several generic vocabulary facilities, such as RVA and the Intergovernmental Committee on Surveying & Mapping (ICSM)’s systems.\nWe have surveyed geology/resources vocabulary users and producers about the value of this facility and we present those results as well as details about operational effort. From these we give a cost/benefit analysis of the work and future plans."
    },
    {
        "day": "Tuesday",
        "time": "17:20 – 19:00",
        "location": "Boulevard Auditorium",
        "session_chair": "Ai-Lin Soo",
        "title": "Welcome Reception | Boulevard Foyer Exhibition AreaThe Welcome Reception offers a prime opportunity for attendees to unwind, connect, and build relationships in a casual setting, creating a positive and collaborative atmosphere that extends beyond formal sessions; additionally, mingling with exhibitors provides a valuable chance to explore innovations, partnerships, and industry insights.",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Wednesday",
        "time": "08:00 – 17:30",
        "location": "N/A",
        "session_chair": "N/A",
        "title": "Registration | Boulevard Auditorium Registration Desk",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Wednesday",
        "time": "08:30 -08:40",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "Welcome to Day 2Conference Chairs",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Wednesday",
        "time": "08:40 – 09:20",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "The Convergence of HPC, AI, and Quantum: Towards cutting-edge eScience on FugakuNEXT",
        "authors": ", Director of RIKEN Center for Computational Science (R-CCS)",
        "url": "https://conference.eresearch.edu.au/the-convergence-of-hpc-ai-and-quantum-towards-cutting-edge-escience-on-fugakunext/",
        "details": "The convergence of High-Performance Computing (HPC), large-scale experimental data, and Artificial Intelligence is ushering in a new paradigm for eScience. At the RIKEN Center for Computational Science (R-CCS), we are leveraging the success of the supercomputerFugakuto build a comprehensive, AI-native discovery platform for the next decade. This talk will outline our strategy, beginning with current breakthroughs and culminating in our vision for the post-exascale or “Zettascale” era withFugakuNEXT. The talkwill present several case studies where the integration of AI with large-scale simulation on Fugaku has led to transformative results, achieving effective performance far beyond the hardware’s nominal capabilities in fields like earthquake simulation, materials science, and computational drug design.A key focus will be ourTRIP-AGISinitiative, which aims to developGenerative AI and Foundation Models for Scienceto automate and accelerate the entire research cycle—from hypothesis generation to the design of robotic experiments.The core of this vision is FugakuNEXT, a next-generation supercomputer co-designed withNVIDIA and Fujitsuto be a premier platform for “AI for Science”. I will detail the system’s architectural targets and, just as importantly, our open and sustainable R&D model.This includes the development of“Virtual Fugaku,”a vendor-agnostic software stack designed to ensure portability across platforms from the cloud to future supercomputers.Finally, I will address the practical challenges of this new era, including our approach to building a secure, on-premise generative AI infrastructure, and look ahead to the integration of Quantum-HPC systems to tackle currently intractable problems.\nBiography\nSatoshi Matsuoka has been the director of RIKEN Center for Computational Science (R-CCS) since 2018. He is responsible for developing the supercomputer Fugaku which has become the fastest supercomputer in the world in all four major supercomputer rankings in 2020 and 2021 (Top500, HPCG, HPL-AI, Graph500), along with multitudes of ongoing cutting edge HPC research being conducted, including investigating Post-Moore era computing, especially the future FugakuNEXT supercomputer.\nHe holds Ph. D. from the University of Tokyo in 1993. He was a Professor at the Global Scientific Information and Computing Center (GSIC), the Institute of Science Tokyo (formerly known as the Tokyo Institute of Technology), the director of the joint AIST-Tokyo Tech. Real World Big Data Computing Open Innovation Laboratory (RWBC-OIL). He was the leader of the TSUBAME series of supercomputers that had also received many international acclaims at the Institute of Science Tokyo, where he still holds a professor position, to continue his research activities in HPC as well as scalable Big Data and AI.\nHe has written over 500 articles according to Google Scholar, and chaired numerous conferences, including the Technical Papers Chair and the Program Chair for ACM/IEEE Supercomputing 2009 and 2013 (SC09 and SC13) respectively as well as many other conference chairs, and the ACM Gordon Bell Prize selection committee chair in 2018.\nHe is a Fellow of ACM, ISC, JSSST (Japan Society for Software Science and Technology) and IPSJ (Information Processing Society of Japan). His accolades are the ACM Gordon Bell Prizes in 2011 & 2021; the IEEE-CS Sidney Fernbach Award in 2014 as well as the IEEE-CS Computer Society Seymour Cray Computer Engineering Award in 2022, both being the highest awards in the field of HPC, and the only individual to receive both awards. He is selected as one of the HPCwire 35 Legends by the HPC wire in 2024.His longtime contribution for the computer science research was commended with the Medal of Honor with Purple ribbon by His Majesty the Emperor of Japan in 2022."
    },
    {
        "day": "Wednesday",
        "time": "09:20-10:00",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "Meet the Practitioners",
        "authors": "Machine Learning Computational Methods Specialist, Computational and Data Intensive Sciences, Research Technology & Operations, Defence Science and Technology Group",
        "url": "https://conference.eresearch.edu.au/meet-the-practitioners/",
        "details": "We all know research doesn’t succeed because of any one role alone. It takes an impressive range of eResearch skills to enable research outcomes: data centre specialists racking new compute, security boffins navigating NIST frameworks, software engineers wrangling -omics variables, and community builders bridging the gaps between them all.\nWhile it’s challenging to keep up with the rapidly developing eResearch ecosystem, it’s more important than ever to get to know the people behind the technology. When the security boffin and the data engineer connect, we design platforms that protect research data. When the software engineer and cloud architect connect, we build efficient and resilient software, and fit-for-purpose research infrastructure. When we go one level deeper in our connections across the eResearch landscape, we get one step closer to enabling the best possible research outcomes.\neResearch Australasia is an annual opportunity for eResearch practitioners to do just that – connect with each other and share what we have learned. In this vox-pop style keynote, you’ll meet some of the practitioners who bring eResearch platforms to life, and their domain-specific challenges. By amplifying the voices behind the technology, this keynote sets the stage for conversations and connections to go one step deeper at this year’s conference.\nKiowa Scott-Hurley is a Dja Dja Wurrung woman who spends her 9-5 leading a team of awesome Machine Learning engineers at the Defence Science and Technology Group. Her unique mix of skills across ML, Cybersec and HPC means you can always catch her grumbling something about GPU drivers. In her free time, you’ll find her covertly drawing comics during boring meetings, using LLMs for evil, and getting nerd-sniped by malware analysis."
    },
    {
        "day": "Wednesday",
        "time": "10:00 – 10:30",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "From Lab to Cloud: How Queensland University of Technology (QUT) Transforms Research Through Scalable Infrastructure",
        "authors": ", Associate Director of Enterprise Technology and Digital Research Infrastructure, Queensland University of Technology",
        "url": "https://conference.eresearch.edu.au/from-lab-to-cloud-how-queensland-university-of-technology-qut-transforms-research-through-scalable-infrastructure/",
        "details": "Join Queensland University of Technology as they showcase groundbreaking research discoveries enabled by strategic cloud partnerships with AWS and NVIDIA.Dr. Michael Milford will present on cutting-edge science – from revolutionary robot capabilities powered by the new AI revolution, to autonomous robots exploring and monitoring Antarctica and the Great Barrier Reef, revealing how these research breakthroughs become possible when scientists have access to the right computational tools.Dr. Andrew Janke will explore the research enablement strategy behind these discoveries, including quantum cybersecurity investigations, indoor air quality studies, and the practical challenges of empowering academics to pursue ambitious science. Learn how QUT bridges the gap between research ambition and computational reality, creating pathways for scientists to focus on discovery rather than infrastructure limitations.\nSpeakers\nDr Michael Milford, Director of the Centre for Robotics, Queensland University of Technology\nMichael Milford, FTSE, conducts interdisciplinary research at the intersection of robotics, neuroscience, and computer vision. He currently holds appointments as Australian Research Council Laureate Fellow, Director of the QUT Centre for Robotics, QUT Professor, Microsoft Research Faculty Fellow, and Fellow of the Australian Academy of Technology and Engineering. He has led or co-led projects totalling more than 56 million dollars in research and industry funding, receiving over 17,000 citations, and his work has culminated in numerous accolades, including six best paper awards (and ten finalists), the ATSE Batterham Medal, a joint outright Queensland Young Tall Poppy award, and was made a Microsoft Research Faculty Fellow, one of only three Australians ever to receive this recognition. From 2022 – 2027 he is leading a large research team combining bio-inspired and computer science-based approaches to provide a ubiquitous alternative to GPS that does not rely on satellites. His team has worked on commercial autonomous vehicle technology with companies like Ford and Caterpillar, and been funded by and/or collaborated with organizations like Amazon, Intel and Google Deepmind. Michael is also an educational entrepreneur: as founder of Math Thrills, he has produced engaging STEM education materials distributed to over 35 countries for more than 20 years. As a keen mentor, Michael works with individuals and groups, both locally at the individual, institute or university level across Australia, but also globally through his Hacking Academia initiative, and won the 2025 Outstanding Mentor of Researchers Eureka Prize.\nDr Andrew Janke, Associate Director of Enterprise Technology and Digital Research Infrastructure, Queensland University of Technology\nAndrew J is the Associate Director of Enterprise Technology and Digital Research Infrastructure at QUT. Andrew has an academic background in the development of imaging techniques for Magnetic Resonance, PET, Ultrasound and Microscopy. He has worked with great colleagues from ICT at the Uni of Sydney, the National Imaging Facility at the Centre for Advanced Imaging – UQ, the department of Geriatric Medicine, ANU and the Montreal Neurological Institute working on acquisition, statistical analysis and quantification of Biomedical Imaging data."
    },
    {
        "day": "Wednesday",
        "time": "10:30-11:00",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "Morning Tea, Exhibition, Poster Viewing | Boulevard Foyer",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Wednesday",
        "time": "11:00 – 11:20",
        "location": "Boulevard Auditorium",
        "session_chair": "Alex Reid",
        "title": "Harmonising metadata to improve data discoverability and interoperability",
        "authors": "Keeva Connolly",
        "url": "https://conference.eresearch.edu.au/harmonising-metadata-to-improve-data-discoverability-and-interoperability/",
        "details": "Ms Keeva Connolly1,2, Matt Andrews3, Peter Brenton3, Jack Brinkman3, Christopher Mangion3, Emily Marshall1, Winnie Mok1, Lisa Phippard1, Sarah Richmond4, Goran Sterjov3, Nigel Ward1, Tom Harrop1, Kathryn Hall3\n1Australian BioCommons, Australia,2QCIF, Australia,3Atlas of Living Australia, Australia,4Bioplatforms Australia, Australia\nKeeva is a scientific business analyst for the Australian BioCommons, where she works on two projects related to biodiversity genomics, the Australian Reference Genome Atlas (ARGA) and the Australian Tree of Life (AToL). She works within QCIF and is based in Brisbane.\nThe volume of genetic data being generated and deposited online is increasing exponentially, giving way to growing opportunities for data application and reuse. This is particularly the case for high-throughput analyses, which commonly use standardised pipelines to process or transform data in line with research goals. These analyses typically rely on the primary data being both discoverable and interoperable. For genetic sequence data, this usually requires a minimum set of metadata being described according to a common standard – something which is rarely the case for data deposited in different repositories.\nHere, I will talk about how we used metadata harmonisation to address this challenge in the context of two projects: the Australian Reference Genome Atlas, a genomic data indexing platform for data discoverability, and; the Australian Tree of Life, an infrastructure project for semi-automated genome assembly, annotation, and publication. We identified equivalent fields between metadata schemata to generate metadata mappings between data repositories, including the Bioplatforms Australia data portal and International Nucleotide Sequence Database Collaboration (INSDC) databases, including the Sequence Read Archive (SRA), BioSample, and GenBank. For the Australian Reference Genome Atlas, mapping multiple data sources to a common schema facilitates data searching, filtering, aggregation, and enabling access to metadata in a common format for bulk analysis. In the Australian Tree of Life project, metadata mapping enables data exchange between repositories and the reuse of pipelines previously developed by the Darwin Tree of Life initiative to process data which meet their metadata standards."
    },
    {
        "day": "Wednesday",
        "time": "11:00 – 11:20",
        "location": "Boulevard B1",
        "session_chair": "Natalie House",
        "title": "SEAF and the promise of International Data Spaces (IDS)",
        "authors": "Amber Daniels",
        "url": "https://conference.eresearch.edu.au/seaf-and-the-promise-of-international-data-spaces-ids/",
        "details": "Amber Daniels1\n1WABSI, Australia\nAmber Daniels is Policy and Governance Lead for the Shared Environmental Analytics Facility (SEAF). With nearly 20 years of experience in higher education, Amber has developed deep expertise in governance, compliance, and innovation strategy. Before joining WABSI, she served as a senior consultant specialising in governance and compliance for the Australian higher education sector.\nAmber now brings her governance expertise to the emerging domains of data spaces and collaborative research infrastructure, where she focuses on applying robust policy frameworks to complex digital environments. Through her work with the Australian Access Federation (AAF), she has contributed as a subject matter expert to AARC guidelines, bringing practical insights to authentication and authorisation frameworks.\nThe Shared Environmental Analytics Facility (SEAF) is a collaborative initiative working to improve the accessibility, interoperability, and reusability of environmental data. It is built at a regional level to focus on specific users and needs and is supported by lean central shared services.\nSEAF is investigating International Data Spaces (IDS) approaches. IDS offers a structured, policy-compliant approach to managing who can access data, under what conditions, and for what purposes. It promises transparent, enforceable sharing agreements and supporting FAIR+CARE principles. For Australia’s complex and distributed environmental data landscape, this model may help simplify collaboration while preserving the autonomy of data custodians.\nIn this paper, we explore the governance and policy opportunities of IDS in relation to SEAF. We outline the current challenges facing SEAF: aligning incentives across diverse stakeholders, integrating legacy data systems, and translating high-level architectures into practical, domain-specific implementations. We also explore challenges and opportunities specific to IDS, in particular repeatability and transparency, cost, time and adaptation from the EU to the Australian context.\nSEAF is in operation and expanding into additional regions and these issues are real and important. We are keen to partner and learn from others and invite researchers and policy experts to engage with us in this work."
    },
    {
        "day": "Wednesday",
        "time": "11:00 – 11:20",
        "location": "Boulevard B2",
        "session_chair": "Amr Hassan",
        "title": "The changing face of data risks for research in the age of AI",
        "authors": "Dianne Brown",
        "url": "https://conference.eresearch.edu.au/the-changing-face-of-data-risks-for-research-in-the-age-of-ai/",
        "details": "Ms Dianne Brown1, Ms Komathy Padmanabhan\n1Monash University, Australia\nDianne is a research data governance consultant working with sensitive data research such as clinical quality registries (CQRs) and the broader research community at Monash University. Dianne began her career working globally for a strategy consulting firm and joined Monash in 2015, developing a Health Research Data Governance Framework that took a novel value chain approach to how all data activities should be governed. She coordinates COPs in data linkage and runs health research data dictionary training at the University, redesigned Monash's DMP+ and research data website.\nSituation\nArtificial Intelligence (AI) presents unparalleled opportunities in accelerating or lowering the cost of knowledge creation. There are, however, risks associated with AI development and its use. The difficulty in fully identifying, describing and assessing the magnitude of these risks is linked to the comprehensive impact AI may have on society and the novelty of the harms that may be faced. Possible harms may also have a temporal nature, situated upstream, midstream or downstream from people’s immediate actions.\nAI risk has been framed broadly in a number of ways by governments, academia and the corporate sector. But for research Universities, with distributed decision-making and other hallmarks of complex systems, it is difficult to frame these risks and then assess them against the opportunity in each research situation.\nTask/ Action\nTo develop a tool that overcomes this complexity in identifying and managing AI risk in research by focussing initially, where risk is highest – the data risk.\nResult\nThe first steps, as part of research data governance, to ensure that correct controls in place when risk is high in AI use either due to:\nsensitive research data is being used;\nany research data is being used where AI being developed or deployed is considered to be high risk (e.g. systems affecting public health and safety, critical infrastructure, law enforcement, etc)\nThis approach for assessing AI research data risks incorporates the principles of research ethics, integrity, cybersecurity, privacy and IP, ensuring alignment of AI safety and research culture."
    },
    {
        "day": "Wednesday",
        "time": "11:00 – 11:20",
        "location": "Boulevard B3",
        "session_chair": "Werner Scholz",
        "title": "Panel Session: Building a Data Discoverability Reference Architecture",
        "authors": "Andrew Beattie, Leslie Almberg,Isabel Ceron, Yvette Wharton, Ai-Lin Soo, Marlies Hankel",
        "url": "https://conference.eresearch.edu.au/panel-session-building-a-data-discoverability-reference-architecture/",
        "details": "Conveners:\nSpeakers:"
    },
    {
        "day": "Wednesday",
        "time": "11:00 – 11:20",
        "location": "Boulevard Room",
        "session_chair": "Aditi Subramanya",
        "title": "Bunny: From Simple Tool to Shared Infrastructure for NHS Cohort Discovery",
        "authors": "Andy Rae",
        "url": "https://conference.eresearch.edu.au/bunny-from-simple-tool-to-shared-infrastructure-for-nhs-cohort-discovery/",
        "details": "Mr Andy Rae1,2, Mr Jonathan Couldridge1, Ms Vasiliki Panagi1, Dr Tim Beck1, Professor Philip Quinlan1,2\n1Centre for Health Informatics, University of Nottingham, Nottingham, United Kingdom,2Nottingham University Hospitals, Nottingham, United Kingdom\nAndy Rae is a software engineer working at the intersection of health informatics, open source, and real-world impact. Based at the Centre for Health Informatics at the University of Nottingham, Andy works across research, hospitals, and national programmes – including HDR Federated Analytics, the NHS SDE Programme, and DAREUK Trevolution – to help turn technical ideas into practical change.\n0009-0004-8544-2576\nIntroduction\nBunny is a lightweight, open-source tool for patient Cohort Discovery on health datasets. But the real insight is in how it spread, and what that means for anyone trying to get software adopted in complex environments.\nBuilt to solve a specific need inside the English National Health Service (NHS), Bunny was designed for simplicity, rapid deployment, and community ownership. Within months, it was adopted by 80% of NHS Secure Data Environments (SDEs). It is also in use at Clinical Practice Research Datalink (CPRD), covering over 40 million patient records.\nThis is a case study in collaborative tooling: how open infrastructure can gain traction in the NHS, what makes adoption possible, and why technical delivery alone isn’t enough without trust, timing, and a human approach.\nMethods\nWe worked directly with SDE teams, assembling early adopters into a shared user group. Bunny was released under an MIT license, with documentation, and user feedback loops. Contributions were encouraged early and integrated quickly, creating momentum and shared ownership.\nResults\nEight SDEs are now using Bunny, with five actively contributing to the codebase. Adoption required no formal mandate. Instead, this was achieved with only a tool, storytelling, and an ecosystem approach. Bunny now supports live Cohort Discovery on real data and is shaping future open-source efforts across the SDE programme.\nConclusion\nCode alone doesn’t make impact. Bunny succeeded because it was trusted, usable, and open. Its journey shows how we can build shared and sustainable infrastructure in healthcare, by enabling empowered teams and collaboration."
    },
    {
        "day": "Wednesday",
        "time": "11:20 – 11:40",
        "location": "Boulevard Auditorium",
        "session_chair": "Alex Reid",
        "title": "AI-Powered Analysis of ORCID and RLA Data for Technology Landscaping",
        "authors": "Luhan Cheng",
        "url": "https://conference.eresearch.edu.au/ai-powered-analysis-of-orcid-and-rla-data-for-technology-landscaping/",
        "details": "Mr Luhan Cheng1, A/Prof. Amir Aryani1, Dr. Stephen Petrie1, Dr. Hui Yin1\n1Swinburne University of Technology, Hawthorn, Australia\nLuhan Cheng is a PhD candidate with the ARC Center of Information Resilience (CIRES) at Swinburne University of Technology. He completed his Bachelor of Computer Science (Honours) degree at Monash University on the topic of multi-agent reinforcement learning and game theory. Luhan worked at Monash eResearch Centre as a high-performance computing consultant while working at a teaching assistant at the Faculty of IT. His research interest lies in applying computational approach to scientific problems that have significant impacts on our society. His PhD project “Automated Horizon Scanning for Scientific Papers and Patents” aims to create a pathway for automated knowledge mining for scientific articles and invention patents. The project is supervised by Dr. Stephen Petrie and A/Prof. Amir Aryani.\nORCID profiles provide a substantial amount of information about research communities. ORCID is considered a key data source for providing valuable and open-access data about researchers and their research works. However, ORCID also has potential to inform Technology Landscaping regarding researchers and their associated research networks, with potential applications including identifying technical expertise, emerging research communities, and investment opportunities.\nThis presentation features a case study leveraging over 100,000 Australian ORCID researcher profiles and their research outcome collected from ARDC Research Link platform. We demonstrate how we have collected, connected, and analysed this dataset to observe the time-series evolution of the Australian research communities. The resulting animation will illustrate five years of historical developments within these communities. This will also establish a robust foundation for projecting future technological trends in Australia.\nAligning with the technical aspect of this presentation, we demonstrate an AI pipeline using LLM model to analyse our data and achieve the visualisation results. This makes the solution presented in our work accessible and interoperable with many research infrastructure solutions in universities. We will discuss the technical challenges and opportunities observed while developing and deploying this solution in Nectar Cloud. We believe the lessons learned from our work can help identify potential new use cases for application of AI pipelines to analyse ORCID and other open scholarly data."
    },
    {
        "day": "Wednesday",
        "time": "11:20 – 11:40",
        "location": "Boulevard B1",
        "session_chair": "Natalie House",
        "title": "Trainstation: An orchestration system for frictionless delivery of Programming and HPC training",
        "authors": "Aidan Wilson",
        "url": "https://conference.eresearch.edu.au/trainstation-an-orchestration-system-for-frictionless-delivery-of-programming-and-hpc-training/",
        "details": "Mr Aidan Wilson1\n1Intersect Australia, Sydney, Australia\nAidan Wilson is Digital Research Services Manager at Intersect Australia, where he supervises a team of Digital Research Analysts, and manages Intersect’s researcher training program. Aidan has been heavily involved in training delivery and systems since he joined Intersect in 2015.\nPrior to joining Intersect, Aidan was a linguistic researcher working in the field of Australia’s Indigenous Languages, focusing in particular on the verb morphology of Traditional Tiwi, and other languages from the top-end, one of the most linguistically diverse regions on Earth.\nhttp://orcid.org/0000-0001-9858-5470\nAccess to software can be a barrier for many people in attending and fully participating in digital skills training. Unless the learner spends time preparing for the course by installing software and downloading data, trainers spend time during the course ensuring that everyone is ready to participate.\nFor years, we have been developing and refining systems that enable frictionless training where attendees (in programming courses in particular) have access to all necessary tooling: software and data. The latest iteration of these efforts, Trainstation, has eliminated account creation and course setup issues for all programming courses, and has been central to our training delivery for nearly two years.\nThis year, we turn to address the problems presented by HPC training delivery: In many cases, HPC administrators expect users to have taken training in HPC before they can be granted access to top-tier systems. But how can users take training without access to the cluster? To break this reciprocal dependency, we have developed a virtualised Slurm cluster using Docker, for delivery of HPC courses in a sandboxed environment. Integration with Trainstation furthermore ensures that user accounts, data, and HPC resources are provisioned and ready with minimal manual effort.\nThis presentation will focus on these two case studies—Programming courses, such as Python, R; and High-Performance Computing courses—the different challenges these courses present for training delivery, how a system like Trainstation can address these challenges, and how we might deploy similar solutions for other courses."
    },
    {
        "day": "Wednesday",
        "time": "11:20 – 11:40",
        "location": "Boulevard B2",
        "session_chair": "Amr Hassan",
        "title": "When AI Goes Rogue: Exposing the Hidden Threats in Language Models",
        "authors": "Vijay Chakravarthy",
        "url": "https://conference.eresearch.edu.au/when-ai-goes-rogue-exposing-the-hidden-threats-in-language-models/",
        "details": "Mr Vijay Chakravarthy1\n1Fujitsu Australia Ltd, Melbourne, Australia\nVijay a well-regarded Cyber security professional with expertise in delivering diverse product portfolios and driving innovation. Skilled in AI security, cloud security and Operational technology security domains, he has been proficient in solutioning and using data-driven insights as part of addressing security challenges. He is also an active member of Fujitsu's global Cybersecurity research and development (R&D) group where global innovations are born from.\nIntroduction\nThe integration of large language models (LLMs) into digital systems has introduced a new class of security vulnerabilities that challenge conventional cybersecurity paradigms. Despite their utility, LLMs exhibit a range of exploitable behaviours, from susceptibility to prompt injection to the inadvertent generation of insecure code. As these models become increasingly embedded in critical infrastructure, their vulnerabilities demand rigorous academic scrutiny.\nMethods\nThis study presents a qualitative analysis of LLM vulnerabilities through empirical case studies and adversarial testing. We examine the mechanisms behind prompt-based attacks, including psychological manipulation and reverse-engineering techniques, and assess the implications of these behaviours in real-world deployment contexts. Particular attention is given to the intersection of LLM behaviour with broader system-level security concerns.\nResults\nFindings indicate that LLMs are highly susceptible to manipulation via both technical and social vectors. Even benign prompts can elicit unintended outputs, and AI-generated code frequently contains exploitable flaws. Moreover, LLMs demonstrate persistent challenges in maintaining data confidentiality. These vulnerabilities are not isolated; they propagate through interconnected systems, amplifying risk across the cybersecurity ecosystem.\nConclusion\nMitigating LLM-related threats requires a paradigm shift in how AI security is conceptualised. We argue for a novel, systems-oriented approach that integrates LLM security into the broader cybersecurity framework. This work contributes to the growing discourse on AI safety by offering actionable insights and proposing a research agenda aimed at developing resilient, context-aware mitigation strategies."
    },
    {
        "day": "Wednesday",
        "time": "11:20 – 11:40",
        "location": "Boulevard B3",
        "session_chair": "Werner Scholz",
        "title": "Safeguarding data, Accelerating discovery: Secure, scalable infrastructure for research innovation",
        "authors": "Shaun Domingo",
        "url": "https://conference.eresearch.edu.au/safeguarding-data-accelerating-discovery-secure-scalable-infrastructure-for-research-innovation/",
        "details": "Researchers are navigating a landscape where compliance, security, and innovation collide. New funding streams and global collaborations demand rigorous data protection and the ability to manage sensitive, regulated information without slowing discovery. The stakes are high: falling short can mean missed opportunities, delayed outcomes, and diminished trust.\nThis session explores how secure, high-performance research environments can turn these challenges into opportunities. Attendees will see how purpose-built platforms such as CAUDIT Cloud enable institutions to meet complex compliance standards, safeguard sensitive data with confidence, and accelerate research outcomes – all while reducing cost, avoiding legacy lock-in, and empowering teams to innovate at speed.\nSpeakers:\nShaun Domingo, Chief Technology Officer, Macquarie Cloud Services\nShaun Domingo is the Chief Technology Officer at Macquarie Cloud Services, where he leads the development of cloud-native solutions aligned to customer needs. With more than two decades of experience spanning software engineering, DevOps, product, and architecture, Shaun has built and operated public and private cloud platforms that provide universities and research institutes with secure, high-performance, AI-ready infrastructure."
    },
    {
        "day": "Wednesday",
        "time": "11:20 – 11:40",
        "location": "Boulevard Room",
        "session_chair": "Aditi Subramanya",
        "title": "National Skills Taxonomy",
        "authors": "Les Kneebone",
        "url": "https://conference.eresearch.edu.au/national-skills-taxonomy/",
        "details": "Mr Les Kneebone1\n1Jobs And Skills Australia, Melbourne, Australia\nWith 20 years experience in metadata projects, Les has worked across government, research and social service sectors. He's helped developed important linked data vocabularies, including the Schools Online Thesaurus (ScOT), Public Policy Taxonomy, and Biosecurity Thesaurus. Les has also built grey literature databases for government- and industry-led research, including the Analysis & Policy Observatory (APO) and Biosecurity Portal.\nAt Jobs and Skills Australia, Les provides advise on optimising vocabulary standards in the National Skills Taxonomy project.\nJobs and Skills Australia has commenced a pilot project to build a National Stills Taxonomy (NST).\nA skills taxonomy is needed for Australia to support economic research, including analysis of lifelong learning initiatives and mobility within the tertiary education sector. A standardised skills nomenclature will improve linkages between employment opportunities and training outcomes by including skill concepts as part of the jobs and skills metadata ecosystem. Within the tertiary sector, a skills taxonomy will also serve harmonization initiatives between and within training and higher learning institutions and curriculum.\nThe need for a skills-first approach to better education and industry analytics is far from a parochial concern and the NST team is benchmarking the taxonomy project with significant skills metadata projects in other countries. Engagement with other skills taxonomy projects has delivered insights into approaches for skills ontology modelling; skills extraction methodologies; and taxonomy editing workflows.\nThe NST has already learned valuable lessons from a pre-pilot project which trialled artificial intelligence (AI) methods for extracting skills concepts from training literature. We present preliminary findings from this pre-pilot and the resulting methodology for the current pilot phase. We present our evolving approach to AI-assisted skills extraction, and implications for vocabulary modelling and construction. Work on a skill definition is also presented, and the impact that the definition has had on extraction and taxonomy methodologies discussed. We will present upcoming work and highlight challenges and opportunities for the NST in the post-pilot phase."
    },
    {
        "day": "Wednesday",
        "time": "11:40 – 12:00",
        "location": "Boulevard Auditorium",
        "session_chair": "Alex Reid",
        "title": "The importance of being indexed: introducing Genome Tracker by the Australian Reference Genome Atlas",
        "authors": "Kathryn Hall",
        "url": "https://conference.eresearch.edu.au/the-importance-of-being-indexed-introducing-genome-tracker-by-the-australian-reference-genome-atlas/",
        "details": "Dr Kathryn Hall1, Mr Matt Andrews1, Mr Peter Brenton1, Mr Jack Brinkman1, Ms Keeva Connolly2, Mr Christopher Mangion1, Ms Winnie Mok2, Ms Sarah Richmond3, Mr Goran Sterjov1, Dr Nigel Ward2\n1Atlas of Living Australia, CSIRO, Australia,2Australian BioCommons, Australia,3Bioplatforms Australia, Australia\nKathryn Hall:https://orcid.org/0000-0002-8785-4513\nDr Kathryn Hall leads the Australian Reference Genome Atlas (ARGA), a national platform advancing the discoverability and strategic use of genomic data for Australia’s biodiversity. Based at the Atlas of Living Australia (CSIRO), she brings a background in taxonomy and systematics to her work, which lies at the intersection of biodiversity science and digital research infrastructure. Dr Hall has extensive experience in research management, data governance, and cross-sector collaboration, and is committed to building platforms that support transparency, provenance, and data FAIRness. Her current focus is on delivering tools that enable genomic data to inform research, policy, and national priorities.\nIt is a truth universally ignored, that most species are unindexed, unsequenced, and utterly underloved. The Australian Reference Genome Atlas (ARGA) soothes this iniquity by offering a dashboard where being findable is the height of virtue. This year, we launched Genome Tracker, the first national dashboard showing the completeness of genomic data across Australia’s biodiversity. It marks a significant milestone for ARGA, meeting a long-standing community need for visibility of what has (and hasn’t) been sequenced.\nTo crave indulgence from Mr Wilde: to lose one genome may be regarded as a misfortune; to lose an entire phylum looks like carelessness. With ARGA’s new Genome Tracker, users can explore sequencing gaps and strengths across key biodiversity groups, including mammals, birds, and reptiles, through compelling, bespoke visualisations. Powered by a comprehensive backend index, Genome Tracker immediately highlights which lineages have strong representation and which are under-sequenced (or entirely missing). Tracker also highlights 30 pivotal milestones through a unique narrative timeline, complete with links to genomes and source publications.\nTargeting researchers, policy-makers and funders, and leveraging the unified view of Australia’s biodiversity served by the Atlas of Living Australia, Genome Tracker is the first tool of its kind to deliver auditable, verifiable baseline data about the progress of genomic sequencing efforts for continent-scale biodiversity.  Built as a stand-alone feature within ARGA, it supports planning, transparency, and data-driven prioritisation for future sequencing campaigns. This release affirms ARGA as a next-generation data platform – more than a metadata aggregator, it drives insight and generates momentum."
    },
    {
        "day": "Wednesday",
        "time": "11:40 – 12:00",
        "location": "Boulevard B1",
        "session_chair": "Natalie House",
        "title": "AuScope Virtual Research Environment Build Program. A Retrospective.",
        "authors": "Dr Jens Klump",
        "url": "https://conference.eresearch.edu.au/auscope-virtual-research-environment-build-program-a-retrospective/",
        "details": "Dr Pavel Golodoniuc1, V Fazio2, B.S Motevalli1, N Taherifar1, Y Li1, J Klump1, S Bradley1, A Devaraju1\n1CSIRO Mineral Resources, Kensington, Australia,2CSIRO Mineral Resources, Clayton, Australia\nJens Klump is a geochemist by training and Group Leader, Exploration Through Cover in CSIRO Mineral Resources, based in Perth, Western Australia. Jens’ work focuses on how information technology can be used to solve geoscience challenges. This includes data in minerals exploration, data capture, and data analysis, automated data and metadata capture, sensor data integration, both in the field and in the laboratory, data processing workflows, and data provenance, but also data analysis by statistical methods, machine learning and artificial intelligence.\nLaunched in 2020, the AuScope Virtual Research Environment (AVRE) Build Program aimed to bridge gaps between geoscientific research needs and technology capabilities by actively collaborating with universities and research institutions across Australia. Through five successful years, the AVRE Build Program delivered a diverse portfolio of collaborative projects spanning numerous scientific and technological domains, including seismic network optimisation, machine learning-based lithology classification, geological sample curation using persistent identifiers and geochemistry laboratory resource management via tailored digital catalogues. The program consistently utilised and championed Open-Source Software and Cloud computing capabilities, adhering to FAIR (Findable, Accessible, Interoperable, and Reusable) data principles and Open Science methodologies, thus establishing robust, reusable and sustainable technological infrastructure and data services.\nKey innovations, such as the ML-based classification of drill cuttings, facilitated deeper subsurface insights significant for advancing renewable energy projects and critical minerals exploration, while seismic network modelling introduced Cloud-enabled applications, accelerating accurate earthquake monitoring workflows.\nBeyond fostering technical innovation, the AVRE Build Program established a sustainable legacy by embedding user-centred design principles that empowered scientist participation from initial concept through to deployment, creating lasting ownership and accelerating methodological adoption across diverse geoscience domains. Consequently, these developments positively impacted research outcomes by enhancing interdisciplinary collaboration within Australian research institutions, democratising data access and interpretation, and instilling best practice standards to expand and enrich Australia’s geoscience digital infrastructure."
    },
    {
        "day": "Wednesday",
        "time": "11:40 – 12:00",
        "location": "Boulevard B2",
        "session_chair": "Amr Hassan",
        "title": "Securing Australia’s AI Future: Sovereign Infrastructure and Open-Source Innovation for Sensitive Data",
        "authors": "Securing Australia’s AI Future: Sovereign Infrastructure and Open-Source Innovation for Sensitive Data",
        "url": "https://conference.eresearch.edu.au/securing-australias-ai-future-sovereign-infrastructure-and-open-source-innovation-for-sensitive-data/",
        "details": "Dr Amr Hassan1, Prof David Powell1, Dr Gabriel Noaje2, Luc Betbeder-Matibet3, Sue Keay3, Andrew Rohl4, Mark Gray5\n1Monash University, Australia,2NVIDIA,3University of New South Wales,4National Computational Infrastructure,5Pawsey Supercomputing Centre\nThe Panel will include:\n– Dr. Amr Hassan, Director, Emerging Technologies and Program Director, Project MAVERIC, Monash\n– Prof. David Powell, Director Monash eResearch Centre\n– Dr. Gabriel Noaje, HPC Business Development, NVIDIA\n– Mr. Luc Betbeder-Matibet, Executive Director Research Technology Services UNSW\n– Dr. Sue Keay, Director UNSW AI Institute\n– Prof. Andrew Rohl, Director National Computational Infrastructure\n– Mr Mark Gray, Head of Strategic Partnerships, Pawsey Supercomputing Centre\nAs artificial intelligence (AI) becomes integral to sectors handling sensitive data, such as healthcare, defence, and critical infrastructure, Australia faces the imperative to develop secure, sovereign AI capabilities. This BoF session will explore strategies to enhance the security of AI systems and promote the adoption and development of open-source AI models within Australia's jurisdiction.\nKey discussion points include:\n– Secure AI Development Practices: Implementing guidelines for secure AI system development, encompassing design, deployment, and maintenance phases, to mitigate risks associated with sensitive data handling.\n– Sovereign AI Infrastructure: Building and strengthening Australia’s AI infrastructure to ensure data sovereignty and empower local innovation, talent development, and industry collaboration. This includes fostering research, supporting the growth of local expertise, and creating environments where sensitive data can be processed securely and in alignment with Australian values and regulations.\n– Open-Source AI Models: Encouraging the use and development of open-source AI models to foster transparency, adaptability, and innovation, while addressing licensing and security considerations.\nParticipants will engage in collaborative discussions to identify challenges and opportunities in establishing a secure and sovereign AI ecosystem in Australia. The session aims to facilitate knowledge exchange among researchers, policymakers, and industry stakeholders committed to advancing Australia's AI capabilities responsibly and securely."
    },
    {
        "day": "Wednesday",
        "time": "11:40 – 12:00",
        "location": "Boulevard B3",
        "session_chair": "Werner Scholz",
        "title": "Storage and Data Protection Solutions for Long Term Archiving",
        "authors": "Storage and Data Protection Solutions for Long Term Archiving",
        "url": "https://conference.eresearch.edu.au/storage-and-data-protection-solutions-for-long-term-archiving/",
        "details": "Organisations not only generate, process, and store more and more data, they also need to manage, protect and archive it for the long term while keeping it accessible for fast retrieval when reprocessing of the data is required. This is becoming even more important as Artificial Intelligence (AI) solutions rely on access to large amounts of data to train, fine-tune, and augment the underlying deep neural networks. As a result, IT teams are under increasing pressure to design and implement architectures, which can meet the requirements of these new applications, maintain them, and provide migration strategies which ensure the long-term protection and availability of the data.\nJoin this BoF session, to hear from data storage specialists, solution providers, integrators, and customers the latest architectures and solutions, challenges and pitfalls, and real-world experiences. We will cover different archiving paradigms (implicit vs. explicit archiving), protocols (S3, NFS, proprietary), storage technologies (SSD/disk/tape and emerging technologies), and storage formats.\nYou will leave this BoF with the insights needed to start developing your own long term archiving strategy and you will have met some of the foremost experts in the field, who can help refine and implement your strategy.\nSpeakers:\nWerner Scholz (XENON), Bruce Gilpin (Versity), Andrew Beattie (IBM), Matt Starr/Mike Cocks (Spectra Logic), Chris Schlipalius (Pawsey), Jason Lohrey (Arcitecta)"
    },
    {
        "day": "Wednesday",
        "time": "11:40 – 12:00",
        "location": "Boulevard Room",
        "session_chair": "Aditi Subramanya",
        "title": "Beyond the optics: Defining meaningful organisational support for Diversity & Inclusion",
        "authors": "Beyond the optics: Defining meaningful organisational support for Diversity & Inclusion",
        "url": "https://conference.eresearch.edu.au/beyond-the-optics-defining-meaningful-organisational-support-for-diversity-inclusion/",
        "details": "Aditi Subramanya1, Linda McIver2, Emily Barker3, Carina Kemp4, Kiowa Scott-Hurley5, Kerri Wait5, Ms Jana Makar6\n1Pawsey Supercomputing Research Centre, Australia,2Australian Data Science Education Institute, Australia,3University of Western Australia, Australia,4Amazon Web Services, Australia,5Independent, Australia,6REANNZ, New Zealand\nWomen in High Performance Computing – Australasia Chapter (WHPC+ AusNZ)\nThe Australasian Chapter of the global organisation Women in High Performance Computing (WHPC) was launched in 2019. It supports an active Slack community, regular BoFs at conferences, online meetups, and practical resources for organisations to use for creating positive change. We welcome people from all backgrounds and perspectives beyond gender to join our Chapter.\nThe Chapter Organising Committee is currently composed of:\n– Jana Makar, Communications Manager, New Zealand eScience Infrastructure (NeSI)\n– Aditi Subramanya, Manager Partner Engagement, Pawsey Supercomputing Research Centre\n– Emily Barker, Senior High Performance Computer (HPC) Engineer, University of Western Australia\n– Linda McIver, Director, Australian Data Science Education Institute\n– Carina Kemp, International Lead for Research, Amazon Web Services (AWS)\n– Kiowa Scott-Hurley, Independent\n– Kerri Wait, Independent\n– Loretta Davis, Independent\nFor more information, visithttps://tinyurl.com/whpcaunz.\nDiversity and inclusion (D&I) are not just values to aspire to, they are recognised strengths that add measurable value to teams, organisations, and outcomes. In this panel discussion, we’ll hear from a mix of senior leaders and team members to share two interlocking perspectives in this space:\n– What are organisations doing to foster inclusive environments?\n– What has genuinely helped individuals feel seen, supported, and included?\nThis session is not just about showcasing success stories. We’re looking to shift the conversation to honest reflections on what organisational practices move the needle, from inclusive recruitment and promotion pathways, to leadership accountability and everyday team culture. At the same time, we’ll hear from individuals whose careers and well-being have been shaped by these efforts, or their absence.\nBy inviting a mix of senior management and other staff, we aim to spotlight both systemic initiatives and lived experiences. This session will be especially valuable for managers of teams and leaders of organisations who are seeking to align their practices with genuine D&I outcomes. Whether you’re shaping strategy, building a team, or considering sponsorship opportunities, this conversation will offer practical insights and help set shared expectations for D&I practice in the eResearch space.\nWe’ll capture insights and practical advice shared in this session to distribute after the event as a resource for the wider eResearch community."
    },
    {
        "day": "Wednesday",
        "time": "12:00 -12:20",
        "location": "Boulevard Auditorium",
        "session_chair": "Alex Reid",
        "title": "Preserving Our Born-Digital Cultural Heritage – it’s EAASI! – Phase Two of The Australian Emulation Network (AusEAASI)",
        "authors": "Alex Ip",
        "url": "https://conference.eresearch.edu.au/preserving-our-born-digital-cultural-heritage-its-eaasi-phase-two-of-the-australian-emulation-network-auseaasi/",
        "details": "Mr Alex Ip1, Dr Melanie Swalwell2, Dr Cynde Moya2, Mr Adam Bell1, Mr Steele Cooke1\n1Aarnet Pty Ltd, Chatswood, Australia,2Swinburne University of Technology, Hawthorn, Australia\nAlex Ip has worked across diverse sectors including manufacturing, software development, data engineering and eResearch infrastructure development over several decades. He has worked closely with researchers in domains including livestock genetics, Earth observation, geophysics, and, most recently, bio-informatics, digital asset preservation and text analytics.\nAlex developed the operational prototype system which became Digital Earth Australia, and also developed the backend information architecture for Geoscience Australia’s current Geophysical Archive Data Delivery System v2 (GADDS2).\nWithin AARNet, Alex’s team provides innovative eResearch infrastructure solutions to projects including the Australian BioCommons, the Language Data Commons of Australian (LDaCA), and the Play-It-Again digital preservation project. The team also supports the Globus data transfer system in Australia.\nBorn-digital cultural artefacts are, by their very nature, ephemeral: posing the very real risk that we could lose significant parts of our cultural heritage unless we take active steps to preserve them now. Future archaeologists will not be finding digital content in digs.\nThe Australian Emulation as a Service Infrastructure (AusEaaSI) project is a consortium of Australian academic and cultural institutions, led by Swinburne University, which is taking the necessary steps to preserve our digital cultural heritage. AARNet provides shared emulation infrastructure which is used by member institutions to preserve invaluable and irreplaceable content. Targeted domains include digital media arts, architecture, graphic design, industrial design, games, social media, digital/digitised photography, augmented/virtual reality, and web/pre-web networking.\nThe development of the EaaSI system is championed by the Software Preservation Network (SPN), an international organization established to advance software preservation through collective action across five core areas: Law & Policy, Training & Education, Metadata & Standards, Technological Infrastructure, and Research-in-Practice. Yale University remains a leading institution in the development and deployment of EaaSI.\nThe first iteration of AusEaaSI has proven to be an unqualified success. This year, AARNet is rolling out its successor which further improves system capabilities and capacity. We will provide an overview of the Kubernetes-based architecture that improves portability and reliability, and opens up exciting new possibilities including hybrid cloud deployments.\nDo you have important but at-risk media for an obsolete system? Find out how you can join AusEAASI to keep your digital content alive."
    },
    {
        "day": "Wednesday",
        "time": "12:00 -12:20",
        "location": "Boulevard Room",
        "session_chair": "Aditi Subramanya",
        "title": "From Discovery to Action: Tackling AI-Driven Data Challenges Through Global Collaboration",
        "authors": "Hilary Hanahoe",
        "url": "https://conference.eresearch.edu.au/from-discovery-to-action-tackling-ai-driven-data-challenges-through-global-collaboration/",
        "details": "Ms Trish Radotic1, Ms Hilary Hanahoe2, Dr Shalini Kurapati2\n1Australian Research Data Commons, Australia,2Research Data Alliance,\nHilary Hanahoe is the Secretary General of the Research Data Alliance (RDA), an international, non-profit, volunteer organisation addressing the need for open and interoperable sharing and re-use of research data and building the social, technical and cross-disciplinary links to enable such sharing and re-use on a global scale.\nCurrently, RDA has a community of over 14,500 individual data professionals from 151 countries collaborating on different open science and open data activities, operating under six fundamental guiding principles of openness, consensus, harmonisation, community-driven, inclusivity, not for profit and technology neutrality. Hilary is passionate about the work of the Research Data Alliance and its vibrant, volunteer community working to enable the open sharing and reuse of data across the globe.\nBackground\nThe accelerating influence of Artificial Intelligence (AI) on research data, data management, and Open Science presents urgent and complex challenges. While the Research Data Alliance (RDA) traditionally evolves through community-led, bottom-up initiatives, the scale and immediacy of AI’s impact have prompted a rare top-down approach to identifying and addressing priority areas. This initiative aims to catalyse global collaboration to build the social and technical bridges needed to meet AI-related data challenges.\nMethod\nRDA convened a series of structured discovery workshops with its Organisational Members – representing universities, research institutions, and infrastructure bodies globally. These sessions explored where AI intersects with research data practices and data management, revealing high-priority themes requiring immediate community action.\nResults\nThe workshops surfaced a set of thematic priorities, with findings highlighting both immediate gaps and opportunities for new RDA Working and Interest Groups to solve these AI-related data challenges.\nConclusion\nThis presentation shares the key outcomes from the AI discovery initiative and outlines next steps for community engagement. As AI reshapes the research data landscape, we call on the global e-research and data community to take up this challenge – forming the collaborative groups needed to build shared solutions and ensure that data-driven research in the AI era remains open, trustworthy, and impactful."
    },
    {
        "day": "Wednesday",
        "time": "12:20 – 12:40",
        "location": "Boulevard Auditorium",
        "session_chair": "Alex Reid",
        "title": "Co-Designing a National Advanced Analytics and AI Resource Hub",
        "authors": "Bernadette Hyland-Wood, Kerrie Mengersen",
        "url": "https://conference.eresearch.edu.au/co-designing-a-national-advanced-analytics-and-ai-resource-hub/",
        "details": "Dr Bernadette Hyland-wood, Professor Nicola Armstrong, Dr Gnana Bharathy, Distinguished Professor Kerrie Mengersen\n1Queensland University of Technology, Brisbane, Australia\nDr Bernadette Hyland-Wood is a Research Fellow at the Centre for Data Science, Queensland University of Technology, specialising in responsible data science and technological humanism. She is a research investigator on Indigenous-led data governance programs and has chaired international data standards to support community advocacy and evidence-informed policy making. Dr Hyland-Wood actively engages with academia, industry, and public policy makers to advance human-centric technologies.\nDistinguished Professor Kerrie Mengersen is a leading Australian statistician and former Director of the QUT Centre for Data Science. Renowned for her work in Bayesian statistics and complex systems modelling, her research spans health, environmental science, and industry applications. She has authored over 350 peer-reviewed publications and secured more than 30 major research grants. Professor Mengersen is a Fellow of the Australian Academy of Science, the Academy of Social Sciences in Australia, and the Queensland Academy of Arts and Sciences.\nThe Australian Research Data Commons (ARDC) initiated a digital research infrastructure program in 2023 aimed at enhancing digital health research and translation. The ARDC collaborated with the Australian Data Science Network (ADSN) to evaluate digital health infrastructure including computing resources, data and analytics methods, and data accessibility. Having assessed critical national research infrastructure (NRI) requirements to support next generation health data analytics, the Advanced Analytics and AI Resource Hub is now co-designing and building critical components of the infrastructure stack aligned with healthcare-specific use cases. Deployment will be in Virtual Research Environments. The Advanced Analytics and AI Resource Hub emphasises socio-technical assets and resources to be guided by data ethics and governance informed by national and international best practices and the responsible use of artificial intelligence (AI).\nBuilding on the findings from the first phase involving researchers, government and industry representatives, Phase 2 leverages a phased and collaborative methodology, grounded in co-design principles to build a sustainable AI Resource Hub aligned with national healthcare research needs and the ARDC Advanced Analytics Reference Architecture. The approach is structured across seven interlinked Work Packages (WPs), each targeting a critical component of the infrastructure stack. This presentation discusses progress toward a nationally coordinated digital infrastructure. Through a collaborative and iterative co-design process, the ARDC and its partners are building deployable, healthcare-specific tools and resources. These efforts mark a critical step toward enabling scalable, ethical, and domain-sensitive digital health research across Australia."
    },
    {
        "day": "Wednesday",
        "time": "12:20 – 12:40",
        "location": "Boulevard Room",
        "session_chair": "Aditi Subramanya",
        "title": "The IBM/UQ Centre of Excellence",
        "authors": "David Abramson, Jake Carroll",
        "url": "https://conference.eresearch.edu.au/the-ibm-uq-centre-of-excellence/",
        "details": "Prof. David Abramson1, Mr Jake Carroll, Mr Grant Smith, Mr Andrew Beattie\n1University Of Queensland, St Lucia, Australia\nDavid is an Emeritus Professor of Computer Science in the School of Electrical Engineering and Computer Science, at the University of Queensland. Between 2013 and 2024 he was the Director of the University of Queensland Research Computing Centre.\nHe has held appointments at Griffith University, CSIRO, RMIT and Monash University.\nPrior to joining UQ, he was the Director of the Monash e-Education Centre, Science Director of the Monash e-Research Centre, and a Professor of Computer Science in the Faculty of Information Technology at Monash.\nFrom 2007 to 2011 he was an Australian Research Council Professorial Fellow.\nDavid has expertise in High Performance Computing, distributed and parallel computing, computer architecture and software engineering.\nHe has produced in excess of 230 research publications, and some of his work has also been integrated in commercial products. One of these, Nimrod, has been used widely in research and academia globally, and is also available as a commercial product, called EnFuzion, from Axceleon.\nThe IBM UQ Centre of Excellence is a unique collaboration between IBM and the Research Computing Centre at the University of Queensland. It aims to develop industry solutions and impact across a wide range of applications, including health, environment and manufacturing. It draws on IBM technologies and expertise and leverages UQ’s excellence in both computing and research. The Centre expands IBMs network of government and industry partners, and delivers industry scale technical solutions in research computing, while providing leading edge technologies to UQ.\nFormed in 2020, the Centre has deployed novel platforms at UQ and has applied them to research from life sciences to the humanities. These platforms have benefited UQ through the early adoption of technologies improving time-to-outcome. Importantly, such early testing and adoption has also identified opportunities for product improvement, which have been fed back to IBM. In many cases feature enhancements have been integrated into products, and difficult edge cases and errors have been exercised and enhanced.\nSpecific examples of IBM’s technology include the application of IBM Storage Scale™ in UQ’s award winning MeDiCI data storage fabric, a multi-vendor platform that stores research data at scale. The partnership led to the development, testing and validation of AMD’s Instinct Series GPU accelerator for the WatsonX/AI platform with cutting edge Large Language Models (LLMs) – a world first in GPU agnostic AI deployments. In cloud-native storage, the partnership’s shared infrastructure allowed the testing, validation and verification of the IBM block Container Storage Interface on next generation Flashsystem™ storage devices."
    },
    {
        "day": "Wednesday",
        "time": "12:40 – 13:40",
        "location": "Boulevard Auditorium",
        "session_chair": "Alex Reid",
        "title": "Lunch, Exhibition, Poster Viewing | Boulevard Foyer",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Wednesday",
        "time": "13:40 – 14:00",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet",
        "title": "From Bench to GPU: A Biologist’s Journey into Virtualised Cryo-EM Data Processing on Bunya HPC",
        "authors": "Farrah Blades",
        "url": "https://conference.eresearch.edu.au/from-bench-to-gpu-a-biologists-journey-into-virtualised-cryo-em-data-processing-on-bunya-hpc/",
        "details": "Dr Farrah Blades1, Dr Edan Scriven2, Ms Sarah Walters2, Dr Marlies Hankel3, Mr Jake Carroll2\n1Institute For Molecular Biosciences (imb), University of Queensland, St Lucia, Australia,2Research Computing Center, University of Queensland, Australia, St Lucia, Australia,3Australian Institute for Bioengineering and Nanotechnology, University of Queensland, Australia, St Lucia, Australia\nFarrah completed her PhD in Neuroscience at the University of Melbourne in 2021, discovering a novel role for the Tyro3 receptor in myelination and retinal neuron health. She then joined the Hankamer group at UQ, leading structural studies on the photosystem II supercomplex and co-developing a virtual desktop platform powered by the Bunya supercomputer, improving HPC access for researchers. In 2024, Farrah joined the Cater group to investigate nutrient uptake at the blood-brain barrier. Today, she shares her perspective as a biologist working across disciplines and her role in championing UQ’s virtual desktop platform for structural biology.\nhttps://orcid.org/0000-0001-6926-0001\nAs structural biology embraces an era of big data and GPU-intensive workflows, traditional data processing methods are no longer keeping pace, at the University of Queensland (UQ) we have managed to solve this issue at scale for our researchers utilising Open OnDemand and clever integration to Bunya HPC. In this presentation, I share a biologist’s perspective on navigating this shift from broken traditional data processing methods, to leading the field as the new gold-standard thanks to an interdisciplinary collaboration with the Research Computing Centre and the structural biology community at UQ. Together, we developed a visual, on-demand virtual desktop platform for the Bunya supercomputer that enables complete Cryo-EM data processing, no command-line skills required.\nThis new environment integrates key software tools used in structural biology, including CryoSPARC, Relion, ModelAngelo, ChimeraX, Coot, and more, within a single virtual desktop interface. Users can process large datasets, visualize structures, and leverage A100, H100 and L40 GPUs with minimal technical overhead. By removing barriers to HPC access, this platform empowers researchers to focus on discovery, not infrastructure.\nI reflect on what made this project successful —clear communication across disciplines, shared vision, and a user-centred design. As compute becomes increasingly specialised, while scientific research demands higher compute, I argue that platforms like this are essential for bridging domain expertise and digital infrastructure, and I offer lessons for institutions seeking to support transdisciplinary science at scale."
    },
    {
        "day": "Wednesday",
        "time": "13:40 – 14:00",
        "location": "Boulevard B1",
        "session_chair": "Ai-Lin Soo",
        "title": "Intersect AI Training Bootcamps: Upskilling Researchers with Applied AI Skills",
        "authors": "Jiaxin Fan",
        "url": "https://conference.eresearch.edu.au/intersect-ai-training-bootcamps-upskilling-researchers-with-applied-ai-skills/",
        "details": "Dr Jiaxin Fan1, Mr Glen Charlton1, Dr Long Le1, Dr Yueyang Liu1, Ms Echo Zhou1, Dr Ghulam Murtaza1, Dr Anastasios Papaioannou2\n1Intersect Australia, Sydney, Australia,2University of Technology Sydney, Sydney, Australia\nJiaxin holds a PhD in Computational Materials Science from UNSW Sydney. He is currently working as a Research Data Scientist at Intersect Australia. He has over seven years of experience in Data Science, Machine Learning, scientific modelling, and digital skills training. He specialises in developing and deploying data solutions across diverse infrastructures, including high-performance computing and cloud platforms.\nBackground\nArtificial intelligence (AI) is becoming increasingly integral to research across disciplines, yet many researchers face challenges in acquiring practical AI and machine learning (ML) skills due to time constraints and technical complexity. To address this gap, Intersect Advanced Analytics and AI (3AI) Platform has developed a series of AI Training Bootcamps tailored to researchers seeking to integrate AI methodologies into their work.\nActions\nDesigned by 3AI’s Research Data Scientists, the Bootcamps independently cover four key AI domains (Generative AI and Large Language Models, Computer Vision, Machine Learning Operations and Reinforcement Learning). Each bootcamp contains five training days, with a flexible schedule distributed over an extended period to enable participants to balance learning with ongoing research commitments. The curriculum combines foundational theory, hands-on exercises, and project-based learning.\nResults\nThe outcome from the initial pilot at Intersect’s partner organisation has been exceptional. Participant engagement was strong, with an average attendance rate exceeding 95%, reflecting sustained interest and commitment throughout the program. The post-bootcamp survey returned a Net Promoter Score (NPS) of 60, with many participants praising the pedagogy, the depth, and relevance of the content.\nConclusion\nThis modular and research-focused bootcamp model presents an effective strategy to AI upskilling, which empowers researchers to incorporate AI methodologies in their respective domains. Future iterations will involve refining and developing new content, and expanding delivery across Intersect’s member organisations. With several bootcamps currently ongoing, we will present a more comprehensive report of the outcome at the conference."
    },
    {
        "day": "Wednesday",
        "time": "13:40 – 14:00",
        "location": "Boulevard B2",
        "session_chair": "Harshula Jayasuriya",
        "title": "A workflow editor and execution framework for transforming existing code into scalable, distributed workflows",
        "authors": "Ryan Bunney",
        "url": "https://conference.eresearch.edu.au/a-workflow-editor-and-execution-framework-for-transforming-existing-code-into-scalable-distributed-workflows/",
        "details": "Mr Ryan Bunney1,2, Professor Andreas Wicenec1,2, Mr Rodrigo Tobar1,2, Mr Nicholas Pritchard1,2, Mr James Strauss1,2, Mr Moritz Wicenec1,2\n1International Centre for Radio Astronomy Research, Perth, Australia,2University of Western Australia, Perth, Australia\nRyan is a Research Software Engineer at the International Centre for Radio Astronomy Research (ICRAR). He has 10 years experience developing software in both research and industry, and is passionate about improving the quality and access of software to academics. His research interests are in workflow scheduling, observatory operations, and high performance computing. He is a 2018 Westpac Future Leader Scholar.\nhttps://orcid.org/0000-0002-0246-1922\nScience workflows are a tool used to coordinate interdependent computing tasks at scale. Workflow management systems that support scientific workflows take control of scheduling and managing large-scale distributed computing, improving the scale of scientific research and facilitating more reproducible experiments. A challenge with these tools is that the time and resources invested in learning new tools limit their adoption.\nWe present the Data-Activate Graph Flow Engine (DALiuGE), a suite of software tools that aims to reduce this time overhead by enabling scientists and software engineers to reuse existing code in their workflows. In addition to providing standard management support for Bash applications and Docker/Singularity containers, scientists can integrate existing Python code directly into a DALiuGE workflow – often without rewrites.\nAdditionally, the Editor for the Astronomical Graph Language Environment – EAGLE – provides the ability to focus on the logic of the workflow – looping over data, adding conditional logic, etc – without concern for the runtime execution of the workflow. DALiuGE also has reproducibility tracking built into the system that will 'grade' workflow runs according to various levels of reproducibility.\nInitially developed for the Square Kilometre Array Science Data Processor, DALiuGE has demonstrated its scalability using the entire SUMMIT supercomputer and is actively being used to develop workflows for the CHILES, DINGO, and WALLABY surveys. We will present our experience in developing astronomy pipelines using these tools, discuss lessons learned in designing applications for scientists, and demonstrate their efficacy for workflow applications outside of astronomy."
    },
    {
        "day": "Wednesday",
        "time": "13:40 – 14:00",
        "location": "Boulevard B3",
        "session_chair": "Sarah Thomas",
        "title": "Blockchain Commons for Autonomous Digital Custodianship: Designing a Dual NFT infrastructure for Indigenous Food Sovereignty and Collective Benefit",
        "authors": "Shoufeng Cao",
        "url": "https://conference.eresearch.edu.au/blockchain-commons-for-autonomous-digital-custodianship-designing-a-dual-nft-infrastructure-for-indigenous-food-sovereignty-and-collective-benefit/",
        "details": "Mr Shoufeng Cao1, Prof. Kim Bryceson1\n1The University of Queensland, St Lucia, Australia\nShoufeng Cao is an honorary senior fellow at School of Agriculture and Food Sustainability, The University of Queensland. His research lies at the intersection of emerging digital technologies and real-world applications, with a particular focus on the exploration and development of blockchain and non-fungible token (NFT) infrastructure. He is dedicated to designing and evaluating innovative solutions that enhance transparency, traceability, and trust in industries such as agriculture, food production, and supply chain management. His research contributes to a broader vision of technological transformation that promotes social good, environmental stewardship, and economic inclusivity.https://orcid.org/0000-0001-5178-7454\nThe study addresses the intersection of indigenous food sovereignty and data sovereignty in the digital era by exploring community-governed digital infrastructures for indigenous bushfood systems. It explores the use of blockchain networks as a digital commons to safeguard transparent, tamper-proof records and ethical access to indigenosu data or knowledge. Through a participatory design approach embedded in cultural protocols and practices within the Australian bushfood sector, non-fungible tokens (NFTs) were designed to uphold indigenous sovereignty and collective benefit from the research, commerclisation and trade of bushfood species and derived products.\nThis study presented a blockchain-enabled NFT infrastructure incorporating traditional owner tokens (TOTokens) for representing resource and cultural custodianship and enabling usage tracking, and authentic provenance tokens (APTokens) for tracing bushfood provenance and associated traditional ownership. This dual NFT infrastructure design enables the unique digital representation of bushfood and associated traditional ownership, while also provides a socio-economic mechanism to monetise traditional ownership across bushfood research and commerce scenarios.\nThis dual NFT infrastructure is underpinned by smart contracts that enable the tradability and/or transferability of TOTokens and APTokens to automate governance rules, ethical access and collective benefit sharing, without reliance on external authorities. A proof-of-concept was piloted and tested on Polygon – a public blockchain – demonstrating its technical feasibility. The blockchain-based NFT infrastructure aligns with indigenous data sovereignty principles, CARE and FAIR data frameworks, and can integrate with Internet of things (IoTs), AI, machine learning and data analytics to conduct culturally grounded and ethics-controlled deep eResearch for business innovation and industry practice."
    },
    {
        "day": "Wednesday",
        "time": "13:40 – 14:00",
        "location": "Boulevard Room",
        "session_chair": "Peter Sefton",
        "title": "The Triple Point: Building a medical research metadata system using RO-Crate and linked data principles",
        "authors": "Michael Milton",
        "url": "https://conference.eresearch.edu.au/the-triple-point-building-a-medical-research-metadata-system-using-ro-crate-and-linked-data-principles/",
        "details": "Mr Michael Milton1\n1Walter and Eliza Hall Institute of Medical Research, Parkville, Australia\nMichael Milton has worked as a bioinformatician and research software engineer in Melbourne for the past 9 years, with an ongoing focus on developing high-quality open-source software. Michael is currently part of WEHI's Research Computing Platform, who aim to facilitate medical research software through project collaborations, developing software and providing training.https://orcid.org/0000-0002-8965-2595\nMetadata needs to play a key role in WEHI's new research data management system. We must ensure that future researchers can identify the files they need from a huge collection of past data, along with the process that originally created them. Building a system that supports this is filled with difficult decisions that will be explored in this talk.\nWe will firstly examine some metadata standards that were considered as the basis for the system. In particularly, we will explain the advantages of linked data that encouraged us to build a metadata system on RO-Crate.\nWhen a new dataset is processed by the system, baseline metadata is generated based on prior process modelling. This step is powered by RdfCrate, a WEHI-developed Python package. Next, the metadata is reviewed by the scientists involved in generating the data, which required a solution for graphically summarising and editing RO-Crates. Finally, the system validates the final crate using profiles built with SHACL, allowing the enforcement of minimum metadata standards.\nOnce a crate is safely in the research data system, the metadata is used for a number of purposes. These include exporting image data crates to OMERO with attached metadata, searching past crates with the SPARQL query language, and exporting metadata into a format for database or journal submission."
    },
    {
        "day": "Wednesday",
        "time": "14:00 – 14:20",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet",
        "title": "Setting up Cryo-EM tools and workflows in HPC Phoenix : lessons learnt",
        "authors": "Fabien Voisin",
        "url": "https://conference.eresearch.edu.au/setting-up-cryo-em-tools-and-workflows-in-hpc-phoenix-lessons-learnt/",
        "details": "Dr Fabien Voisin1\n1University Of Adelaide, Adelaide, Australia\nAfter finishing my PhD in high energy astrophysics at the University of Adelaide, I have started my journey in high performance computing. I have since helped countless researchers and projects with their research compute needs; from troubleshooting codes to setting up cloud environment that could be used in parallel to our HPC Phoenix.\nI thrive in tackling complex research infrastructure problems and still count on getting back to academia when the opportunity presents itself.\nMy ORCiD ishttps://orcid.org/0009-0004-6872-9858\nThe Adelaide Microscopy has acquired a FEI Glacios 200kV Cryo-Transmission Electron Microscope. This instruments easily generates Terabytes of data that need to be processed as soon as possible. We have opted to make use of our existing storage and compute infrastructure rather than buy more local machines for data processing. Notably, we offered our HPC GPU capabilities (i.e 200x 40GB A100 SXMs) and high performance scratch storage to minimize bottlenecks, with dedicated GPU nodes to account for Cryosparc live processing.\nIn this talk, we will highlight the workflow and infrastructure linked to this setup. While we show that it has shown good performance and scale, as we can easily add GPU resources when demands arise, we will also discuss some serious challenges commonly associated with shared infrastructures: from Cryotools software installations and setup to data transfer and data management."
    },
    {
        "day": "Wednesday",
        "time": "14:00 – 14:20",
        "location": "Boulevard B1",
        "session_chair": "Ai-Lin Soo",
        "title": "The AI Carrots Project: building supportive resources for researchers",
        "authors": "Cameron Fong, Darya Vanichkina",
        "url": "https://conference.eresearch.edu.au/the-ai-carrots-project-building-supportive-resources-for-researchers/",
        "details": "Dr Cameron Fong1, Dr Darya Vanichkina2, Dr Rose Smail2, Dr Eden Zhang2, Dr Angus Fisk2\n1The University of Sydney – RIEA, Australia,2The University of Sydney – SIH, Australia\nCameron engages with researchers, faculty, professional staff, and students to promote, improve, develop, and implement RDM best practices. This varies from developing resources, providing support and training for the research community, working with individual researchers to develop tailored solutions, to collaborating with peers at different institutions.\nORCID:https://orcid.org/0000-0002-4558-3700\nDarya Vanichkina PhD SFHEA is the Data Science & AI Group Lead at the Sydney Informatics Hub (SIH), a Core Research Facility of the University of Sydney dedicated to enabling excellence in data and compute-intensive research. Darya leads a team that delivers consultancy services and training to boost research outcomes and funding, accelerate projects, and foster partnerships with industry and government. In 2025, her team is championing the adoption of AI for research across the University’s faculties and affiliates.\nhttps://orcid.org/0000-0002-0406-164X\nRose Smail –https://orcid.org/0000-0002-7292-024X\nEden Zhang –https://orcid.org/0000-0003-0294-3734\nAngus Fisk –https://orcid.org/0000-0003-2777-4439\nThe rapid implementation of GenAI tools across universities has been spearheaded by undergraduate teaching, with support and resources for researchers lagging behind. This creates a dangerous situation where researchers see, hear and feel limitations around the use of GenAI from their institutions while simultaneously bombarded by questionable practices in academic journals, social media and the newsl. Add in the confusing messaging for those who broach both research and education – and we have the precursors for risky GenAI use.\nThere is a need to develop and promote exemplar ways of using GenAI for common research tasks to accelerate adoption and nurture innovation while reducing risk.\nAt the University of Sydney, the Sydney Informatics Hub initiated a cross-unit collaboration to run a co-creation session to develop such exemplar use cases. In this presentation, we’ll discuss the process we followed and challenges we ran in to when building this collaboration, recruiting researchers to contribute, and negotiating with stakeholders across the university to ensure developed materials were accepted by the diverse cohort of concerned parties.\nThe University of Sydney has established a dedicated hub for the responsible research use of GenAI, providing researchers a curated resource pool that champions innovation and best practice. eResearch conference attendees from other institutions interested in developing similar resources in partnership with their researchers can benefit from our implementation experience and lessons learned, facilitating a more streamlined development process at their own institutions."
    },
    {
        "day": "Wednesday",
        "time": "14:00 – 14:20",
        "location": "Boulevard B2",
        "session_chair": "Harshula Jayasuriya",
        "title": "Workflow-installer: Simplifying Nextflow  Workflow Deployment on HPC Clusters",
        "authors": "Kisaru Liyanage",
        "url": "https://conference.eresearch.edu.au/workflow-installer-simplifying-nextflow-workflow-deployment-on-hpc-clusters/",
        "details": "Dr Kisaru Liyanage1, Dr Matthew Downton1\n1National Computational Infrastructure, Canberra, Australia\nDr Kisaru Liyanage is a High Performance Computing Specialist in Life and Health Sciences at the National Computational Infrastructure (NCI), Australia. He earned his PhD in Computer Science and Engineering from UNSW Sydney, where he focused on accelerating genomics software through hardware–software co-design. His current research interests include accelerated and high-performance computing, bioinformatics, and systems-on-chip.\nNextflow workflows are widely used in bioinformatics, but deploying them on shared HPC clusters can be complex. Cluster environments often require fine-tuned configurations, and systems with restricted internet access necessitate time-consuming offline preparation of workflow dependencies, such as container images and offline-ready configuration files. These challenges impede workflow adoption on HPC clusters.\nWe present workflow-installer, a lightweight framework that can be used to centrally deploy versioned Nextflow pipelines as environment modules with configurations optimised for specific HPC clusters. An installer script created for each workflow automates tasks such as pulling workflow source code, applying patches for offline execution, retrieving Singularity container images, downloading test datasets, configuring a script for registering workflow on Seqera platform, copying cluster-specific configurations and documentation, and generating modulefiles. workflow-installer includes several helper functions for creating reproducible, robust installation scripts (e.g., caching sources, transforming container URIs).\nAfter deployment, users can load a workflow with “module load <workflow>/<version>” and execute it completely offline with cluster-specific optimisations. Using the pre-downloaded test datasets and example commands in the documentation, users can validate installations with minimal effort. The workflow can also be registered on the Seqera platform using the preconfigured script. Users experience virtually zero setup time and benefit from improved performance due to tailored configurations.\nworkflow-installer lowers the barrier to using Nextflow workflows on HPC, enabling more reproducible and efficient bioinformatics analyses. By shifting workflow setup to a single module load, researchers focus on science rather than infrastructure, making HPC resources more accessible to the life sciences community."
    },
    {
        "day": "Wednesday",
        "time": "14:00 – 14:20",
        "location": "Boulevard B3",
        "session_chair": "Sarah Thomas",
        "title": "Australian Trust and Identity Roadmap for National Research Infrastructure: Connecting Australia’s national digital research ecosystem simply and securely",
        "authors": "Heath  Marks, Margie Jantii",
        "url": "https://conference.eresearch.edu.au/australian-trust-and-identity-roadmap-for-national-research-infrastructure-connecting-australias-national-digital-research-ecosystem-simply-and-securely/",
        "details": "Mr Heath Marks1, Ms Margie Jantii1, Mrs. Sarah Thomas1\n1Australian Access Federation, Brisbane, Australia\nHeath Marks was appointed by the Council of Australian University Directors of Information Technology (CAUDIT) in July 2009 to head a team to deliver the sustainable operations of Australia’s Trust and Identity services for Research and Education. This includes the national trust authentication framework the Australian Access Federation (AAF), and the Open Researcher and Contributor ID (ORCID) Consortium Lead for Australia. He is an IT professional with management experience in the successful delivery of transformational IT within the tertiary education and research sector supporting the National Collaborative Research Infrastructure Strategy (NCRIS).\nMargie Jantti is AAF’s Deputy CEO and throughout her career has held many significant leadership roles, including Director Library Services University of Wollongong (UOW), former Chair of the Council of Australasian University Librarians (CAUL), former member and Chair of the ORCID Governance Group, former Board Director of the AAF, Board Director of CAVAL, and member of the Cambridge University Press Australian and New Zealand Advisory Board.\nTrust and identity underpin every aspect of modern research; it is fundamental infrastructure that enables a seamless, secure, efficient and globally connected ecosystem for Australian researchers.\nFunded by the Australian Government’s 2021 National Research Infrastructure Roadmap, the Australian Access Federation (AAF) has been leading a collaborative journey with the national research infrastructure community to define and implement the system-wide adoption of trust and identity.\nThe strategic vision is to create a fully connected Australian research ecosystem and a future where all national research infrastructures, universities, industry and government users can simply and securely connect.\nDrawing upon insights gained through community collaboration, the AAF has identified six priority areas that must be addressed to unlock the full potential of trust and identity within the community. This presentation will delve into these priority areas, outline the proposed Australian Trust and Identity Roadmap for National Research Infrastructure, and explore how the research and education sector can be involved in this transformative process.\nPlease join us to understand how these priority areas can enable access to valuable services, technologies, data and compute, to empower the Australian research community."
    },
    {
        "day": "Wednesday",
        "time": "14:00 – 14:20",
        "location": "Boulevard Room",
        "session_chair": "Peter Sefton",
        "title": "Low friction FAIR interoperability using RO-Crate metadata in  text analytics pipelines",
        "authors": "Peter Sefton, Rosanna Smith, Michael Lynch",
        "url": "https://conference.eresearch.edu.au/low-friction-fair-interoperability-using-ro-crate-metadata-in-text-analytics-pipelines/",
        "details": "Dr Peter Sefton1, Ms Rosanna Smith1, Dr Simon Musgrave1, Mr Michael Lynch2\n1University Of Queensland, St Lucia, Australia,2Univerity of Sydney, Camperdown, Australia\nPeter Sefton is an eResearch expert, specialising in software development, Research Data Management and metadata, currently a senior advisor for the Language Research Data Commons project at the University of Queensland.\nRosanna is a Language Technology Analyst in the LDaCA team. She completed Honours in linguistics at Monash University studying morphology in Scandinavian languages and has previously worked as a linguist and project manager in language technology.\nMike is a software engineer with over ten years experience providing specialised support for research, with expertise in research data management, open standards for data repositories and data publication and the application of modern IT development and deployment practices to research software. He has experience in full-stack web development in Python and JavaScript, machine learning, natural language processing and data visualisation, and is interested in digital humanities and functional programming.\nResearch Object Crate (RO-Crate) is a simple method for linked-data description and packaging of data. Since 2021, the Language Data Commons of Australia (LDaCA) project has onboarded a number of language data collections with thousands of files. These are all consistently described as RO-Crates using a human and machine-readable Metadata Profile, discoverable through an online portal, and available via an access-controlled API. This presentation will show how analytics workflows can be connected to data in the LDaCA repository and use linked data descriptions, such as the W3C “CSV for the web” (CSVW) standard, to automatically detect and load the right data for analytical workflows. We will show how the general-purpose flexible linked metadata and raw data is prepared for use with common tools implemented in Jupyter notebooks.\nThis work, funded by the Australian Research Data Commons ARDC, has enabled novel research by making data collected using sub-disciplinary norms of linguistics available to researchers working in other specialised areas – we will show examples of this and how this approach is relevant to other HASS and STEM disciplines, demonstrating work which would not have been possible without this co-investment between the Language Data Commons partners and ARDC\nThe presentation should be accessible to the broad audience of eResearch and be of particular relevance to those with an interest in workflows and analytics, as well as metadata, vocabulary and repository specialists. It shows a FAIR research system which runs on open specifications and code and can be redeployed for other domains."
    },
    {
        "day": "Wednesday",
        "time": "14:20 – 14:40",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet",
        "title": "Accelerate research and reduce costs with cloud optimised storage formats",
        "authors": "Steven Gillard",
        "url": "https://conference.eresearch.edu.au/accelerate-research-and-reduce-costs-with-cloud-optimised-storage-formats/",
        "details": "Mr Steven Gillard1\n1Amazon Web Services, Melbourne, Australia\nAs an AWS Senior Technologist and Solutions Architect, Steve brings nearly three decades of expertise in designing and implementing mission-critical and large scale data systems across Telco, Internet, Retail, Government, and Higher Education sectors. Based in Melbourne, he specialises in architecting secure, resilient, and cost-optimised solutions for ANZ public sector organisations.\nSteve holds AWS certifications as an AI Practitioner and Solutions Architecture Professional, is a member of the AWS Resilience Specialist Field Community, and earned his Bachelor of Applied Science in Computer Science from RMIT.\nSteve is passionate about leveraging cloud technologies to accelerate scientific research in climate change and healthcare innovation. His work focuses on helping research institutions harness the power of AWS to drive breakthrough discoveries and improved outcomes.\nAs research datasets grow to petabyte scale, traditional data formats like CSV and NetCDF are becoming significant bottlenecks in cloud environments, leading to spiraling costs, slow query times and frustrated researchers. The solution lies in cloud-optimised storage formats that can deliver both dramatic performance improvements and substantial cost savings.\nModern cloud-optimised formats leverage the distributed nature and high throughput capabilities of cloud object storage systems like Amazon S3. By enabling compression, selective data access and parallelised scale-out reading and writing of data without requiring intermediate file system layers, these formats can improve performance by 10x or more while reducing storage and computing costs by 65%. For researchers, this means faster results and more time focusing on science rather than data movement.\nThis talk introduces cloud-optimised formats for different data types: Parquet for tabular data, Cloud Optimized GeoTIFF for imagery, and Zarr for multi-dimensional data. We'll also explore emerging open source technologies like Kerchunk, VirtualiZarr, and Icechunk that can achieve similar benefits without data conversion – a game-changer for organisations with large legacy datasets. You'll leave with practical strategies for data layout and chunking that balance performance with flexibility, plus clear guidance on choosing the right format for your specific use case."
    },
    {
        "day": "Wednesday",
        "time": "14:20 – 14:40",
        "location": "Boulevard B1",
        "session_chair": "Ai-Lin Soo",
        "title": "Advancing AI Literacy for Responsible Research at Scale",
        "authors": "Kyle Hemming, Erin Wood",
        "url": "https://conference.eresearch.edu.au/advancing-ai-literacy-for-responsible-research-at-scale/",
        "details": "Dr Kyle Hemming1, Dr Erin Wood1\n1University Of Auckland, Aotearoa / New Zealand\nDr. Kyle Hemming is a Senior eResearch Engagement Specialist at the University of Auckland. He supports researchers in data science, data management, and responsible AI adoption. Kyle has spent a decade undertaking quantitative research and supporting researchers. He is also passionate about improving research outcomes through improving reproducible research, strategic planning, and stakeholder engagement.\nDr. Erin Wood is Research Services Adviser at the University of Auckland. She provides support to postgraduate, doctoral candidates and staff in areas such as research outputs and discovery, artificial intelligence application in scholarly communication, as well as developing and delivering research skills trainings and workshops. Erin has a PhD in protein expression dynamics from the University of Waikato.\nBackground\nArtificial Intelligence (AI) is transforming research practices. Readily accessible generative AI tools are enhancing the quality, breadth, and impact of research. However, this rapid uptake brings significant challenges, particularly around ethical use, compliance with security, and data privacy. Additionally, data biases and result fabrications are concerns are barriers to realising the benefits of AI for many researchers.\nTo ensure responsible and broad use of AI, it is essential researchers understand the context of their data, how AI tools generate outputs, and how they operate within institutional and societal contexts.\nActions\nWaipapa Taumata Rau, University of Auckland has funded an AI literacy initiative. This initiative includes workshops, self-service resources, and community engagement. We have developed and delivered workshops in Responsible AI, for researchers and for supervisors, Transcription, Sentiment Analysis, Literature Reviews, and Prompt Engineering; we have enhanced one-on-one support, advertised University-approved tools (e.g. Nectar Transcription Virtual Desktop), and published online resources.\nResults\nThere is significant interest and engagement from researchers in the AI workshops. We are also identifying priority knowledge gaps and concerns, and we are working within the University to improve AI infrastructure and guidance.\nConclusion\nOur goal is to empower researchers to confidently use AI tools while upholding research integrity and ethical standards. This presentation will share our approach, key findings, and lessons learned from this initiative so far. Attendees will gain insight into how a large research institution is fostering AI literacy, where critical gaps remain, and strategies to maintain relevant as AI continues to evolve."
    },
    {
        "day": "Wednesday",
        "time": "14:20 – 14:40",
        "location": "Boulevard B2",
        "session_chair": "Harshula Jayasuriya",
        "title": "Modernising building of bespoke software environments for eResearch and sustaining global community engagement",
        "authors": "Modernising building of bespoke software environments for eResearch and sustaining global community engagement",
        "url": "https://conference.eresearch.edu.au/modernising-building-of-bespoke-software-environments-for-eresearch-and-sustaining-global-community-engagement/",
        "details": "Mr Harshula Jayasuriya1, Mr Aidan Heerdegen1, Dr Paul Leopardi1, Mr Tommy Gatti1, Mr Gregory Becker2, Dr Emily Kahl3\n1ACCESS-NRI, Australia,2Lawrence Livermore National Laboratory, USA,3Pawsey Supercomputing Research Centre, Perth, Australia\nGregory Becker is a computer scientist at Lawrence Livermore National Laboratory. He has worked on Spack since 2015, and represents the project on the technical advisory committee of the High Performance Software Foundation. He has done additional research in scalable I/O, ABI compatibility, and benchmark reproducibility. Gregory has been at LLNL since 2015. He received his B.A. in Computer Science and Mathematics from Williams College in 2015.\nHarshula is a member of the Model Release team at ACCESS-NRI. He is Free/Open Source Software advocate and a Debian Developer with two decades of experience working with software build systems. Once upon a time, he was a Linux/Unix filesystem engineer working on NFS and UDF at Red Hat Inc and Silicon Graphics Inc.\nAidan is the model release team lead for ACCESS-NRI. He is passionate about making software work for people. In previous roles he provided computational support to climate researchers in two ARC Centres of Excellence, and prior to that was a physical chemist working in a software role supporting X-ray scattering research.\nPaul Leopardi is a Research Software Engineer at ACCESS-NRI, currently working on supporting an Earth System Model. His qualifications include a PhD in computational mathematics and experience in teaching numerical linear algebra, optimization and combinatorics. His 40 year background in computing includes working with parallel Fortran codes, High Performance Computing and postprocessing of operational weather forecasting data.\nTommy is a Research Software Engineer for the Model Release Team at ACCESS-NRI. He's a multidisciplinary programmer who enjoys developing CI/CD infrastructure and technical pathfinding in different domains. In previous industry roles he has been a driver developer and member of a third level support team.\nEmily is a supercomputing applications specialist at the Pawsey Supercomputing Research Centre, where she develops and maintains software for computational omlecular science. She has extensive experience developing and supporting software for molecular dynamics and quantum chemistry, and has spent her fair share of time wrestling with build systems in HPC.\nDo you build and deploy complex customised software at HPC centres? Are you a researcher that develops and builds software with complex dependencies that may need to be run at multiple HPC centres? What do Amazon, AMD, CERN, Fermilab, Google, INTEL, LLNL, Microsoft, NOAA, NVIDIA, Riken have in common with ACCESS-NRI, the Bureau of Meteorology and Pawsey?\nThe answer is Spack! A leading international Spack expert and core-developer, Greg Becker, will describe the Spack 1.0 release, its benefits for users, strategies for seamlessly upgrading to Spack 1.0 and upcoming features. This will be followed by a discussion to elicit feedback from the community.\nAdditionally, we will discuss the successful community engagement practices undertaken by the Spack project to facilitate and support ACCESS-NRI and Pawsey to meet their objectives.\nTwo case studies will be presented: Pawsey will describe how it developed a customised Spack configuration and integrated regression testing using the ReFrame framework, allowing automated checks on software installations, production of modules and performance of specific packages; and ACCESS-NRI will describe how Spack is used to develop and deploy climate models for climate researchers to undertake climate simulations for the Coupled Model Intercomparison Project 7.\nFollowing our well-attended eResearch Australia 2024 BoF, the interest in Spack sparked a monthly Spack user meeting during Australian business hours involving Spack specialists across the USA, Australia and Japan. This BoF will continue the momentum by establishing further collaboration between the eResearch community in Australasia and Spack core-developers."
    },
    {
        "day": "Wednesday",
        "time": "14:20 – 14:40",
        "location": "Boulevard B3",
        "session_chair": "Sarah Thomas",
        "title": "Trust and Identity for National Research Infrastructure: Codesigning a system-wide approach",
        "authors": "Trust and Identity for National Research Infrastructure: Codesigning a system-wide approach",
        "url": "https://conference.eresearch.edu.au/trust-and-identity-for-national-research-infrastructure-codesigning-a-system-wide-approach/",
        "details": "Mrs. Sarah Thomas, Dr Fahame Emamjome\n1Australian Access Federation, Brisbane, Australia\nSarah Thomas is a Portfolio Manager at the Australian Access Federation (AAF). She has been working on the delivery of Trust and Identity for national research infrastructure, partnering with the community on co-designing a unified approach to trusted and seamless access for Australia's research sector. An enthusiastic and dedicated community engagement manager, with demonstrated expertise in research and innovation ecosystems. Committed to using her skills in relationship management, communication, program design, delivery and evaluation to deliver positive outcomes for the community.\nFahame Emamjome is an eResearch Analyst at the Australian Access Federation (AAF). She has been working on the delivery of Trust and Identity for national research infrastructure, co-designing a unified approach to trusted and seamless access for Australia's research sector. Fahame has a PhD in Information Systems from Queensland University of Technology (QUT), with several years of experience as a post-doctoral researcher and business analyst. She is passionate about technology and innovation, especially in supporting the research sector.\nThe Australian Government's 2021 National Research Infrastructure (NRI) Roadmap identified the importance of trust and identity (T&I) for national research infrastructure and how it could be strengthened to support secure, scalable, and globally connected research.\nThe Australian Access Federation (AAF) is funded to work with the community, and has partnered on six incubator projects with national research infrastructure providers, the National Imaging Facility, Pawsey Supercomputing Research Centre, NCI – National Computational Infrastructure, ARDC – Australian Research Data Commons, ACCESS-NRI, and Microscopy Australia, to uncover emerging T&I use cases and test solutions aligned with international standards.\nThis Birds of a Feather (BoF) session will present insights and practical findings from these projects to date.\nJoin us as we share what we’ve learned, identify patterns across different infrastructures, and invite input on emerging or unresolved use cases. Your contributions will help further shape a responsive and inclusive Australian Trust and Identity Framework for National Research Infrastructure.\nWe will cover recurring challenges including identity assurance, unique identifiers, policies, and how to collaborate across communities and infrastructures. We will open the conversation to hear similar or different use cases from the community, as well as international best-case solutions, and the suitability of these approaches in the Australian context.\nWhether you’re facing similar challenges or designing new services, this session aims to foster collaboration and co-design across the sector.\nThese real-world examples and discussions will highlight and illustrate the opportunities and benefits on building a coherent, federated identity landscape across Australia’s research ecosystem."
    },
    {
        "day": "Wednesday",
        "time": "14:20 – 14:40",
        "location": "Boulevard Room",
        "session_chair": "Peter Sefton",
        "title": "Someone said  to do RO-Crates? RO-Crate Birds of a Feather session meet colleagues and give feedback",
        "authors": "Someone said  to do RO-Crates? RO-Crate Birds of a Feather session meet colleagues and give feedback",
        "url": "https://conference.eresearch.edu.au/someone-said-to-do-ro-crates-ro-crate-birds-of-a-feather-session-meet-colleagues-and-give-feedback/",
        "details": "Dr Peter Sefton1, Ms Rosanna Smith1, Dr Nichola Burton, Mr Robert Fleet2, Mr Michael Lynch3\n1University Of Queensland, Australia,2QUT,3University of Sydney,\nPeter Sefton is an eResearch expert, specialising in software development, Research Data Management and metadata, currently a senior advisor for the Language Research Data Commons project at the University of Queensland.\nNichola has a background in psychology research. She works with the HASS and Indigenous research communities to find out what their data and digital research requirements are and develop infrastructure to meet their needs.\nRosanna is a Language Technology Analyst in the LDaCA team. She completed Honours in linguistics at Monash University studying morphology in Scandinavian languages and has previously worked as a linguist and project manager in language technology.\nMike is a software engineer with over ten years experience providing specialised support for research, with expertise in research data management, open standards for data repositories and data publication and the application of modern IT development and deployment practices to research software. He has experience in full-stack web development in Python and JavaScript, machine learning, natural language processing and data visualisation, and is interested in digital humanities and functional programming.\nRobert Fleet is a Senior Data Scientist/Developer at QUT’s Research Infrastructure, specialising in AI implementation and social network analysis. With extensive experience across Australian universities and research institutions, he brings expertise in machine learning, data visualization, and research software development.\nRO-Crate (Research Object Crate) is a specification for packaging data for distribution, storage and analysis that is being adopted globally particularly in Europe and Australia. For example, many of the ARDC’s Community Data Lab projects have named RO-Crate in their plans.\nThis session aims to provide a venue for RO-Crate adopters and would-be adopters in Australasia to build networks, and give feedback on the specification and the tools and the funding environment. The structure will be a panel-led discussion of a number of topics. We will take contributions to a shared google document, and the moderator and panelists will choose observations and questions from the document and invite discussion. We will not risk going over time with talks at the beginning but each panelist will get two minutes to speak about the biggest issue they see in RO-Crate adoption for their community.\nSefton and Lynch are members of the RO-Crate steering committee.\nThe list of panelists will be finalised closer to the conference but we are approaching people from a range of projects across HASS and STEM disciplines and aim to have a diverse set of perspectives from implementers and trainers to program-level managers."
    },
    {
        "day": "Wednesday",
        "time": "14:40 – 15:00",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet",
        "title": "A Cat In The Matrix",
        "authors": "Gareth Elliott",
        "url": "https://conference.eresearch.edu.au/a-cat-in-the-matrix/",
        "details": "Mr Gareth Elliott1\n1DSTG, Australia\nGareth is currently working with researchers at DSTG in several areas of digital science. He previously undertook research as a computational chemist.\nMany scientific calculations scale easily on a higher number of CPUs or GPUs, allowing for a faster simulation time, or the ability to perform a more accurate or more complicated calculation in the same amount of time. However, some calculations are limited by poor scaling, licensing or limited access to compute hardware. This necessitates them to simply run for a longer period of time to achieve the desired result. For example, gathering sufficient statistics from molecular dynamics, convergence of a geometry optimisation, reaching an optimisation threshold or sufficiently searching a feature space. Many software packages offer a checkpoint/restore functionality which allow for a disaster recovery should there be a hardware failure, or to allow the running of a long job where a wall time limit exists (such as usually implemented on a HPC system). However, what happens when researchers want to use software that does not offer this functionality, and they want to run it uninterrupted for over a month? This talk will discuss the use of generic checkpointing software to implement that functionality allowing the researchers to achieve robustness against hardware failures and wall time constraints where not previously possible."
    },
    {
        "day": "Wednesday",
        "time": "14:40 – 15:00",
        "location": "Boulevard Room",
        "session_chair": "Peter Sefton",
        "title": "Fostering an AI-enabled Scientific Future in Australia with the Google Developer Group AI for Science",
        "authors": "Fostering an AI-enabled Scientific Future in Australia with the Google Developer Group AI for ScienceNathaniel Butterworth",
        "url": "https://conference.eresearch.edu.au/fostering-an-ai-enabled-scientific-future-in-australia-with-the-google-developer-group-ai-for-science/",
        "details": "Dr Nathaniel Butterworth1\n1Google, Sydney, Australia\nNate is the Science Catalyst Program Manager working at Google, uniting the amazing research taking place around the world. He has an Honours in Astrophysics, a PhD in Geophysics, and a long tenure of using research computing infrastructure to help solve tricky science problems.\nThe rapid advancements in Artificial Intelligence (AI) present unprecedented opportunities for scientific discovery. However, the adoption and effective integration of these powerful tools within the Australian research landscape still faces challenges. This lightning talk will introduce a newly established community dedicated to accelerating the use of AI tools in Australian scientific research, the Google Developer Group (GDG) AI for Science.\nWith the support of Google behind it, the GDG AI for Science provides a vital platform for researchers across public and private sectors to connect, share knowledge, and learn from peers and industry experts. Our mission is to foster a thriving ecosystem of AI-powered scientific discovery by empowering scientists with the tools, skills, and collaborative environment needed to leverage advanced computing for their work. This talk will briefly highlight the community's objectives, initial activities, and how it is enabling researchers to unlock new insights, streamline workflows, and tackle complex scientific challenges. Attendees will gain an understanding of how this community is addressing the critical need for greater AI literacy and collaboration within Australian science, ultimately contributing to a more innovative and impactful research future."
    },
    {
        "day": "Wednesday",
        "time": "15:00 -15:20",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet",
        "title": "Rust for Scientific Computing – It’s Good!",
        "authors": "Emily Kahl",
        "url": "https://conference.eresearch.edu.au/rust-for-scientific-computing-its-good/",
        "details": "Dr Emily Kahl1\n1Pawsey Supercomputing Research Centre, Perth, Australia\nEmily Kahl is a Supercomputing Applications Specialist at the Pawsey Supercomputing Research Centre. She develops and maintains software for molecular simulation, with a specific focus on GPU-accelerated computing and machine learning methods in quantum chemistry. She has extensive experience developing and supporting software for computational chemistry and molecular modeling and has contributed to multiple open-source molecular dynamics projects.\nEmily is also an advocate for open-source software in computational science, and the code she has developed for atomic and molecular simulation has seen widespread use by Australian and international researchers. Prior to joining Pawsey, Emily completed her PhD in physics at the University of New South Wales and worked as a research software engineer at the University of Queensland.\nThe Rust programming language has rapidly become one of the most popular and well-loved languages for system programming, due to its focus on performant memory- and thread-safety, sophisticated toolchains, and commitment to fostering an inclusive and diverse developer community. It has seen widespread adoption in applications ranging from web browsers to hardware device drivers and operating system kernels.\nBut despite its performance and safety benefits, Rust has seen comparatively little adoption in the research computing space. I believe that this is a missed opportunity.\nThis talk will provide a brief overview of the Rust programming language, with a focus on technical features that are well-suited to the unique challenges of scientific programming. I will also discuss some of the issues that have blocked more widespread adoption in research, and finish with some examples of scientific computing projects that have successfully used Rust to improve their robustness and performance."
    },
    {
        "day": "Wednesday",
        "time": "15:50 – 14:10",
        "location": "Boulevard Auditorium",
        "session_chair": "Deanna Deveson Lucas",
        "title": "End to end workflows in Ecoacoustics: Verifying AI Output",
        "authors": "Anthony  Truskinger",
        "url": "https://conference.eresearch.edu.au/end-to-end-workflows-in-ecoacoustics-verifying-ai-output/",
        "details": "Ms Nelli Holopainen1, Dr Anthony Truskinger, Professor Paul Roe\n1Queensland University of Technology, Australia\nDr Anthony Truskinger is a Research Software Engineer at Open Ecoacoustics and Queensland University of Technology. Since 2009, he has developed tools that help ecologists monitor biodiversity through sound. He leads development of the Acoustic Workbench, an open-source platform for managing and analysing large-scale ecoacoustic data. Anthony’s work spans backend and frontend development, systems administration, and mentoring. He is passionate about usability, reproducibility, and making research software a first-class research output. His contributions are central to the Open Ecoacoustics initiative, supported by the Australian Research Data Commons (ARDC) through the Planet RDC program.\nOrcID:https://orcid.org/0000-0003-1110-1483\nMonitoring biodiversity at scale is critical for conservation, yet many of Australia’s most threatened species are cryptic, rare, or inhabit hard-to-reach places. Traditional survey methods are often limited in reach and repeatability. Passive acoustic monitoring offers a practical, repeatable, and non-invasive way to detect presence, assess diversity, and monitor ecological change; all by listening to the soundscapes of Country.\nWe at Open Ecoacoustics are continuously developing our Acoustic Workbench software, an open-source system that enables users to upload, annotate, and analyse large volumes of acoustic data. This software integrates AI models such as Google’s Perch and Cornell’s BirdNET to automate species identification. We work in partnership with NGOs, government agencies, and citizen scientists, supporting diverse ways of working. The Acoustic Workbench software has enabled long-term, landscape-scale monitoring across diverse ecosystems. In several case studies, acoustic data has revealed greater species diversity than traditional methods. Automated analysis has significantly reduced the time and expertise required to process recordings, supporting faster, evidence-based decision-making in conservation and land management. This presentation will cover improvements to our analysis workflows, particularly in relation to the verification of AI generated results.\nOpen Ecoacoustics is a project supported by the Australian Research Data Commons (ARDC) through the Planet RDC program. This infrastructure is designed to support machine observation and data processing at scale. By integrating open platforms and AI, we are deepening ecological insight, detecting early signs of decline, and empowering communities to take informed action."
    },
    {
        "day": "Wednesday",
        "time": "15:50 – 14:10",
        "location": "Boulevard B1",
        "session_chair": "Jana Makar",
        "title": "That’s FAIR enough! Targeted FAIR Skills Training and Support in the Australian Grains Industry",
        "authors": "Melanie Dixon",
        "url": "https://conference.eresearch.edu.au/thats-fair-enough-targeted-fair-skills-training-and-support-in-the-australian-grains-industry/",
        "details": "Ms Melanie Dixon1, Erin Elstermann1, Jane Gibberd1, Mr John Brown1, Dr Fatima Naim1\n1Curtin University, Bentley, Australia\nMelanie is the Extension Coordinator for the DataHarvest Project, in this role she collaborates with researchers in the Australian grains industry to improve their research data management skills and practices.\nShe holds a Master of Data Science and is deeply committed to supporting the agricultural industry by promoting best-practice research data management, ensuring that the data generated, along with the time and effort invested in it, delivers maximum value.\nAs global research data grows in complexity and volume, effective data management, access, and reuse have become increasingly critical. The FAIR principles offer a framework for improving data stewardship. While their adoption grows globally, research organisations in the Australian grains industry continue facing barriers to consistent and effective implementation. Time constraints, competing priorities, lack of specialised training often prevent these organisations from realising the long-term benefits of research data management (RDM). Inconsistent understanding and interpretation of FAIR also contribute to fragmented implementation across the sector. To address these challenges, the Grains Research and Development Corporation (GRDC) invested in DataHarvest, a project designed to strengthen the capacity and capability of its research partners to apply FAIR principles. The project delivers tailored training materials in face-to-face and online formats to equip researchers with practical skills and the confidence to apply FAIR in their work. The workshops are structured around the GRDC Research Development & Extension (RD&E) Data Management Guidelines and informed by the Diffusion of Innovation theory, acknowledging that individuals adopt new practices at different rates. I present our approach to workshop delivery, which will focus on practical skills such as developing data management plans, creating rich metadata and using repositories. Post-workshop, participants receive ongoing, on-demand support to help apply their learning in practice. I will also present post-training evaluations, which demonstrated significant increases in participants’ confidence across key RDM areas. While challenges such as limited time remain a constraint, our approach has proven effective in making FAIR implementation more achievable."
    },
    {
        "day": "Wednesday",
        "time": "15:50 – 14:10",
        "location": "Boulevard B2",
        "session_chair": "Greg D’Arcy",
        "title": "Open Science in Data-intensive Research Requires Multiple Entry Points: A Case Study from AuScope in Solid Earth Science Infrastructures.",
        "authors": "Lesley Wyborn",
        "url": "https://conference.eresearch.edu.au/open-science-in-data-intensive-research-requires-multiple-entry-points-a-case-study-from-auscope-in-solid-earth-science-infrastructures/",
        "details": "Dr Lesley Wyborn1,2, Dr Rebecca Farrington2, Alex Hunt3, Dr Jens Klump3, Anusuryia Deveraju3, Dr Tim Rawling2, Dr Bryant Ware4, Dr Angus Nixon5\n1NCI, Australian National University, Canberra, Australia,2AuScope Ltd, Melbourne, Australia,3Mineral Resources, CSIRO, Kensington, Australia,4Curtin University, Bentley, Australia,5The University of Adelaide, Adelaide, Western Australia\nLesley Wyborn is an Honorary Professor at ANU at the National Computational Infrastructure (NCI) and at the Research School of Earth Sciences. She also works part time for ARDC. She had 42 years’ experience in Geoscience Australia (GA) in both research and in data science/data management. Since leaving GA in 2014 has continued research in aspects of Data Science including data quality, versioning, reproducibility, operationalising the FAIR, CARE and TRUST principles and Open Science and is a player in many global informatics initiatives. Her current focus is the development of transparent high-performance aggregated national-scale datasets that are compatible with international data networks. ORCID: 0000-0001-5976-4943\nSolid Earth Science datasets provide evidence-based insights into surface and subsurface environments, including the quantification of longitudinal changes over decades. However, their increasing diversity and scale present significant challenges. Primary Observational Datasets (PODs) range widely in size, from small-scale collections in the megabyte range, suitable for on-premise or cloud storage, to high-volume collections that are petabytes in volume and require co-located High Performance Compute-Data (HPC-D) platforms for timely, effective analysis.\nMany funders now request compliance with the FAIR, CARE and TRUST principles, whilst increasing demands for Open Science set a high bar for reproducibility, transparency and sharing requiring open publication of all data collected, tools and processes (UNESCO, 2021;https://doi.org/10.5281/zenodo.5741832).\nIt is no longer possible for a single repository to meet these requirements and serve all users, who range from expert power-users to novices. Instead, a ‘Repository Ecosystem’ is needed, one that balances resources along the full-path of research data use, including:\n1. Curation and sustainable preservation of raw full-resolution PODs captured directly off instruments;\n2. Calibration and conversion of raw PODs into full-resolution reference datasets using community-agreed machine-readable data formats, standards and vocabularies and annotation with rich machine-actionable metadata;\n3. Systematic reprocessing of PODs into reusable downstream analysis-ready products that meet specific researcher needs.\nThis paper outlines Auscope’s approach to developing a Solid Earth Science Data Ecosystem that enables seamless access to PODs, hosted on HPC-D platforms and cloud environments, and clear pathways that connect these datasets to processed, analysis-ready data products delivered through distributed data platforms and portals."
    },
    {
        "day": "Wednesday",
        "time": "15:50 – 14:10",
        "location": "Boulevard B3",
        "session_chair": "Siddeswara Guru",
        "title": "Enabling Climate Model Evaluation in Australia: Infrastructure, Standards, and CMIP7",
        "authors": "Romain Beucher",
        "url": "https://conference.eresearch.edu.au/enabling-climate-model-evaluation-in-australia-infrastructure-standards-and-cmip7/",
        "details": "Dr Romain Beucher1, Felicity Chun1, Charles Turner1, Marc White1, Clare Richards1, Owen Kaluza1, Kelsey Druken1, Yousong Zeng1\n1ACCESS-NRI, Canberra, Australia\nRomain is the Leader of the ACCESS-NRI Model Evaluation and Diagnostics. He is highly experienced in data exploration and analysis where he provides technical support, develops open-source software solutions and provides training to facilitate effective research and development of workflows.\nRomain’s favorite aspect of his work is working with smart people on highly interesting and relevant topics as well as learning from them.\nRomain has 12 years research experience working on world-leading computational geoscience research groups in France, Norway and Australia. He is a former member of the technological development stream of the Australian Research Council Basin Genesis Hub (BGH).\nRomain has a strong educational background in natural sciences as well as in physics and chemistry. He obtained a PhD from the University of Grenoble, France titled “Neogene Evolution of the south-Western Alps (French-Italian border): An integrated sismotectonic and thermochronological approach”.\nOutside work, Romain enjoys spending time his family and friends. He likes cooking, running and fiddling with computers and code, as well as slowly improving his guitar playing.\nThe Australian climate science community is preparing for the Coupled Model Intercomparison Project Phase 7 (CMIP7), a global initiative to advance climate modelling. CMIP enables systematic evaluation of climate models by comparing outputs with observations and with other models, helping identify strengths, weaknesses, and areas for improvement.\nACCESS-NRI is addressing the technical and scientific challenges of CMIP7 preparation by developing a flexible, reproducible evaluation framework. This includes integrating international tools such as ESMValTool and ILAMB, and creating notebook-based diagnostic recipes that target key climate processes like ENSO and the IOD. Significant work is also underway to ensure model outputs meet CMIP standards for structure, metadata, and variable naming, simplifying integration into evaluation workflows.\nA major priority is improving access to observational datasets and model outputs on high-performance computing systems. ACCESS-NRI is building infrastructure to streamline the discovery and loading of these data into evaluation tools, ensuring transparency, reproducibility, and adaptability across different scientific goals.\nThis work aligns with the CMIP7 Assessment Fast Track Rapid Evaluation Framework (CMIP7 AFT REF), a WCRP-supported initiative designed to deliver rapid and robust performance assessments for models contributing to the seventh IPCC Assessment Report (AR7). REF will play a critical role in supporting timely and coordinated international climate science.\nThis presentation will outline the key infrastructure and data challenges involved and highlight ACCESS-NRI’s coordinated approach to sustainable CMIP7 model evaluation. It will also showcase the ongoing development of interactive 3D visualisation tools for enhanced interpretation of model results."
    },
    {
        "day": "Wednesday",
        "time": "15:50 – 14:10",
        "location": "Boulevard Room",
        "session_chair": "Slava Kitaeff",
        "title": "NeSI Platform Refresh Reflections",
        "authors": "Blair Bethwaite",
        "url": "https://conference.eresearch.edu.au/nesi-platform-refresh-reflections/",
        "details": "Mr Blair Bethwaite1\n1New Zealand Escience Infrastructure, University of Auckland, New Zealand\nBlair Bethwaite is NeSI’s Solutions Manager. When not on a bike, squash court, or in the ocean, he specialises in innovation of the underlying systems and platforms to enable NeSI’s teams to deliver greater value to researchers. He has almost 20 years of experience in distributed computing; both in research and for research; for institutional and national projects; from science application collaborations, through grid & cloud middleware development, to full HPC & cloud system infrastructure design, implementation, and operations.\nNew Zealand eScience Infrastructure (NeSI) is in the final stages of a major platform refresh, designed to better support the growing and changing needs of Aotearoa’s research community. This effort has involved not only upgrading hardware and infrastructure but also rethinking how researchers access and interact with high-performance computing (HPC) and data services.\nWe've evolved the way we work by embracing tools and approaches that are more flexible, scalable, and that can adapt to the changing shape of our sector. We want researchers across disciplines — from climate science to genomics, engineering to health sciences — to work more efficiently, collaborate more easily, and explore new approaches like machine learning and data-intensive analysis.\nThis talk will share insights from our platform refresh journey, lessons learned during the transition, and how we’ve kept the needs of researchers at the centre of our design and implementation process.\nI'll also take you on a whirlwind tour of the technologies, and share some of the data centre jenga and pie-in-the-sky dreams we had for this refresh and migration of NeSI's platforms. Did we end up bringing HPC to the Cloud or turning HPC into Cloud? Maybe it was a bit of both – come along to find out."
    },
    {
        "day": "Wednesday",
        "time": "16:10 – 16:30",
        "location": "Boulevard Auditorium",
        "session_chair": "Deanna Deveson Lucas",
        "title": "AI-assisted Humanities Researcher Workbenches",
        "authors": "Ian McCrabb",
        "url": "https://conference.eresearch.edu.au/ai-assisted-humanities-researcher-workbenches/",
        "details": "Dr Ian McCrabb1\n1University of Sydney, Sydney, Australia\nIan McCrabb is the founder and managing director of Systemik (systemiksolutions.com), a Sydney based IT consulting group focused on open-source digital humanities platforms and research sites. Since its establishment in 1994, he has led the design, development and commercialization of consulting methodologies, web technologies and content transformation services: adapting the organizations operational models to map to rapidly evolving web content management platforms and strategies across manufacturing, finance and the public sector.\nSystemik is an innovative Consulting Group and Digital Humanities Lab. We design, develop, implement and support research solutions for humanities projects: open-source platforms, infrastructure integration, APIs, and website content management. Our focus is the realisation of effective, engaging and sustainable digital research.\nSystemik currently supports a portfolio of humanities research platforms, web content management solutions and associated consulting and support services. Our solutions are clustered around content transformation and relationship graphing with integrated models for text analysis, mapping and image annotation. Systemik forte is the realisation of elite scholarship as immersive and engaging research experiences.\nIan is the founder and director of Prakaś Foundation (prakas.org), a non-profit association established in 2005 to support digital scholarship in Buddhist studies and Sanskritic languages. Prakaś provides funding, strategic planning and program management for platform developments.\nIan completed his MA in Sanskrit and Buddhist Studies at the University of Sydney in 2010. His 2021 PhD dissertation 'Buddha Bodies and the Benefits of Relic Establishment: Insights from a Digital Framework for the Analysis of Formulaic Sequences in Gāndhārī Relic Inscriptions' continued his focus on methodologies for the analysis of donative inscriptions and characterization of the ritual practices and religious significance of relic establishment in Gandhāra.\nThe significance of generative AI for humanities research methods is poorly understood, and greatly underestimated—generally limited to anecdotes about where it makes mistakes and how students are using it to cheat.\nThe affordances are quite astonishing. Where AI can generate near perfect Sanskrit, translate and analyse grammar expertly then research practice has changed radically. Where AI acts as a master’s level assistant able to instantly research, collate, analyse, and present, then then the practice of a digital humanist has changed radically.\nThe methodological challenge is what to ask, how to ask it, and how to validate responses. As humanities researchers, we are ground truth. We need to engage with generative AI is as active collaborators rather than passive consumers—building workflows that accept or reject outputs, embedding emendations back into model training to align with scholarly standards.\nThis presentation will explore how generative AI is being integrated across a range of humanities research workbenches we support:\nGlycerine, a IIIF annotation workbench: has integrated an open-source Image AI and IIIF annotation pipeline, supporting iterative, scalable workflows for training models in image segmentation, captioning, and semantic tagging.\nTLCMap, a workbench for mapping history and culture: has implemented an open-source mapping pipeline to extract and geolocate Australian place names from large texts. Researchers can review, amend, and validate results.\nOmeka S, a graph-based content management system: we are designing modules that embed writing, translation, annotation, and visualisation tasks—powered by generative AI—directly into researcher workflows."
    },
    {
        "day": "Wednesday",
        "time": "16:10 – 16:30",
        "location": "Boulevard B1",
        "session_chair": "Jana Makar",
        "title": "From PhD to RSE: Training the Next Generation of Research Software Engineers",
        "authors": "Kelsey Druken",
        "url": "https://conference.eresearch.edu.au/from-phd-to-rse-training-the-next-generation-of-research-software-engineers/",
        "details": "Dr Paige Martin1, Dr. Kelsey Druken1, Dr. Heidi Nettelbeck1\n1Australia's Climate Simulator (ACCESS-NRI), Australia\nPaige leads the User Training Team at Australia’s Climate Simulator (ACCESS-NRI) and is working to build a comprehensive and impactful training framework for ACCESS-NRI tools to ensure that they are useful to the Australian climate modelling community.\nPaige enjoys engaging with researchers and supporting community-driven action around open-source, scientific tooling. She has extensive experience in leading international capacity sharing activities and has led the Python computing portion of a West African oceanography summer school (https://coessing.org) for many years. Before joining ACCESS-NRI, Paige worked at NASA Headquarters as a Support Scientist in the Office of the Chief Science Data Officer.\nPaige is passionate about the idea of open science – that every step of our science should be made accessible and transparent to all. She is also involved in international, open science-related communities: she is a steering committee member of Pangeo (https://pangeo.io/) – a community for big data geoscience – and OSSci (https://www.opensource.science/) – a community at the intersection of science and open-source software.\nPaige was awarded a PhD in Physical Oceanography from the University of Michigan, and a postdoc in Climate Data Science at Columbia University.\nWhen not behind a computer, Paige enjoys standing on her hands, doing partner acrobatics and aerial arts, performing in musical theater, taking nature walks to look for birds, and enjoying time with her family.\nORCID:https://orcid.org/0000-0003-3538-633X\nIn 2024, Australia’s Climate Simulator (ACCESS-NRI) launched the trial phase of a PhD internship program. The program enables PhD students to join the ACCESS-NRI team for a few months and get first-hand experience of what it means to be a Research Software Engineer (RSE) in climate modelling. Interns are offered the chance to learn a range of skills, from developing scientific software collaboratively to understanding the technical foundations of climate models. Within this trial phase, which was open to current PhD students at the Australian National University, we have hosted four interns for 3-4 months each. In this presentation, we will share our experience designing and implementing the program, highlight key learnings and outcomes from the trial phase, and discuss our vision for the full program moving forward. Based on feedback from the trial phase, we plan to extend eligibility to include PhD students from other Australian universities when the full program launches in 2026."
    },
    {
        "day": "Wednesday",
        "time": "16:10 – 16:30",
        "location": "Boulevard B2",
        "session_chair": "Greg D’Arcy",
        "title": "An interoperable and secure model to support data mobility across the national research ecosystem.",
        "authors": "Greg D’Arcy",
        "url": "https://conference.eresearch.edu.au/an-interoperable-and-secure-model-to-support-data-mobility-across-the-national-research-ecosystem/",
        "details": "Mr Greg D'Arcy1\n1Aarnet, Sydney, Australia\nGreg D'Arcy is the Head Digital Research, Life Sciences at AARNet. With over twenty years working in the research and education sectors, Greg's background lies in managing complex digital transformation and infrastructure initiatives. Greg's experience ranges across policy development, managing sensitive data, digital repositories and content distribution networks (CDN), data analytics, service design, stakeholder engagement, and strategic planning.\nAs unprecedented volumes of research data are being created at increasing speeds, it is imperative data can be easily moved and shared to be findable and accessible. With the pace of research accelerating into the age of Quantum and AI, data mobility has become the life blood of modern scientific research. This talk will present some of the latest case studies where Globus has been used to facilitate data transfers and automation across diverse research domains.\nAchieving seamless data mobility in scientific research faces significant challenges that span technical, ethical, and institutional domains. If moving data from A to B is difficult, it undermines all existing investments in the latest instruments, supercomputers, and virtual research environments. Looking to the future, Australia’s Academic and Research Network (AARNet) is advancing Australia’s national digital infrastructure to manage network speeds of up to 400 Gbps to meet the evolving needs of modern, data-driven research fields.\nAARNet is the not-for-profit company that operates Australia’s National Research and Education Network (NREN). Our shareholders are 38 Australian universities and the Commonwealth Scientific and Industrial Research Organisation (CSIRO). For more than 30 years, AARNet has provided reliable telecommunications services, along with an expanding range of cyber security, data, and collaboration services.\nGlobus is a big data transfer platform that is a global leader in data movement and distributed user networks. Globus allows users to transfer terabytes (or more) scheduled using a simple a web-interface, a feat that would be impossible using normal file transfer tools."
    },
    {
        "day": "Wednesday",
        "time": "16:10 – 16:30",
        "location": "Boulevard B3",
        "session_chair": "Siddeswara Guru",
        "title": "Biosecurity Commons – The ground-breaking platform for modelling and analysing biosecurity risk and response",
        "authors": "Rupert Marquand",
        "url": "https://conference.eresearch.edu.au/biosecurity-commons-the-ground-breaking-platform-for-modelling-and-analysing-biosecurity-risk-and-response/",
        "details": "Mr Rupert Marquand1\n1University Of Melbourne – Biosecurity Commons, Melbourne, Australia\nRupert is an experienced science communications and media relations professional with a background in journalism. He is responsible for overseeing and implementing communications and stakeholder engagement activities for Biosecurity Commons. In particular, he develops resources for users which explain the platform in easy-to-understand terms. He has worked for universities across England and Australia for the past decade.\nChanging climate and globalisation has dramatically increased the exposure of countries to new pests and diseases that can have devastating economic, environmental, and social impacts. Consequently, governments, industry and environmental groups must become smarter and more efficient in how they estimate and manage these risks. To meet this challenge, significant investment has focused on the development of standardised biosecurity databases and new surveillance technologies. However, a critical gap remains – the need for a standardised, easy-to-use risk analytics platform that harnesses existing data, estimates risk, and informs policy and operational decisions focused on managing biosecurity risk. Currently, cutting-edge risk analytics are developed by academics for academics and these tools are largely inaccessible to those who need them most – surveillance practitioners, policymakers, and decision-makers. This lack of access has resulted in national inconsistencies in how risk is estimated and managed, with different jurisdictions and industries utilising different models and datasets for informing operational decisions such as where to allocate finite surveillance resources.\nTo address this critical gap, the Centre of Excellence for Biosecurity Risk Analysis (CEBRA) in collaboration with EcoCommons, Griffith University, Department of Agriculture, Fisheries and Forestry, the Queensland government, and the Australian Research Data Commons (ARDC) has developed a platform called Biosecurity Commons (www.biosecuritycommons.org.au) – an open-access, cloud-based platform for modelling and analysing biosecurity risk and response, developed for the Australian biosecurity sector.\nIn this presentation, I will provide an overview of what the platform offers and how it is being used to improve biosecurity management in Australia."
    },
    {
        "day": "Wednesday",
        "time": "16:10 – 16:30",
        "location": "Boulevard Room",
        "session_chair": "Slava Kitaeff",
        "title": "Institutional Research Computing Capabilities in Australia",
        "authors": "Slava Kitaeff, Luc Betbeder-Matibet",
        "url": "https://conference.eresearch.edu.au/institutional-research-computing-capabilities-in-australia/",
        "details": "Dr Slava Kitaeff1,2, Luc Betbeder-Matibet2\n1Monash University, Melbourne, Australia,2The University of New South Whales, Sydney, Australia\nDr. Slava Kitaeff is a nationally recognised leader in high-performance computing, data-intensive science, and large-scale digital research infrastructure. As Associate Director eResearch (HPC) at Monash University, he led major national initiatives including the National ML/AI Research Platform and the Australian Characterisation Commons at Scale Platform. As the Australian SKA Regional Centre Program Lead at UWA & CSIRO, he directed the Design Study Program and chaired the International SRC Architecture Working Group. Dr. Kitaeff was also Research Associate Professor for HPC and Data-intensive science at the University of Western Australia, where he developed novel methods for high-performance computing applications and data-intensive processing pipelines. He is an adjunct professor of HPC at the School of Engineering of Monash University. He has held senior leadership roles for over a decade across multiple institutions, focusing on research computing, digital innovation, and software engineering education.\nLuc Betbeder-Matibet is a nationally recognised subject matter expert in High-Performance Research Computing, Research Data practices and shared Research Infrastructure services. He is Vice-President of AeRo (Australia's eResearch Organisation) and co-chair of the eResearch Australasian peak Annual Conference. He is an Adjunct Senior Lecturer in UNSW Faculty of Medicine Centre for Big Data and has been a Visiting Scientist with the Data61 Visual Analytics Team in Australia's National Science Agency CSIRO. He has held director-level roles for 15 years in higher-ed ICT and eResearch. Luc is currently the Executive Director Research Technology Services at UNSW.\nInstitutional research computing infrastructure is vital in Australia’s research ecosystem, complementing and extending national-level facilities. We present an analysis of research computing capabilities across Australian universities and research organisations, examining how institutional infrastructure supports research excellence through localised compute resources, specialised hardware, and cluster solutions. Our 2024 study reveals that institutional computing resources of nearly 112,258 CPU-cores and 2,241 GPUs serve as essential bridges between desktop computing and national facilities for over 6,000 researchers, enabling research workflows that span from code development to large-scale computations. We estimate the total replacement value of this infrastructure to be approximately $144M AUD. Based on detailed infrastructure data provided by research computing facilities across multiple institutions, we identify key patterns in infrastructure deployment, utilisation metrics, and strategic alignment with research priorities. Our findings demonstrate that institutional computing resources provide critical support for data-intensive research, facilitate training for higher-degree research students, enable prototyping and development, and ensure data sovereignty compliance when necessary. We discuss how these facilities leverage national infrastructure investments while addressing institution-specific needs that national facilities cannot meet alone. We discuss how the strategic investment in institutional research computing capabilities yields significant returns through increased research productivity, enhanced graduate training, and improved research outcomes. We also extend our discussion to the critical importance of developing a national research computing infrastructure framework that leverages the strength of national and institutional investments."
    },
    {
        "day": "Wednesday",
        "time": "16:30 – 16:50",
        "location": "Boulevard Auditorium",
        "session_chair": "Deanna Deveson Lucas",
        "title": "Layers of Life: AI-Ready Species Images, Sounds, and Spatial Occurrence Data in the Atlas of Living Australia",
        "authors": "Kylie Morrow",
        "url": "https://conference.eresearch.edu.au/layers-of-life-ai-ready-species-images-sounds-and-spatial-occurrence-data-in-the-atlas-of-living-australia/",
        "details": "Ms Kylie Morrow1, Dr Nick dos Remedios1, Mr Simon Bear1\n1Atlas Of Living Australia (ALA) / CSIRO, Australia\nKylie is an experienced tech professional who currently leads the coordination of digital content across the Atlas of Living Australia (ALA), CSIRO. Driven by a deep passion for biodiversity and technology, Kylie plays a central role in projects spanning UX/UI, software testing and user support to drive innovation across ALA’s digital infrastructure. Her work is informed by a diverse professional and educational background encompassing design, marketing, ecology, and genomics. Kylie believes in harnessing data and technology to inform transformative research and decision-making. She is actively committed to championing diversity and inclusion in STEM and across the product lifecycle.\nThe Atlas of Living Australia (ALA) is a nationally significant open infrastructure that aggregates, enriches, and shares biodiversity data from across the country. With over 150 million species occurrence records, taxonomy covering over 153,000 species, and a suite of 150+ web services, the ALA supports researchers, policymakers, educators, government agencies, and the public—delivering open data and tools that underpin evidence-based decision-making and conservation.\nAt the core of this capability is BioCache the ALA’s spatially aware occurrence indexing and query system. BioCache not only provides fast, flexible access to species distribution data but also integrates rich multimedia content such as high-resolution species images and audio recordings. These records are often georeferenced, allowing users to explore biodiversity through both spatial and sensory dimensions.\nWith more than 23 million images accessible via images.ala.org.au, the ALA supports large tile image handling for high-resolution datasets, enabling researchers to inspect fine morphological details critical for taxonomy, species identification, and ecological analysis. The integration of audio files further enhances occurrence records, particularly for species where vocalisations are key to detection and classification.\nThis abstract will showcase how BioCache connects occurrence data, spatial coordinates, and media to offer a multi-dimensional research resource. It also highlights how data providers can contribute images and audio through ALA’s data ingestion pipelines, enriching the national biodiversity knowledge base.\nBy combining spatial precision with visual and auditory richness, the ALA’s infrastructure transforms species occurrence records into deeply contextualised, AI and research-ready assets opening new frontiers for biodiversity science, education, and environmental stewardship."
    },
    {
        "day": "Wednesday",
        "time": "16:30 – 16:50",
        "location": "Boulevard B1",
        "session_chair": "Jana Makar",
        "title": "Principles for the Operationalisation of Responsible AI in the Research and Public Sector",
        "authors": "James Smithies",
        "url": "https://conference.eresearch.edu.au/principles-for-the-operationalisation-of-responsible-ai-in-the-research-and-public-sector/",
        "details": "Prof. James Smithies1, Mr Glen Berman1, Dr Karaitiana Taiuru2\n1Australian National University, Canberra, Australia,2Taiuru & Associates Ltd, Wellington, Aotearoa New Zealand\nJames Smithies is Professor of Digital Humanities & Director of the HASS Digital Research Hub at The Australian National University. Before ANU he was Professor of Digital Humanities in the Department of Digital Humanities at King’s College London, founding director of King's Digital Lab and deputy director of King's eResearch. Prior to working at King's James worked as a senior lecturer at the University of Canterbury in Aotearoa/New Zealand, helping develop the UC CEISMIC Canterbury Earthquakes Digital Archive. He has also worked in the government and commercial IT sectors in the United Kingdom and New Zealand, as a technical writer, editor, business analyst, and project manager.\nThis paper presents a set of principles designed to guide the implementation of Large Language Models (LLMs) and Artificial Intelligence (AI) technologies in research and public sector organisations. The principles are an output of the AI as Infrastructure (aiinfra.anu.edu.au) project, informed by a combination of technical development and focus group activities. Notably, the principles combine Research Software Test Engineering (RSTE) with Responsible AI, balancing technical assessment with organizational, legal, and cultural considerations. By establishing baselines for the longitudinal assessment of Large Language Models (LLMs) and fostering environments conducive to learning, through technical experimentation and dialogue, our approach supports the development of organizational knowledge and capabilities over decades rather than the most recent hype cycle. As a case study, we examine Aotearoa New Zealand’s Public Service AI Framework, which exemplifies the need for guiding principles that are grounded in real-world practice, capacity building, and eventual operationalization. The framework’s inclusion of Māori data sovereignty principles, shaped by the foundational Treaty of Waitangi, highlights Aotearoa’s unique approach to AI. This approach holds significant relevance for research and cultural institutions both within and beyond Aotearoa, particularly those with commitments to Indigenous and First Nations communities. By focusing on practical engagement while respecting cultural and ethical considerations, this paper outlines a pathway for integrating LLMs and AI responsibly within the research and GLAM sectors and highlights the pressing need to increase the maturity of our RSTE methods."
    },
    {
        "day": "Wednesday",
        "time": "16:30 – 16:50",
        "location": "Boulevard B2",
        "session_chair": "Greg D’Arcy",
        "title": "Enabling Research Connectivity: The Strategic Role of Data Mobility in an Integrated National Research Infrastructure Ecosystem.",
        "authors": "Enabling Research Connectivity: The Strategic Role of Data Mobility in an Integrated National Research Infrastructure Ecosystem.",
        "url": "https://conference.eresearch.edu.au/enabling-research-connectivity-the-strategic-role-of-data-mobility-in-an-integrated-national-research-infrastructure-ecosystem/",
        "details": "Mr Greg D’Arcy1\n1Aarnet, Sydney, Australia\nGreg D’Arcy is the Head Digital Research, Life Sciences at AARNet. With over twenty years working in the research and education sectors, Greg’s background lies in managing complex digital transformation and infrastructure initiatives. Greg’s experience ranges across policy development, managing sensitive data, digital repositories and content distribution networks (CDN), data analytics, service design, stakeholder engagement, and strategic planning.\nPanelist:\nDr Marlies Hankel has a master in mathematics and a PhD in computational chemistry. She moved to The University of Queensland in 2004 and has held positions in the Australian Institute for Bioengineering and Nanotechnology and School of Mathematics as a senior research fellow and lecturer. Her research was in energy storage materials, and she taught numerical methods. She built her first high performance computing cluster in 2004 and has built 4 clusters over the years. She enjoys teaching and helping others and finally moved full time to the Research Computing Centre (RCC) in 2021. Her role in RCC is skills training and support of users and the development of operational procedures.\nGin Tan is the Associate Director of Monash eResearch. She began her career in both enterprise and research computing, eventually finding her true passion in high-performance computing (HPC).Over the past eight years at Monash University, Gin has been instrumental in the development and deployment of a new HPC cluster, and has since grown into a technical leadership role. She enjoys diving deep into the technical aspects of her work while also excelling at translating complex needs into practical, effective solutions.\nKyle Chard is a Research Associate Professor in the Department of Computer Science at the University of Chicago. He also holds a joint appointment at Argonne National Laboratory. He co-leads the Globus Labs research group, which focuses on a broad range of research problems in data-intensive computing and research data management.\nBlair Bethwaite is NeSI’s Solutions Manager. When not on a bike, squash court, or in the ocean, he specialises in innovation of the underlying systems and platforms to enable NeSI’s teams to deliver greater value to researchers. He has almost 20 years of experience in distributed computing; both in research and for research; for institutional and national projects; from science application collaborations, through grid & cloud middleware development, to full HPC & cloud system infrastructure design, implementation, and operations.\nAs research becomes increasingly data-intensive and collaborative, the ability to move and share data reliably, securely and at scale is fundamental to realising an integrated national research infrastructure (NRI) ecosystem. This panel session will explore how robust data mobility underpins collaboration and innovation across universities, scientific facilities and industry.\nA key focus will be on the role of advanced tools and platforms — particularly Globus.org — in enabling secure, high-performance data pipelines. Widely adopted across the research community in the US, Globus provides a trusted solution for automating and managing data transfers across institutional and geographic boundaries, simplifying the movement of large datasets between instruments, storage systems and computing environments.\nThe BoF will highlight opportunities for deeper integration across infrastructure layers and sectors, as panellists share views on the policy and governance requirements necessary to support federated data access and supporting cross-sector interoperability. The panel will discuss how secure and efficient data mobility capabilities strengthen research agility, protects data assets and maximises the impact of other national research investments. The session will showcase how tools like Globus are critical in facilitating scalable scientific workflows involving distributed national facilities and cloud or HPC resources.\nThis session is ideal for infrastructure providers, researchers, and policymakers seeking to understand and shape the future of an integrated NRI ecosystem. Participants are encouraged to bring their questions, ideas and experiences to help build this emerging community of practice across the research sector."
    },
    {
        "day": "Wednesday",
        "time": "16:30 – 16:50",
        "location": "Boulevard B3",
        "session_chair": "Siddeswara Guru",
        "title": "Biodiversity and ecosystem data management: Semantic web approach",
        "authors": "Biodiversity and ecosystem data management: Semantic web approach",
        "url": "https://conference.eresearch.edu.au/biodiversity-and-ecosystem-data-management-semantic-web-approach/",
        "details": "Dr Siddeswara Guru1, Dr Nicholas Car2, Miss Junrong Yu1, Dr Mieke Strong3\n1University Of Queensland, Indooroopilly, Australia,2KurrawongAI, Brisbane, Australia,3Gaia Resources, Brisbane, Australia\nSiddeswara Guru is the program lead of TERN Data Services and Analytics\nNic Car is the founder, Data Architect in KurrawongAI\nJunrong Yu is the software Engineer in the TERN Data Services and Analytics\nMieke Strong is aBusiness Analyst and Data Management expert in the Gaia Resources.\nThe vision of the Semantic Web is to provide artefacts on the web and make them machine-readable. This will enable machines to negotiate content on behalf of humans.\nThe Semantic Web approach for scientific data management will significantly improve the discoverability and accessibility of data. However, it presents several challenges, including determining the standard representation of artefacts, the need for standardised ontologies, controlled vocabularies, and tools to create and access its complex data formats.\nOver the past several years, significant efforts have been made in the Earth and environment science domains to develop domain ontologies, domain-specific controlled vocabularies, and tools to discover and access these artefacts. Apart from the Research Infrastructure project, the Australian federal and state government agencies are building and adopting solutions to make their data semantic-enabled.\nThe BoF will be a mixture of presentations and group discussions to bring together the community of individuals working on semantic web solutions. The presentations will outline a semantic web approach to managing biodiversity and ecosystem data within the Australian context, including the development of domain ontologies and controlled vocabularies, as well as the creation of data exchange profiles, data ingestion processes and repository development. In addition, the organisers will showcase some of the recent tools that have been developed."
    },
    {
        "day": "Wednesday",
        "time": "16:30 – 16:50",
        "location": "Boulevard Room",
        "session_chair": "Slava Kitaeff",
        "title": "Sustainable Research Computing in T1&T2",
        "authors": "Sustainable Research Computing in T1&T2",
        "url": "https://conference.eresearch.edu.au/sustainable-research-computing-in-t1t2/",
        "details": "Dr Slava Kitaeff1,2, Luc Betbeder-Matibet2, Jake Carrol3\n1Monash University, Australia,2The University of New South Whales, Sydney, Australia,3The University of Queensland, Brisbane, Australia\nDr. Slava Kitaeff is a nationally recognised leader in high-performance computing, data-intensive science, and large-scale digital research infrastructure. As Associate Director eResearch (HPC) at Monash University, he led major national initiatives including the National ML/AI Research Platform and the Australian Characterisation Commons at Scale Platform. As the Australian SKA Regional Centre Program Lead at UWA & CSIRO, he directed the Design Study Program and chaired the International SRC Architecture Working Group. Dr. Kitaeff was also Research Associate Professor for HPC and Data-intensive science at the University of Western Australia, where he developed novel methods for high-performance computing applications and data-intensive processing pipelines. He is an adjunct professor of HPC at the School of Engineering of Monash University. He has held senior leadership roles for over a decade across multiple institutions, focusing on research computing, digital innovation, and software engineering education.\nLuc Betbeder-Matibet is a nationally recognised subject matter expert in High-Performance Research Computing, Research Data practices and shared Research Infrastructure services. He is Vice-President of AeRo (Australia's eResearch Organisation) and co-chair of the eResearch Australasian peak Annual Conference. He is an Adjunct Senior Lecturer in UNSW Faculty of Medicine Centre for Big Data and has been a Visiting Scientist with the Data61 Visual Analytics Team in Australia's National Science Agency CSIRO. He has held director-level roles for 15 years in higher-ed ICT and eResearch. Luc is currently the Executive Director Research Technology Services at UNSW.\nJake Carroll is the Director of The University of Queensland Research Computing Centre and is a Doctoral Candidate in the School of Electrical Engineering and Computer Science in the same institution. He has been involved in large scale architecture, systems and design of digital scientific research infrastructure for twenty years. He serves on numerous boards of advisory across various sectors encompassing corporate, health and defence related advanced computing domains. Carroll has a Bachelor of Information Technology (Honors) from Southern Cross University and an MBA from The University of Queensland.\nThe sustainability of research computing is emerging as a critical issue across institutions and national facilities alike. As AI workloads drive unprecedented demand for GPU infrastructure, and as energy prices, carbon goals, and physical constraints place new pressures on data centres, the research computing community must rethink how resources are provisioned, operated, and used.\nThis BoF invites participants from across the research computing landscape—operations teams, research leaders, infrastructure strategists, and power users—to discuss practical challenges and share emerging solutions. Key topics will include data centre sustainability strategies (e.g., commercial vs on-premise), power and cooling limitations, the energy cost of AI and large-scale workflows, and how user behaviour, software choices, and scheduling policies impact overall efficiency.\nWe will also consider what “sustainability” means beyond electricity: how funding models, training programs, usage culture, and policy levers can influence longer-term viability and equitable access. Can we track power usage like CPU hours? Should GPU jobs have sustainability guidelines? What new norms or collaborations are needed?\nWhether your concern is carbon, cost, or capacity, this BoF aims to provide space for informed discussion, shared learning, and actionable next steps for making research computing more sustainable—without compromising performance or innovation."
    },
    {
        "day": "Wednesday",
        "time": "16:50 – 17:10",
        "location": "Boulevard Auditorium",
        "session_chair": "Deanna Deveson Lucas",
        "title": "Exploration of Large Language Models as the Basis for Natural Language Query Interfaces to Big Data Systems",
        "authors": "Richard Sinnott, Luca Morandini",
        "url": "https://conference.eresearch.edu.au/exploration-of-large-language-models-as-the-basis-for-natural-language-query-interfaces-to-big-data-systems/",
        "details": "Prof. Richard Sinnott1, Mr Luca Morandini1\n1University Of Melbourne, Melbourne, Australia\nProfessor Richard O. Sinnott is Professor of Applied Computing Systems and Director of the Melbourne eResearch Group at the University of Melbourne. He has been lead software engineer/architect on an extensive portfolio of national and international projects, with specific focus on those research domains requiring finer-grained access control (security) and those dealing with big data challenges. He has over 450 peer reviewed publications across a range of applied computing research areas.\nMr Luca Morandini is a Cloud architect in the Melbourne eResearch Group and an Honorary Fellow in the School of Computing and Information Systems at the University of Melbourne. He heads the development of the AIReD platform.\nOne of the key challenges of big data systems is development of flexible and intuitive user interfaces to explore potential patterns in the data. This is especially challenging for big data which is commonly typified by variety, velocity and volume. Within the context of the Australian Research Data Commons (ARDC –www.ardc.edu.au) funded Australian Internet Observatory (AIO –https://internetobservatory.org.au/) and specifically within one key component of the AIO: the Australian Internet observatory Research Dashboard (AIReD –https://www.aio.eresearch.unimelb.edu.au/) web-based filters offering logic-based searches have been realised for searching through large quantities of diverse social media posts. Such filters are realised by web forms that offer ways for researchers to find data of interest, e.g. within a particular time period, on a particular topic. However such forms are often difficult to create and not aligned with the ways in which serendipitous discovery of patterns in big data can arise.\nLarge language models (LLMs) offer an alternative approach to querying data. The ability to write natural language as the basis for exploring big data offers many potential advantages – especially with regards to expressiveness and usability. This talk will showcase the use of LLMs as the basis for natural language query interfaces to AIReD in allowing researchers to explore the diverse and evolving social media data sets. The talk will include a demonstration of exemplar case studies highlighting the advantages and disadvantages of LLMs within AIO."
    },
    {
        "day": "Wednesday",
        "time": "16:50 – 17:10",
        "location": "Boulevard Room",
        "session_chair": "Slava Kitaeff",
        "title": "Measuring impact – recording support metrics to uncover trends and drive change",
        "authors": "Cameron Fong",
        "url": "https://conference.eresearch.edu.au/measuring-impact-recording-support-metrics-to-uncover-trends-and-drive-change/",
        "details": "Dr Cameron Fong1\n1The University of Sydney, Australia\nCameron engages with researchers, faculty, professional staff, and students to promote, improve, develop, and implement RDM best practices. This varies from developing resources, providing support and training for the research community, working with individual researchers to develop tailored solutions, to collaborating with peers at different institutions.\nManagement has a culture of using metrics to measure value, justify initiatives, budgets, and staffing levels. Not all parts of the research operations division are able to easily produce usable metrics, and in some cases, the work and value provided are very qualitative with few readily reportable metrics compared to other teams that can draw reportable metrics directly from the systems they use.\nResearch Integrity and Ethics Administration (RIEA) is an area that do not have easily reportable metrics, and for some teams, the work is very hard to quantify traditionally. Compounding this, is the need to demonstrate workloads in an environment shared with other research support teams who can report positive metrics such as grants, and contracts.\nThe teams within RIEA have adapted a REDCap logging system to record interaction and workload metrics using simple, team standardised surveys. These surveys record interaction type, activity type, the time spent, and the details of the client. Effort has been placed in ensuring team specific surveys have been developed to best quantify the highly qualitative work that these teams do.\nThese interaction logs are then used to quantify and standardise support metrics for each RIEA team, collate this data and deliver a quantitative analysis of the scale and scope of work for comparative RIEA reporting. This is not a perfect quantification of qualitative work but has allowed meaningful recording of work in a manner that can compete effectively for staff funding, systems resourcing and recognition of importance to research excellence and compliance."
    },
    {
        "day": "Wednesday",
        "time": "17:10 -17:30",
        "location": "Boulevard Auditorium",
        "session_chair": "Deanna Deveson Lucas",
        "title": "Using AI tools in a virtual desktop for transcription and translation.",
        "authors": "Yvette Wharton",
        "url": "https://conference.eresearch.edu.au/using-ai-tools-in-a-virtual-desktop-for-transcription-and-translation/",
        "details": "Ms Yvette Wharton1, Mr James Love1, Mr Noel Zeng1, Dr Nidhi Gowdra1, Mr Sean Matheny1\n1The University of Auckland, New Zealand\nYvette Wharton works at the Centre for eResearch, Waipapa Taumata Rau, University of Auckland. She is the Research Data Management Programme Lead, working on the Secure Research Environment and machine-actionable Data Management Planning initiatives.http://orcid.org/0000-0002-6689-8840\nTranscribing audio into text is part of the research process for many researchers. Manually transcribing text can be time-consuming, so an increasing number of researchers are using software to transcribe and translate, for example, interview or focus group audio recordings. Recognising this need, we aim to enable researchers to use AI responsibly while being mindful of data sovereignty issues and responding to the needs of a multi-lingual society.\nIn collaboration with the core Nectar team, the Centre for eResearch has piloted a simple, easy-to-use, locally hosted, ephemeral digital transcription desktop. The desktop runs on existing CeR-managed local UoA Nectar infrastructure. This desktop enables researchers to safely and efficiently transcribe digital audio content using AI-powered tools. In this talk, we will share how we have delivered this service, including bidding for funding, technical and ethical considerations, desktop environment development, support material creation and outcomes."
    },
    {
        "day": "Wednesday",
        "time": "17:10 -17:30",
        "location": "Boulevard Room",
        "session_chair": "Slava Kitaeff",
        "title": "Business Continuity through Reproducible Research – a practical workshop on building knowledge trust and retention",
        "authors": "Amanda Miotto",
        "url": "https://conference.eresearch.edu.au/business-continuity-through-reproducible-research-a-practical-workshop-on-building-knowledge-trust-and-retention/",
        "details": "Mr Riad Akhundov1, Mrs. Amanda Miotto1\n1Griffith University, Australia\nAmanda Miotto is an eResearch Analyst for Griffith University, with 18 years of experience in the industry. She started off in the field of Bioinformatics and learnt to appreciate the beauty of science before discovering the joys of coding. She aims to bring innovations and resources to researchers to help accelerate their research projects.\nRiad Akhundov is an eResearch Analyst (QCIF) in the eResearch and Specialised Advisory department at Griffith University. His PhD focused on Computational Biomechanics (i.e., computer/data science applied to human motion). As a eResearch Analyst, Riad is involved in various projects and collaborations, including instructing/assisting with workshops, helping researchers and students with their coding/IT needs, and building an eResearch knowledge base.\nSituation\nWe write research papers to contribute knowledge, insights, or perspectives to a field of study. By publishing research, we ask our audience to believe in what we say; to accept that the knowledge we have created is true.\nOne way we ensure our research credibility is reproducibility – can someone repeat a researcher's published work and end up at the same conclusion? Knowledge should be reproducible.\nTraining researchers in reproducibility is crucial to building trust, credibility and quality in an organisation's research outputs. But what do researchers need to know?\nTask\nGriffith University partnered with UKRN to expand training in Reproducible Research, as Train-the-trainer and self-guided materials.\nWe built a course that was discipline agnostic and does not focus on coding solutions. The workshop offers to take researchers 'further down the path', focusing on small, incremental changes, building reproducibility into everyday workflows.\nThis builds on principles from our previous workshop \"9 Reproducible Research Things\" written in 2018.\nAction\nMultiple cohorts have been trained through the UKRN, and go on to host Reproducible Research workshops across the UK. Leveraging the Carpentries template, this workshop can be done either as a self-based tutorial or as a script for an instructor. Also integrated are instructions on how instructors can customise the tutorial for their own organisation.\nResult\nNow hosted in the Carpentries Incubator at carpentries-incubator.github.io/ReproducibleResearch/. This workshop has been taught to +100 trainers across UK and Australia, to host in their own institutes."
    },
    {
        "day": "Wednesday",
        "time": "17:30 – 19:00",
        "location": "Boulevard Auditorium",
        "session_chair": "Deanna Deveson Lucas",
        "title": "Poster Reception | Boulevard FoyerCap off the second day of the conference by attending the Poster Reception, providing an informal space for presenters to engage directly with attendees, encouraging interactive discussions and in-depth exploration of their work, making it a must-attend event for valuable insights and networking.",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Thursday",
        "time": "07:30 – 08:30",
        "location": "N/A",
        "session_chair": "N/A",
        "title": "Women+ in HPC & HPC Systems Administrators Joint Breakfast, sponsored by SharonAI",
        "authors": "This breakfast is free to attend, but space is limited. Register your place when registering for the conference.",
        "url": "https://conference.eresearch.edu.au/women-in-hpc-australasia-breakfast/",
        "details": "As high-performance computing (HPC) evolves to meet the demands of modern science and industry, system administrators across Australasia are facing a convergence of critical challenges. The increasing complexity of heterogeneous environments, driven by specialised accelerators, tight budgets, and evolving software stacks, demands agile, scalable, and reproducible infrastructure solutions.\nCompounding technical challenges is a pronounced skills shortage. Fewer junior sysadmins are entering the field with foundational command-line experience, forcing teams to consider new models of talent development, such as structured training straight from university. These pressures underscore the importance of fostering a strong, collaborative HPC community in the ANZ and broader Australasian region, one that can share knowledge, tools, and strategies for building resilient and efficient infrastructure.\nThrough conversation with WHPC+ Australasia Chapter, the Australian Systems Administrator Group and SharonAI, we will explore ways to strengthen and grow the technical and digital research community across Australia.\nAbout Women+ in HPC Australasia\nThe Australasian Chapter of the global organisationWomen in High Performance Computing (WHPC)aims to better support diversity within Australia’s and New Zealand’s HPC and eResearch sectors.\nWe’ve added a “+” to our Chapter’s name, as we welcome people from all backgrounds and perspectives beyond gender to join our community – we all have a part to play in supporting diversity and inclusion.\nWatch our 3min video onWhy the WHPC Australasian Chapter in importantorview the longer 8min version.\nOur goals\nContact us by email atahoy@whpcanz.org.\nAbout the Australian HPC Sysadmin SIG\nThe Australian HPC Sysadmin SIG is a collaborative community that brings together high-performance computing (HPC) system administrators from across Australia and New Zealand. While unofficial discussions take place on Slack, covering a wide range of topics such as technical challenges, infrastructure design, security, and user support, the group also convenes formally once a year at the eResearch Australasia conference. These annual meetings, whether through roundtable discussions or conference workshop, provide an opportunity for in-person engagement and knowledge exchange. The primary goal of the SIG is to foster connection and collaboration among professionals in this niche field, filling a gap where no other dedicated platform currently exists to support HPC sysadmins in the region."
    },
    {
        "day": "Thursday",
        "time": "08:00 – 17:30",
        "location": "N/A",
        "session_chair": "N/A",
        "title": "Registration Open | Boulevard Auditorium Registration Desk",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Thursday",
        "time": "08:40 -08:50",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "Welcome to Day 3Conference Chairs",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Thursday",
        "time": "08:50 – 09:30",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "High-Performance, Quantum and AI-Driven Chemical Discovery",
        "authors": ", Professor of Quantum Medicinal Chemistry, Monash University",
        "url": "https://conference.eresearch.edu.au/high-performance-quantum-and-ai-driven-chemical-discovery/",
        "details": "Addressing today’s most pressing technological and scientific challenges—from developing new therapeutics and biotechnologies to creating greener chemical processes—requires the ability to accurately model matter at large atomistic scales and with high speed. However, current simulation techniques can be prohibitively slow, imprecise, or computationally too costly, resulting in heavy reliance on physical experiments that are time-consuming, expensive, and subject to inherent limitations.\nIn response, we present a pioneering digital chemistry framework that integrates many-GPU high-performance computing, computational quantum chemistry, and machine learning, delivering unprecedented accuracy and scalability in chemical discovery—particularly for drug design. By leveraging novel algorithms and software optimizations, we overcome the steep computational barriers that have historically limited quantum chemistry at large molecular scales. Recognized with the 2024 Gordon Bell Prize, our approach enables the first quantum-level simulations of complex bioscale molecular systems with accuracy nearing that of physical experiments.\nThrough exascale supercomputing, agentic AI workflows, and machine learning models trained on quantum-accurate data, we streamline end-to-end simulation and design processes. These intelligent workflows reduce reliance on laboratory experiments, providing a fully automatable, cost-effective, adaptive, and accurate solution for investigating complex molecular phenomena.\nThe result is a powerful platform that accelerates chemical discovery, enhances molecular design, and redefines in silico research opportunities across chemistry, biology, and related fields.\nBiography:\nProf. Giuseppe M. J. Barca is a leading expert in high-performance computing (HPC), artificial intelligence (AI), and digital chemistry, recognized for pioneering scalable algorithms that have redefined the limits of molecular modelling and in silico drug discovery. His research integrates HPC, AI, and quantum chemistry to accelerate chemical R&D, enabling predictive simulations with accuracy close to physical experiments.\nFrom 2018 to 2023, Barca led Australia’s only contribution to the U.S. Department of Energy’s Exascale Computing Project, collaborating with Georgia Tech, Argonne, Oak Ridge, and Ames National Laboratories as well as NVIDIA, Intel, AMD, and Cray. Between 2020 and 2024, his team set four world records in quantum chemical modelling, culminating in the 2024 ACM Gordon Bell Prize for enabling the first quantum-accurate simulations of biomolecular systems and the first-ever exaflop-scale computation in double precision. In 2025, he received the WATOC Dirac Medal, the highest international recognition in theoretical and computational chemistry for researchers under 40.\nIn 2023, Barca co-founded QDX Technologies, a deep-tech company with operations in Singapore, Melbourne, and Canberra, where he serves as Head of Research. Alongside this role, he leads academic teams at the Australian National University, focusing on high-performance computing, and at the Monash Institute of Pharmaceutical Sciences, advancing AI- and quantum-driven drug discovery."
    },
    {
        "day": "Thursday",
        "time": "09:30 – 10:30",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "Why Australasian Research Needs SupercomputingPanelists:Hayley Teasdale, Australian Academy of ScienceAndy Hogg, ACCESS-NRITim Rawling, AuScopeGeorgina Rae, REANNZ",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Thursday",
        "time": "10:30 – 11:00",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "Morning Tea, Exhibition, Poster Viewing | Boulevard Foyer",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Thursday",
        "time": "11:00 – 11:20",
        "location": "Boulevard Auditorium",
        "session_chair": "Noel Faux",
        "title": "Enabling health researchers through national data sharing infrastructure: Health Studies Australian National Data Asset",
        "authors": "Kathy Dallest, Katie Ozdowska",
        "url": "https://conference.eresearch.edu.au/enabling-health-researchers-through-national-data-sharing-infrastructure-health-studies-australian-national-data-asset/",
        "details": "Ms Kathy Dallest1, Katie Ozdowska2, Olya Ryjenko3, Dr Rhys Williams4\n1QCIF Ltd, Brisbane, Australia,2University of Melbourne, Melbourne, Australia,3The University of Sydney, Sydney, Australia,4Australian Research Data Commons, Caulfield East, Australia\nhttps://orcid.org/0000-0003-4109-3146\nKathy Dallest is the Senior Health Informatician in QCIF and is co-leader of the HeSANDA Queensland Node delivering national infrastructure enabling secondary use of health and medical research data. Kathy has a background in nursing and public health and transitioned to Health Informatics in Scotland in the mid 1990s. Over the past 9 years she has provided expert domain knowledge and guidance to eResearch infrastructure programs, research groups and health services. Kathy has over 30 years’ experience working with sensitive health and medical data in clinical, research and educational settings with a focus on patient safety, governance, information security, integrity and knowledge generation.\nAustralia’s health research community faces a pressing need to maximise the impact of valuable data generated through clinical trials and health studies. While data sharing and reuse offer significant scientific, economic, and ethical advantages, complex challenges often impede these practices in Australia.\nThe Health Studies Australian National Data Asset (HeSANDA) program is addressing these challenges by building national data sharing infrastructure.  The program established a national network of research organisations, structured under nine HeSANDA Nodes working in collaboration with the Australian Research Data Commons (ARDC).\nAn important component of this national infrastructure is Health Data Australia (HDA), an online repository of health and medical research metadata contributed by the Nodes. The repository allows researchers to discover health data available for secondary use and facilitates access and sharing by connecting researchers to data owners and their institutions.  This includes metadata from clinical trials, cohort studies and other types of health and medical research.\nThe HeSANDA network is addressing multiple components of the health research ecosystem that together facilitate sharing of health data for research. These include processes for patient consent to data sharing, ethical approval, data and metadata standards, policy development and cultural change.\nBy unlocking the potential of reusing existing research data, HeSANDA aims to promote research efficiency, reduce research waste, foster collaboration, and create opportunities to address new research questions.\nThis presentation will use real world examples of how this key infrastructure delivers benefits for researchers, research participants, the public, institutions, funders, educators and other stakeholders."
    },
    {
        "day": "Thursday",
        "time": "11:00 – 11:20",
        "location": "Boulevard B1",
        "session_chair": "Ghulam Murtaza",
        "title": "From Policy to Practice: Building a Research Data Management Ecosystem",
        "authors": "Sow Boubacar",
        "url": "https://conference.eresearch.edu.au/from-policy-to-practice-building-a-research-data-management-ecosystem/",
        "details": "Mr Sow Boubacar1, Mr MOTOKI MASAKAZU2, Mr HAMATE YUICHIRO1\n1Tohoku University, Research Management Center / Open Science Division, Sendai, Japan,2Institute for Data Synergy Creation, Sendai, Japan\nBoubacar Sow is a Specially Appointed Assistant Professor at Tohoku University, Japan, where he plays a leading role in the Open Science Division. His work focuses on advancing research data management systems and promoting open science initiatives across the university.\nOpen science policies and funder mandates increasingly call for robust management of research outputs and data.\nWe present the development of a comprehensive Research Data Management platform at the university level that translates open science strategy into practical tools for researchers. The platform integrates management of ongoing research projects, research publications with their evidence data, and open access initiatives into a unified ecosystem."
    },
    {
        "day": "Thursday",
        "time": "11:00 – 11:20",
        "location": "Boulevard B2",
        "session_chair": "Edoardo Tescari",
        "title": "Failing Forward: Experiments with AI Tools to Build a Policy Finder",
        "authors": "Wei Shen",
        "url": "https://conference.eresearch.edu.au/failing-forward-experiments-with-ai-tools-to-build-a-policy-finder/",
        "details": "Mr Wei Shen1\n1Griffith University, Southport, Australia\nWei Shen is an eResearch Analyst at Griffith University, where Wei helps researchers adopt digital tools and technologies to enable and enhance their research. With over 17 years of experience in the higher education sector and a world-class private research lab, Wei brings a systems-thinking approach to problem-solving, grounded in deep knowledge of research infrastructure and digital solutions.\nWei is particularly interested in helping researchers navigate the hype surrounding artificial intelligence—making it approachable, safe, and genuinely useful, especially in academic environments where context, accuracy, and trust are critical.\nThis presentation shares an honest journey of trial, error, and learning while trying to build an AI-assisted policy finder for researchers. What started as a simple idea—using AI to help users locate relevant, more importantly, complete government policies—quickly became a series of unexpected challenges.\nWei experimented with a wide range of AI tools and techniques, from no-code solutions to standalone large language models (LLMs) and retrieval-augmented generation (RAG). Each method promised to simplify the task, yet all struggled with real-world issues like context loss, inconsistent responses, and difficulty aligning outputs with user expectations. In particular, Wei will reflect on the limitations of popular tools and why even technically “correct” implementations can still fail to meet practical needs.\nRather than showcasing a polished solution, this talk offers practical lessons learned from what didn’t work – valuable for anyone building AI tools for institutional knowledge, policy search, or similar domains. Attendees will gain a realistic view of the current capabilities and pitfalls of AI-powered search, and hopefully, avoid some of the detours I took along the way."
    },
    {
        "day": "Thursday",
        "time": "11:00 – 11:20",
        "location": "Boulevard B3",
        "session_chair": "Andrew Underwood",
        "title": "Research isn’t limited by ideas; it’s limited by infrastructure",
        "authors": "Ben Svalbe",
        "url": "https://conference.eresearch.edu.au/research-isnt-limited-by-ideas-its-limited-by-infrastructure/",
        "details": "Ben Svalbe, Principal Consultant, Macquarie Cloud Services\nBiography:\nBen is a trusted expert in colocation, hybrid cloud, and secure infrastructure design. With over 25 years in technical, pre-sales, and consulting roles, he has partnered with universities and research organisations to deliver scalable, fit-for-purpose solutions. Known for unifying diverse technical and business requirements, Ben helps organisations navigate complexity while achieving security and performance.\nAbstract:\nAs data volumes explode and AI and HPC reshape discovery, universities need environments built to match its pace. This session will explore the critical factors that shape research outcomes including enabling collaboration across institutions, securing sensitive projects, giving researchers access to power when they need it, and ensuring infrastructure scales.\nDrawing on 25 years of experience, we’ll share insights into the evolving role of infrastructure in research, highlight examples of how the right foundation accelerates outcomes, and provide strategies to guide future technology decisions. Participants will leave with a clear understanding of why infrastructure – not ideas – define the limits of research, and how universities can position themselves to succeed in this new era of accelerated discovery."
    },
    {
        "day": "Thursday",
        "time": "11:00 – 11:20",
        "location": "Boulevard Room",
        "session_chair": "Maciej Cytowski",
        "title": "SCIGET: Making Scientific Software Accessible – Everywhere for Everyone!",
        "authors": "Aswin Narayanan, Steffen Bollmann, Mark Endrei, Joshua Scarsbrook",
        "url": "https://conference.eresearch.edu.au/sciget-making-scientific-software-accessible-everywhere-for-everyone/",
        "details": "Mr Aswin Narayanan1,4, Dr Steffen Bollmann2, Mr Mark Endrei3, Mr Joshua Scarsbrook2\n1Australian Institute for Bioengineering and Nanotechnology, The University of Queensland, Brisbane, Australia,2School of Electrical Engineering and Computer Science, The University of Queensland, Brisbane, Australia,3Research Computing Centre, The University of Queensland, Brisbane, Australia,4National Imaging Facility, Brisbane, Australia\nAswin Narayanan (orcid.org/0000-0002-4473-7886):\nAswin Narayanan is the Imaging Informatics Fellow at the University of Queensland Node of the National Imaging Facility (NIF). Aswin is based at UQ's Centre for Advanced Imaging and works on data management and analysis solutions for biomedical imaging, including modalities such as MR, PET, CT, and radiochemistry. With a background in Biomedical Engineering, Aswin has over 15 years of experience supporting scientific research and building software research infrastructure.\nSteffen Bollmann (orcid.org/0000-0002-2909-0906):\nAfter a PhD on multimodal imaging at the University Children’s Hospital and ETH Zurich, Switzerland, Dr Bollmann joined the Centre for Advanced Imaging at the University of Queensland (Australia), where he pioneered the application of deep learning methods for quantitative susceptibility mapping. In 2019 Dr Bollmann joined the Siemens Healthineers collaborations team at the MGH Martinos Center in Boston where he worked on the translation of deep learning reconstruction techniques into clinical applications. Since joining the School of Information Technology and Electrical Engineering at the University of Queensland in 2020 Dr Bollmann develops computational methods to process magnetic resonance imaging data.\nMark Endrei (orcid.org/0000-0001-9368-2773):\nMark Endrei is a senior principal research software engineer at the Research Computing Centre, The University of Queensland, Australia. He also has more than 20 years of experience in IT industry, working with large corporations both nationally and internationally. He has a PhD from The University of Queensland and a Bachelor of Engineering Degree (H1) in Computer Systems Engineering from RMIT University.\nJoshua Scarsbrook (orcid.org/0000-0002-0071-5466)\nJoshua Scarsbrook is Research Officer with 10 years of professional research experience. Their passion lies in developing solutions to tackle the ever-growing challenges in the field of operating systems.\nThroughout their career, they have been involved in numerous projects that have allowed me to hone my skills in the areas of visualization, virtual machines, and operating systems. Their work has been focused on developing innovative approaches to address complex issues across interdisciplinary fields.\nBackground\nThe analysis of scientific data necessitates specialised software and complex processing pipelines. Researchers and research infrastructure maintainers expend significant time and effort on software compilation, installation, and managing deployments across diverse computing platforms (laptops, workstations, HPCs, cloud). Additionally, system-specific dependency issues and challenges in publishing reproducible pipelines alongside data often compromise research outcomes.\nIntroduction\nSCIGET aims to deliver a robust solution tackling these challenges in scientific software accessibility and research reproducibility. Leveraging containerisation and projects like Neurodesk.org, CernVM-FS, eessi.io, and tinyrange, SCIGET provides a robust software distribution system alongside accessible and portable virtual desktops that can be co-located with data for scientific analysis.\nMethods\nAt the core is a community-engaged build pipeline, where scientific applications and reference datasets are proposed, packaged, security scanned, and published to container registries. Applications are accessible via graphical virtual desktops embedded into JupyterLab, and command line as modules. Containers run on infrastructure through a variety of mechanisms: Apptainer/Singularity and CernVM-FS on HPCs; Kubernetes on research and commercial cloud; Docker/Podman or unprivileged QEMU VMs for researchers. A comprehensive metadata database provides easy tool discovery. In collaboration with the National Imaging Facility, SCIGET is integrated into the Australian Imaging Service, offering biomedical imaging researchers a comprehensive data and analysis solution.\nConclusion\nSCIGET aims to accelerate scientific discovery by streamlining access to essential software. It reduces setup overhead, enhances reproducibility across computing platforms, and fosters greater collaboration by ensuring consistent analytical environments. SCIGET is domain-agnostic and seeks to expand across scientific domains via community collaboration and engagement."
    },
    {
        "day": "Thursday",
        "time": "11:20 – 11:40",
        "location": "Boulevard Auditorium",
        "session_chair": "Noel Faux",
        "title": "MAVERIC: Enabling the Future of AI-Driven Research in Australia",
        "authors": "Amr Hassan",
        "url": "https://conference.eresearch.edu.au/maveric-enabling-the-future-of-ai-driven-research-in-australia/",
        "details": "Dr Amr Hassan1, Mr Simon Forsayeth1, Mr Joseph Pineda1, Ms Gin Tan1, Prof David Powell1\n1Monash University, Clayton, Australia\nAmr Hassan is the Director of Emerging Technologies at Monash University. He leads the development and implementation of strategic initiatives to identify, adopt, and embed emerging technologies, such as AI and intelligent computing, across the University’s education, research, and operations.\nAmr works closely with senior stakeholders to deliver enterprise-wide transformation programs. He is responsible for the delivery of a major infrastructure initiative, overseeing its technical and organisational components, and ensuring alignment with Monash’s Impact 2030 strategy.\nMonash University is developing MAVERIC (Monash AdVanced Environment for Research and Intelligent Computing), a new AI supercomputing platform designed to support cutting-edge research. MAVERIC will provide Monash researchers with the computational power needed to train and fine-tune large-scale AI models.\nThis presentation will explore MAVERIC’s architecture and software platform, highlighting how its design supports large-scale data analysis and AI model training. We will outline how the integration of high-performance GPU resources and high-speed connectivity contributes to efficient performance and enables seamless data processing across distributed systems.\nWe will also discuss the software tools and frameworks available to researchers for developing, deploying, and optimising AI models across various applications. MAVERIC’s scalable architecture is designed to meet a wide range of research needs, enabling Monash researchers to address complex scientific challenges."
    },
    {
        "day": "Thursday",
        "time": "11:20 – 11:40",
        "location": "Boulevard B1",
        "session_chair": "Ghulam Murtaza",
        "title": "Your Future Self Will Thank You: Encouraging Good Data Management Practice at Project Closure",
        "authors": "Vanessa Crosby",
        "url": "https://conference.eresearch.edu.au/your-future-self-will-thank-you-encouraging-good-data-management-practice-at-project-closure/",
        "details": "Dr Vanessa Crosby1\n1CSIRO, Australia\nDr Vanessa Crosby is Executive Manager, Research Services at CSIRO, leading the Research Data Services, Technology Solutions for Research, Lab Support, and Enterprise Information Management teams.\nCSIRO’s Information Management and Technology (IM&T) provides cutting edge data storage and compute services for CSIRO’s Science Units. Like many research organisations, we’ve been grappling with exponential growth of research data holdings and a tendency for projects to leave data in working storage spaces after projects end closure resulting in orphaned data, cost inefficiencies, and gaps in metadata essential for managing data assets into the future. A concerted change management effort was required to shift the perception of data clean-up activities from a low priority administrative task to an enabler for science impact.\nThe Research Data Management Capability Uplift Project, part of the Managed Data Ecosystem Programme, brought together stakeholders from Research Data Services, Technology Solutions for Research and the Research Units to investigate barriers faced by researchers preventing completion of four key activities: Publication, Archiving, Deletion, and Transfer. Workshops with researchers identified resource and time constraints at the delivery end of projects, difficulty navigating requirements and retention schedules, and unclear workflows across different systems as pain points.\nBy employing human centred design principles to map researcher journeys through our platforms and embedding just-in-time guidance into the Resource Management System’s automated project closure processes, researchers are now guided through the decision-making process required to clean up and manage end-of-project data.\nThis paper will share lessons learned and successes on our path to instilling the idea among our researchers that good data lifecycle management is fundamental to maintaining community trust in CSIRO science and driving research impact."
    },
    {
        "day": "Thursday",
        "time": "11:20 – 11:40",
        "location": "Boulevard B2",
        "session_chair": "Edoardo Tescari",
        "title": "FromMetricsto Meaning: A Contextual AI Platform for Research Impact Assessment",
        "authors": "Sebastian Haan",
        "url": "https://conference.eresearch.edu.au/from-metrics-to-meaning-a-contextual-ai-platform-for-research-impact-assessment/",
        "details": "Dr Sebastian Haan1, Mrs. Kirsten Jackson1\n1The University of Sydney, Newtown, Australia\nSeb is a machine learning and AI specialist at the Sydney Informatics Hub, University of Sydney, with expertise in probabilistic models, computer vision, geospatial ML, and AI agent networks. With a background in particle and astrophysics, he has held research positions at Caltech, CSIRO, and the Max-Planck-Institute. An active open-source contributor and published author in major scientific journals, Seb is passionate about developing innovative AI applications that accelerate research and create real-world impact. He brings extensive data-driven research experience to tackle emerging challenges in the rapidly evolving AI landscape.\nResearch impact evaluation is crucial for funding and policy decisions, yet traditional assessment methods remain confined to bibliometric and funding metrics, often overlooking real world benefits. Evidence scattered across disconnected sources makes comprehensive evaluation both time-consuming and incomplete. ResearchPulse automates the Research Impact Assessment Framework (RIAF) by combining Large Language Models with intelligent evidence discovery to generate impact assessments and use-cases within minutes.\nWe developed ResearchPulse featuring a Context Agent that uses Large Language Models to iteratively identify knowledge gaps and proactively search for missing evidence across heterogeneous data sources — from academic databases to policy documents, web content, and structured data catalogues. The system embeds RIAF criteria directly into its reasoning process, using semantic understanding to connect research outputs with their real-world impacts.\nThe platform synthesises findings from multiple sources into coherent narratives, linking research activities to measurable impacts whilst maintaining accurate source referencing. The system provides explainable AI decisions at each assessment stage, ensuring transparency in the evaluation process. Feedback integration enables researchers to refine assessments and customise indicators. Trials at the University of Sydney and Monash University demonstrate that ResearchPulse reduces assessment time from weeks to minutes whilst maintaining evidence-based rigour through automated fact validation.\nAs research funding becomes increasingly competitive and impact-driven, this web-based platform transforms how institutions evaluate, document, and communicate research outcomes. Field-specific templates enable adaptation across academic disciplines. This presentation will showcase the technical architecture, demonstrate practical implementation, and outline our roadmap for making the platform accessible to all Australian research organisations."
    },
    {
        "day": "Thursday",
        "time": "11:20 – 11:40",
        "location": "Boulevard B3",
        "session_chair": "Andrew Underwood",
        "title": "Next Generation System Integration for Sustaining AI/HPC Growth",
        "authors": "Joshua Fryman",
        "url": "https://conference.eresearch.edu.au/next-generation-system-integration-for-sustaining-ai-hpc-growth/",
        "details": "As data centres scale to multi-gigawatt (GW) capacities with 1 megawatt (MW) racks, the challenges in resource sustainability for delivering these technologies are increasing. While businesses race to be the first to realize Artificial General Intelligence, the continuing increase of computing performance in an era after Dennard scaling failed means that the energy demand and power issues dominate the practical aspects of building such systems. We are standing at a technology inflection point where the methods used previously for packaging, integration, and interconnects of semiconductors will not be viable into the next decade. New methods of designing chips, packages, memories, and fabrics must rapidly converge for the next decade’s products to succeed.\nThis talk will provide an overview of current economic trends, historical technical performance factors, and demonstrate where technology scaling is not keeping pace with the demand from the current AI/HPC boom. It will be followed by a proposed direction that next-generation products will have to adopt to deliver sustainable, continued performance growth for consumers.\nSpeaker:\nDr. Joshua Fryman, Intel Fellow and Director of Intel Government Technology’s R&D Group\nDr. Joshua Fryman is an Intel Fellow and Director of Intel Government Technology’s R&D group, setting the strategy and priorities for all external R&D between Intel and Public Sector sponsor agencies or commercial entities.  He joined IGT from the Intel Corporation’s Office of the CTO.\nPrior to his role in IGT, his work was primarily focused on applied R&D for commercial and government customers, focusing on problems that are 3-10 years out on the horizon. Throughout his career, Dr. Fryman has been engaged in multiple project types, from fabricating and testing circuits to designing microprocessor and system architectures, interconnects, packaging, compilers, runtimes, applications, and algorithms.\nIn his role as Chief Architect or Principal Investigator (PI), Dr. Fryman has led pre-exascale programs such as DARPA UHPC and DOE Fast Forward and Xstack. He has also played a pivotal role as PI for at-scale future AI and analytics programs such as DARPA HIVE, IARPA AGILE, and DOE’s Advanced Memory Technology efforts. His role as a PI also includes security programs such as DARPA GAPS and other appliances.\nDr. Fryman obtained his BS in Computer Engineering from the University of Florida and a PhD in Computer Architecture from Georgia Tech. Renowned for his insights in the field, he is a frequently invited speaker on microelectronics, systems, packaging, integration, and hardware-software co-design topics."
    },
    {
        "day": "Thursday",
        "time": "11:20 – 11:40",
        "location": "Boulevard Room",
        "session_chair": "Maciej Cytowski",
        "title": "Evolving Systems for Biodiversity Science: The Atlas of Living Australia’s Next-Gen Cloud Platform Journey",
        "authors": "Kylie Morrow",
        "url": "https://conference.eresearch.edu.au/evolving-systems-for-biodiversity-science-the-atlas-of-living-australias-next-gen-cloud-platform-journey/",
        "details": "Ms Kylie Morrow1, Mr Joe Lipson1\n1Atlas Of Living Australia (ALA) / CSIRO, Australia\nKylie is an experienced tech professional who currently leads the coordination of digital content across the Atlas of Living Australia (ALA), CSIRO. Driven by a deep passion for biodiversity and technology, Kylie plays a central role in projects spanning UX/UI, software testing and user support to drive innovation across ALA’s digital infrastructure. Her work is informed by a diverse professional and educational background encompassing design, marketing, ecology, and genomics. Kylie believes in harnessing data and technology to inform transformative research and decision-making. She is actively committed to championing diversity and inclusion in STEM and across the product lifecycle.\nThe Atlas of Living Australia (ALA) is a nationally significant, open infrastructure platform that aggregates, enriches, and shares biodiversity data. With over 150 million species occurrence records, the ALA empowers researchers, policymakers, educators, government agencies, and the public—delivering open data and tools that drive evidence-based decisions and conservation efforts.\nTo address growing demands for performance, scalability, and security, the ALA has launched a multi-phase digital transformation: modernising both its platform and software systems through the Next Generation Cloud Platform initiative. This journey is centred on re-architecting ALA’s infrastructure and systems using containerised services, automated orchestration, and scalable, modular microservice patterns, paired with secure, responsive user interfaces to enable long-term flexibility and sustainability.\nAs this transformation continues, several key milestones have already been achieved, including:\n– End-to-end CI/CD automation, streamlining everything from infrastructure provisioning to application deployment.\n– A cloud-native frontend portal, powered by high-availability, low-latency storage systems.\n– A centralised security and exposure management layer, providing unified monitoring.\n– A modern identity and access management system using OpenID Connect protocol and MFA currently being piloted across core services.\nWe will share insights from this ongoing evolution, highlighting architectural advances, practical challenges, and real-world benefits already realised. These include improved deployment efficiency, reduced operational costs, and enhanced platform resilience, even as the system continues to adapt and scale.\nBy laying a robust foundation for interoperability, cloud scalability, and user experience, the ALA is building a future-ready digital ecosystem that will continue to empower Australia’s biodiversity science and conservation community for years to come."
    },
    {
        "day": "Thursday",
        "time": "11:40 – 12:00",
        "location": "Boulevard Auditorium",
        "session_chair": "Noel Faux",
        "title": "Towards a Unified Access Infrastructure for Australia’s Life Sciences Platforms",
        "authors": "Amanda Zhu,Dr Steven Manos",
        "url": "https://conference.eresearch.edu.au/towards-a-unified-access-infrastructure-for-australias-life-sciences-platforms/",
        "details": "Ms Amanda Zhu1, Dr Steven Manos, Uwe Winter\n1Australia Biocommons, Sydney, Australia\nAmanda Zhu is the Software Group Lead at Australian BioCommons, where she leads the development of national-scale infrastructure to support life sciences research. With a background in autonomous systems and bioinformatics, she specializes in building secure, interoperable, and user-centric platforms. Amanda currently heads the BioCommons AAI initiative—an identity and access solution designed to unify authentication across Australia’s life science services. She collaborates with teams from Galaxy Australia, Bioplatforms Australia, and other partners to streamline data access and service integration. Amanda is passionate about creating digital research environments that are both robust and accessible for diverse scientific communities.\nBackground\nThe Australian BioCommons operates a range of online data and analysis platforms for molecular life sciences research. These include Galaxy Australia and the Bioplatforms Data Portal. Each of these platforms maintains its own identity and access management. This has led to fragmented user experiences, inconsistent security, and reliance on repetitive manual tasks—such as logging into multiple platforms to transfer data for analysis. These disjointed identity workflows and siloed data mobility are major barriers to efficiency, collaboration, and reproducibility in modern life sciences.\nMethod\nTo address these systemic limitations, Australian BioCommons is building a common Identity and Access Infrastructure (AAI) designed to unify authentication and authorization across the ecosystem. Using Auth0 as its core, the solution enables email-based self-registration, role-based access control, metadata-driven automation, and centralized administration. This solution supports ISO 27001 compliance and is engineered for extensibility across future services.\nResults\nEarly integrations with Galaxy Australia and the Bioplatforms Data Portal have validated core functionality including centralized user registration, automated group-based permissions, and seamless user management. These components reduce the operational burden on individual services and demonstrate how a shared identity layer can unify disparate systems under a single, researcher-friendly access framework.\nConclusion\nMore than a technical uplift, this initiative establishes a foundational infrastructure for streamlined, secure, and scalable access across Australia’s life sciences platforms. By eliminating redundant identity handoffs and supporting transparent data flow between services, the AAI unlocks a federated model for managing researcher access—enhancing productivity, fostering collaboration, and enabling deeper integration across the national research landscape."
    },
    {
        "day": "Thursday",
        "time": "11:40 – 12:00",
        "location": "Boulevard B1",
        "session_chair": "Ghulam Murtaza",
        "title": "Operationalising Trust in Data Spaces",
        "authors": "Fahamame Emamjome",
        "url": "https://conference.eresearch.edu.au/operationalising-trust-in-data-spaces/",
        "details": "Dr Fahamame Emamjome1\n1Australian Access Federation (AAF), Brisbane, Australia,2Australian Research Data Commons (ARDC), Brisbane, Australia\nDr Fahame Emamjome is an eResearch Analyst at the Australian Access Federation (AAF), where she works with the Trust and Identity team to co-design a national approach to identity and access management for Australia’s research sector. She brings extensive experience in data governance, digital infrastructure, and business analysis, with a strong focus on enabling secure and seamless access to research services. Fahame is passionate about leveraging technology and innovation to support collaboration, privacy, and trust in research environments.\nEstablishing trust and enabling collaboration are core goals of both European and global data spaces. However, operational guidance on trust and identity is often underdeveloped in current dataspace frameworks.\nTo address this gap, the Australian Access Federation (AAF) and the Australian Research Data Commons (ARDC), are partnering on an incubator as part of the development of an Australian National Trust and Identity (T&I) Framework for National Research Infrastructures (NRI) funded by the Australian Government’s National Collaborative Research Infrastructure Strategy (NCRIS).\nThis incubator investigates how existing dataspace governance and implementation guidelines align with the Framework, with the aim of demonstrating how it can support essential dataspace functions—such as participant onboarding, authentication, and access governance.\nThe initial focus is on mapping the Sitra rulebook—a widely referenced governance model developed by the Finnish Innovation Fund—to the components of T&I Framework. This mapping evaluates how it addresses specific implementation challenges outlined in Sitra’s policy checklists, using real-world Australian use cases for grounding. Where needed, the analysis also incorporates elements from international dataspace reference architectures to support broader applicability.\nThis work contributes to the development of an Australian dataspace rulebook by identifying the alignments and gaps between the Sitra rulebook and the established, interoperable T&I Framework for Australians NRIs.\nBy bridging high-level governance models with operational trust and identity guidelines and policies, this work advances the creation of secure, scalable, and collaborative data spaces. It offers actionable insights for policy makers, system architects, and implementers seeking to operationalise trust in cross-institutional data sharing environments."
    },
    {
        "day": "Thursday",
        "time": "11:40 – 12:00",
        "location": "Boulevard B2",
        "session_chair": "Edoardo Tescari",
        "title": "Different Models, Same Challenges: A Conversation oneResearchOperations",
        "authors": "Different Models, Same Challenges: A Conversation oneResearchOperations",
        "url": "https://conference.eresearch.edu.au/different-models-same-challenges-a-conversation-on-eresearch-operations/",
        "details": "Dr Edoardo Tescari1, Mr Shaun Grady2, Dr Julie Iskander3, Dr Jacky Kwun Lun Cho4, Dr Anastasios Papaioannou5\n1The University of Melbourne, Australia,2Hunter Medical Research Institute, Australia,3Walter and Eliza Hall Institute of Medical Research, Australia,4University of New South Wales, Australia,5University of Technology Sydney, Australia\nEdoardo Tescari is a Senior Research Data Specialist in the Melbourne Data Analytics Platform (MDAP) team at The University of Melbourne. Edoardo has extensive experience in the use of multi-core computer clusters, parallel computational techniques and management of large data sets. In his current role within MDAP, Edoardo has been collaborating with academics from different disciplines, and contributed to research on climate, biosecurity and dietary modelling, as well as bioinformatics and genomics. Edoardo has been advocating for the recognition and development of a digitally skilled eResearch workforce within and outside The University of Melbourne.\nShaun Grady is a Senior Health Informatician within the Data Management & Health Informatics (DMHI) stream at Hunter Medical Research Institute and manages the Data Sciences Research Training Program. While the DMHI team specialises in REDCap, data collection and data linkage, support provided by the broader Research Translation and Healthcare Improvement Platform includes biostatistics, health economics, clinical trials coordination and research impact evaluation. Shaun is originally a Radiation Therapist by training with specific research interests in digital health delivery, clinical process improvement and utilisation of health data.\nJulie Iskander leads the Research Computing Platform at WEHI, where she brings her background in software engineering, computational biology, and biomedical research to drive infrastructure development and support scalable, data-driven science. Her team helps researchers bridge the gap between scientific questions and computational solutions, with growing emphasis on making AI tools and cloud platforms accessible across disciplines.\nJacky Cho is a program manager with Research Technology Services at UNSW, focusing on infrastructure and governance for research data storage and management. He is also a lead for the ResTech community program, which upskill and engages research communities to make the best use of research infrastructure both at UNSW and nationally. Prior to this, he was a researcher specialising in the physical chemistry of surface coatings and used research data and research computing infrastructure in synchrotrons and universities in Australia and Europe.\nAnastasios Papaioannou is a Senior Manager leading the eResearch Platforms and Services at UTS, where he oversees cloud and high-performance computing (HPC), research data storage, training, and the integration of AI in research. He actively works on the adoption of AI by collaborating with UTS colleagues to develop a comprehensive suite of AI guidelines, tools, and training programs for researchers. With a strong background in research, data science, and computational physics, he works closely with academics and HDR students to help them leverage large-scale infrastructure and digital technologies to accelerate their research.\nThe delivery of eResearch support varies across institutions, ranging from centralised service teams to embedded roles within research units, and from professional staff to academic and technical specialists. While the sector shares many common challenges, solutions are often developed in isolation, limiting opportunities for shared learning and collaboration.\nThis BoF session aims to explore how eResearch support units are evolving in response to shifting demands, with a focus on three key areas: operations and team structure, cost recovery and funding models, and strategies for demonstrating value. Our BoF aims to better understand how different institutions are navigating these areas and to identify opportunities for alignment and collaboration.\nWe will begin with a guided discussion around three core questions:\n– Who is on your team—and why? How are we structuring our teams to meet growing and changing needs? What skills are we prioritising, and how do we balance technical, academic, and support roles?\n– Where does the money come from—and how do we keep the lights on? What funding models are sustainable?\n– How do you prove your value? How do we communicate our impact to leadership? What metrics matter?\nAttendees will be encouraged to share their own experiences, challenges, and strategies, sparking a lively exchange of ideas. The session will conclude by exploring ways to keep the momentum going beyond the conference and continue the conversation by promoting ongoing knowledge sharing and cross-institutional collaboration."
    },
    {
        "day": "Thursday",
        "time": "11:40 – 12:00",
        "location": "Boulevard B3",
        "session_chair": "Andrew Underwood",
        "title": "Unlocking New Research Frontiers with AI-Powered Infrastructure",
        "authors": "Unlocking New Research Frontiers with AI-Powered Infrastructure",
        "url": "https://conference.eresearch.edu.au/empowering-a-new-wave-of-science-with-dell-poweredge-xe9712-nlv72/",
        "details": "Part of the Dell AI Factory with NVIDIA, the Dell PowerEdge XE9712 NVL72 delivers transformative compute density and speed, enabling researchers to push the boundaries of artificial intelligence, simulation, and modelling. This session will discuss how this platform unlocks new possibilities for scientific discovery, driving innovation across fields.\nThe talk will be followed by an interactive session that explores the transformative impact of AI adoption on scientific research infrastructure. We’ll discuss how the rise of trillion-parameter models and mixture of expert architectures demands innovative systems design, unlocking unprecedented computational capabilities. Join a collaborative conversation on how these advances are accelerating discovery, fostering new global partnerships, and reshaping scientific workflows for the better.\nSpeakers\nAndrew Underwood, Dell TechnologiesLuka Topic, Data Management Specialist, Dell TechnologiesDr Amr Hassan, Director of Emerging Technologies, Monash UniversityDr François Bissey, eResearch Consultant, University of CanterburyDarren Hart, Head of eResearch Support, University of Otago\nAndrew Underwood is a technology leader and system theorist who orchestratesDell Technologies’ high performance computing, edge computing and AI go to market in Asia Pacific and Japan. He joined Dell in 2014 after five years at the Victorian Partnership for Advanced Computing, known as VPAC Ltd, where he led investment in supercomputers and scientific computing platforms, and enterprise companies across Australia. Today, he works hand-in-hand with industrial, academic and government clients to build the computational foundations critical to their scientific advancement and global economic competitiveness.​"
    },
    {
        "day": "Thursday",
        "time": "11:40 – 12:00",
        "location": "Boulevard Room",
        "session_chair": "Maciej Cytowski",
        "title": "Scientific Applications and Workflows Support – Best Practices, Challenges and Approaches",
        "authors": "Scientific Applications and Workflows Support – Best Practices, Challenges and Approaches",
        "url": "https://conference.eresearch.edu.au/scientific-applications-and-workflows-support-best-practices-challenges-and-approaches/",
        "details": "Dr Maciej Cytowski1, Mike Lynch2, Glen Charlton3, Dr Rui Yang4, Anselm Motha5\n1Pawsey Supercomputing Research Centre, Perth, Australia,2The University of Sydney, Sydney, Australia,3Intersect Australia, Sydney, Australia,4National Computational Infrastructure, Canberra, Australia,5University of Technology Sydney, Sydney, Australia\nMaciej Cytowski:\nMaciej Cytowski is a computational scientist and expert in high-performance computing and data science. He specialises in designing and implementing computational strategies for complex scientific problems, particularly in fields like computational physics, computational biology, weather, climate, and artificial intelligence. His background is mathematics, and he holds a PhD in computational science. Currently, he is the Head of Scientific Services at the Pawsey Supercomputing Research Centre in Perth, Western Australia.\nMike Lynch:\nMike Lynch is a data science group lead at the Sydney Informatics Hub, with expertise in research data management, open standards for research data and the application of modern IT development and deployment practices to research software. In his current role he leads a team of software engineers and research compute specialists working to enable researchers to best utilise national cloud compute and the University’s GPU cluster.\nGlen Charlton:\nGlen is the Lead Data Scientist for the Advanced Analytics & AI (3AI) Platform at Intersect Australia. The 3AI team empowers researchers with data science and AI through hands-on support and the transfer of knowledge to improve the efficiency and increase the capability of researchers to conduct novel and practically relevant research. Glen actively promotes the novel and responsible adoption of AI for both research and operational purposes within members, partners and internally within Intersect.\nRui Yang:\nRui works as a Senior HPC Specialist at the National Computational Infrastructure (NCI), where he leads the Software and Data Modernisation team. The team provides dedicated support to multiple domain research communities by maintaining specialised computing environments, optimising performance for scientific models, and supporting collaborative projects. Rui led initiatives in deploying and evaluating state-of-the-art AI/ML models on HPC systems, maintaining  ML-ready datasets and co-develop AI workflows with research groups to meet domain-specific needs.\nAnselm Motha:\nAnselm is the Specialist Research Technology Manager at the University of Technology Sydney (UTS). He leads a team that drives engagement with the research community at UTS. The team support a traditional queue based HPC system along with an interactive HPC system specialised for engineering workloads. Anselm strives for a collaborative IT environment for research at UTS, he works closely with other team within the IT Unit to ensure that research projects are closely aligned with the polices at UTS.\nSituation\nWith developments in Artificial Intelligence (AI), scientific research increasingly relies on complex computational workflows that integrate diverse applications, data sources, and infrastructure. Supporting these workflows requires a deep understanding of both domain-specific needs and evolving technologies. Additionally, the diversity of applications and tools used by researchers grows exponentially with the development of new approaches in High Performance Computing (HPC) and AI.\nTask\nThis session invites practitioners, researchers, and support professionals to share and discuss best practices, challenges, and strategies for enabling robust, scalable, and reproducible scientific workflows. We will explore approaches to workflow design, automation, and optimization across a range of disciplines, with a focus on practical and scalable support models, user engagement, and cross-institutional collaboration. Topics may include workflow management systems, reproducibility and containerization, GPU porting and optimisation, data management and storage, application enabling and integration with HPC, cloud, and hybrid environments.\nAction\nThis session aims to foster a collaborative dialogue around what works, what doesn’t, and what’s next in supporting scientific applications and workflows. The session will begin with short presentations from leading institutions in Australia, focusing on changes and challenges in scientific applications and workflows support over the last couple of years. We will then switch to discussion in subgroups to focus on requirements of specific domains and selected aspects of the support ecosystem.\nResult\nWe hope that participants will leave with actionable insights and new connections to strengthen and improve services (providers) and leverage or integrate with existing services (researchers)."
    },
    {
        "day": "Thursday",
        "time": "12:00 – 12:20",
        "location": "Boulevard Auditorium",
        "session_chair": "Noel Faux",
        "title": "The future of eResearch in Aotearoa",
        "authors": "Georgina Rae",
        "url": "https://conference.eresearch.edu.au/the-future-of-eresearch-in-aotearoa/",
        "details": "Dr Georgina Rae1, Amber McEwen1, Deidre Hill1, Yeshaswini Ramesh1, Brett Calton1, Blair Bethwaite1\n1REANNZ, Auckland, Aotearoa / New Zealand\nGeorgina is the Head of Science Engagement at REANNZ where she ensures that REANNZ supports NZ's researchers and research priorities through meaningful partnerships and user-driven approaches. Before diving into the eResearch space, she worked in molecular biology and intellectual property.\nAotearoa’s research sector is going through a period of significant change, emerging from various reviews and consultations aimed at advising the government on how to strengthen the science, innovation, and technology system. EResearch is identified as a critical enabler of research success and impact.\nOver the last decade, REANNZ and NeSI have supported the sector to achieve significant research impacts. In response to the advice from these reviews, the Minister of Science, Innovation and Technology requested that REANNZ outline how it could integrate the services and capabilities provided by NeSI, into REANNZ. Since then, we have been on a journey to make that happen.\nThis evolution of how eResearch is delivered and supported in Aotearoa aims to combine the unique capabilities of both REANNZ and NeSI to develop a stronger value proposition for the sector, including maintaining and growing our long-standing and valuable trans-Tasman partnerships.\nIn this talk, we will provide some background on the New Zealand research sector and celebrate the heritage and impact of eResearch in New Zealand. We will then give an update on what has been happening in 2024/2025 (spoiler alert – lots!) and end with a view to the future."
    },
    {
        "day": "Thursday",
        "time": "12:00 – 12:20",
        "location": "Boulevard Room",
        "session_chair": "Maciej Cytowski",
        "title": "Leading the REDCap service across UWA",
        "authors": "Leading the REDCap service across UWARose-Anna Inglis",
        "url": "https://conference.eresearch.edu.au/leading-the-redcap-service-across-uwa/",
        "details": "Ms Rose-anna Inglis1\n1University Of Western Australia, Perth, Australia\nRosie Inglis is currently an Information Specialist at the University of Western Australia, having worked in information management for the majority of her career. In this role, she works primarily in systems management for the UWA Profiles and Research Repository as well as with REDCap and the UWA Research Data Hub. She is enthusiastic about collaboration and positive solution design in the research space.\nThe Research Publications and Data Services team at UWA took over the university's REDCap instance at the start of 2022 because of changes within the university structure. The system needed a major overhaul bring it up to date and to meet the needs of our researchers. This talk gives an overview of our journey revitalising the system, implementing cybersecurity measures, building support materials and processes. It also outlines our journey developing our knowledge about REDCap to become system experts at our institution.\nWe performed our own research, designed training materials and met with current users to understand how the platform was being used. We also participated in user groups to grow our expert network.\nREDCap remains a core research platform for our academics but now operates in line with university standards and with defined support structures. We have successfully migrated to a new server and implemented single sign on for user integrity.\nThe knowledge we gain in this process has positioned us as valuable team members for other library projects, including navigating our move away from the Qualtrics survey platform over the next year."
    },
    {
        "day": "Thursday",
        "time": "12:20 – 12:40",
        "location": "Boulevard Auditorium",
        "session_chair": "Noel Faux",
        "title": "The Australian Imaging Service as the National Imaging Facility’s Foundational Digital Research Infrastructure",
        "authors": "Ryan Sullivan",
        "url": "https://conference.eresearch.edu.au/the-australian-imaging-service-as-the-national-imaging-facilitys-foundational-digital-research-infrastructure/",
        "details": "Dr Ryan Sullivan1,2,3\n1The Australian Imaging Service,2The University of Sydney, Sydney, Australia,3National Imaging Facility,\nRyan is the Director of the Australian Imaging Service, under the PVC-Research Infrastructure at the University of Sydney.\nhttps://orcid.org/my-orcid?orcid=0000-0001-5554-7378\nThe first phase of the Australian Imaging Service (2020-24) saw the creation of the national federated software platform for secure data management, analysis, and informatics of biomedical imaging data through a series of projects funded by the ARDC Platforms Program, NIF, and MRFF NCRI. This resulted in a data centric computing design, tightly coupling 4 key services into a cohesive platform: Data Capture & Management (XNAT), Automated Pipelines (K8s scheduler & ARCANA/Pydra), Interactive Analysis (JupyterHub & Neurodesk/SciGet), and Machine Learning (MONAI & NVFlare).\nIn our second phase (2025-28), AIS is moving to a more tightly coupled federation to provide a cohesive national service through the National Imaging Facility Foundational Digital Research Infrastructure (FDRI) project. In addition to continued technical improvements and features in the 4 key services, the FDRI project will see the implementation of a single front door for accessing the infrastructure and our network of skilled imaging experts, a national training program to onboard new user communities, a joint financial and operating model for equitable and sustainable operations, and a node maturity model and legal framework for operating cyber-secure digital research infrastructure operated at an accredited quality standard.\nThis talk will outline our new capabilities and opportunities for engagement and partnership with NCRIS capabilities, institutions, or research projects."
    },
    {
        "day": "Thursday",
        "time": "12:40 – 13:40",
        "location": "Boulevard Auditorium",
        "session_chair": "Noel Faux",
        "title": "Lunch, Exhibition, Poster Viewing | Boulevard Foyer",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Thursday",
        "time": "13:40 – 14:00",
        "location": "Boulevard Auditorium",
        "session_chair": "Liz Stokes",
        "title": "WHACS: A new global Wave Hindcast for the Australian Climate Service, calibrated for extremes",
        "authors": "Claire Trenham",
        "url": "https://conference.eresearch.edu.au/whacs-a-new-global-wave-hindcast-for-the-australian-climate-service-calibrated-for-extremes/",
        "details": "Grant Smith2, Alberto Meucci3,1, Mx Claire Trenham1, Stefan Zieger2, Claire Spillman2, Ron Hoeke1, Bryan Hally1, Emilio Echevarria1, Vanessa Hernaman1, Blake Seers1\n1CSIRO, Australia,2Australian Bureau of Meteorology, Australia,3University of Melbourne, Australia\nClaire Trenham is a Senior Experimental Scientist leading the Coastal Extremes Modelling & Projections team within CSIRO’s Climate Intelligence program, and is the Digital and Data representative.\nClaire Trenham joined CSIRO in 2011 working on wave climate modelling for Australian, Pacific and global contexts. After a period working as Senior Research Data Services Specialist for the National Computational Infrastructure (NCI) in Canberra between 2014-2017, Claire returned to CSIRO.\nClaire specialises in coastal extremes modelling, littoral and wave modelling, regional climate projections, data management, data optimisation and data publishing, and other climate data work across the research unit.\nhttps://orcid.org/my-orcid?orcid=0000-0003-4258-9936\nA new wave hindcast dataset is available known as WHACS (Wave Hindcast for the Australian Climate Service) which updates the now 15-year-old CAWCR Wave Hindcast with improved model physics, inputs, and resolution.\nThe Australian Climate Service is a partnership made up of world leading science and expertise from the Bureau of Meteorology, Geoscience Australia, CSIRO and Australian Bureau of Statistics. It brings the Commonwealth’s extensive climate and natural hazard information into a single national view. The WHACS product is part of the Coastal and Flooding Hazards work package.\nThe new hindcast updates the CAWCR product with a newer version of WaveWatchIII, a different parameterisation of model physics, and higher resolution atmospheric forcing data (ERA5 reanalysis). The new product leverages a “spherical multi-cell grid” structure to achieve resolutions of ~7km in coastal regions globally, and ~¼degree in the global open ocean, with an expanded set of locations with spectral data output.\nThe model was carefully tuned to respond more strongly to extreme winds in line with the forcing data being used tending to under-represent winds in extreme situations like tropical cyclones. This leads to better validation performance against in-situ and remotely sensed observations compared to the CAWCR wave hindcast.\nThe initial data release covers the period 1979-2023, we plan to operationalise it and eventually retire the CAWCR wave hindcast.\nIn this talk we will discuss the differences and improvements in the new data product, with a focus on data structure optimisation undertaken to vastly improve performance of the data for users."
    },
    {
        "day": "Thursday",
        "time": "13:40 – 14:00",
        "location": "Boulevard B1",
        "session_chair": "Masoud Rahimi",
        "title": "Helping deliver Nature-Positive outcomes through a Shared Environmental Analytics Facility (SEAF)",
        "authors": "Luke Edwards",
        "url": "https://conference.eresearch.edu.au/helping-deliver-nature-positive-outcomes-through-a-shared-environmental-analytics-facility-seaf/",
        "details": "Mr Luke Edwards1,2, Mr Brendan Busch2,3, Mr Chris Gentle2,3\n1Pawsey Supercomputing Research Centre, Kensington, Australia,2Western Australian Marine Science Institution (WAMSI), Crawley, Australia,3Western Australian Biodiversity Science Institute (WABSI), Crawley, Australia\nhttps://orcid.org/0000-0001-8590-3361\nLuke is a highly experienced IT professional with more than 15 years of experience across State Government, academia and research infrastructure institutions. A large part of this time has been spent helping marine researchers manage their data and working towards a sustainable data ecosystem.\nThe global community and the Australian Government have committed to Nature-Positive outcomes which involves halting environmental degradation, and nature being repaired and regenerated to previous levels. To enable this change we need to be able to account and report on environmental performance with increased assurance, transparency and accountability. An effective ‘supply chain’ of environmental information, data and tools that is repeatable and trusted for a range of stakeholders enables this to occur. The Independent Review of the Environment Protection and Biodiversity Conservation Act 1999 (EPBC Act) highlighted this requirement and the development of a SEAF aims to assist in this goal.\nA joint project is currently underway between State and Federal Government funded entities as well as the Australian Research Data Commons (ARDC) and industry. It will accelerate shared infrastructure, governance and organisational linkages between research computing infrastructure, commercial computing infrastructure and industry. The focus will be on enabling environmental assessment in the Cockburn Sound and Pilbara regions but will also develop capacity to on-board new regional ‘spokes’.  This approach will create real impact, translating science knowledge and data analytics into timely, practical management outputs whilst delivering significant economic, environmental and social benefits.\nDetails on the proposed SEAF can be found here –https://wabsi.org.au/our-work/projects/seaf/.  This presentation will provide updates on this journey and discuss the next steps with the eResearch community."
    },
    {
        "day": "Thursday",
        "time": "13:40 – 14:00",
        "location": "Boulevard B2",
        "session_chair": "Lesley Wyborn",
        "title": "A Scorecard to Measure Earth and Environmental Science Repository Resilience Both During Normal Times and in Times of Crisis.",
        "authors": "Lesley Wyborn",
        "url": "https://conference.eresearch.edu.au/a-scorecard-to-measure-earth-and-environmental-science-repository-resilience-both-during-normal-times-and-in-times-of-crisis/",
        "details": "Dr Lesley Wyborn1,5, Joseph Gum2, Ruth Duerr3, Reyna Jenkyns4, Dr Rebecca Farrington5, Dr Tim Rawling5, Dr Kelsey Druken6\n1NCI, Australian National University, Canberra, Australia,2National Center for Atmospheric Research, Boulder, United States of America,3Ronin Institute for Independent Scholarship, United States of America,4World Data System, Victoria, Canada,5AuScope Ltd, Melbourne, Australia,6ACCESS-NRI, Canberra, Australia\nLesley Wyborn is an Honorary Professor at ANU at the National Computational Infrastructure (NCI) and at the Research School of Earth Sciences. She also works part time for ARDC. She had 42 years’ experience in Geoscience Australia (GA) in both research and in data science/data management. Since leaving GA in 2014 has continued research in aspects of Data Science including data quality, versioning, reproducibility, operationalising the FAIR, CARE and TRUST principles and Open Science and is a player in many global informatics initiatives. Her current focus is the development of transparent high-performance aggregated national-scale datasets that are compatible with international data networks.\nEarth and environmental (E&E) datasets play a crucial role in Sustainable Development of our Planet, contributing to the prediction of natural hazards, effective development of natural resources, long term monitoring of vegetation changes, etc. Since the 1990s there have been concerted efforts to create global data networks (e.g., OneGeology, Federated Digital Seismology Network (FDSN), Global Biodiversity Information Facility (GBIF), Earth System Grid Federation (ESGF). As these networks become accepted as critical inputs into E&E data supply chains, their vulnerability is becoming of concern. Contemporary events have highlighted the need to formally assess the resilience of infrastructures underpinning these networks.\nRecently members of the Sustainable Data Management Cluster of the Earth Science Information Partners (ESIP) developed a repository scorecard (https://zenodo.org/records/15208172) to enable repositories to assess their resilience based around four scenarios:\n1. Incoming Natural Disaster forecast to hit the repository in 48 hours. The focus is on local destruction of the facility;\n2. Loss of Organizational Funding requiring shut down within one month. The focus is total loss of the repository, but with time for mitigating actions;\n3. Cyberattack/Organisational Infiltration with hostile agents gaining control of the cyberinfrastructure. The focus is on sudden denial of access to data; and\n4. Loss of Technical Expertise critical to running the repository. The focus is on loss of expertise to keep the repository running.\nThis presentation will outline key elements of measuring repository resilience, and will present suggestions to develop international consensus on policies and agreements for sustaining critical global E&E data supply chains."
    },
    {
        "day": "Thursday",
        "time": "13:40 – 14:00",
        "location": "Boulevard B3",
        "session_chair": "Jonathan Smillie",
        "title": "Evolving Data Landscape: Governance dimensions of Australian Dataspaces",
        "authors": "Shannon Callaghan, Kheeran Dharmawardena",
        "url": "https://conference.eresearch.edu.au/evolving-data-landscape-governance-dimensions-of-australian-dataspaces/",
        "details": "Mr Shannon Callaghan1, Mr Kheeran Dharmawardena1\n1ARDC, Australia\nShannon has 20 years’ experience developing cross-organisational decision systems for defence, law enforcement, government and research. As part of the policy team, he advocates for a coherent policy environment that supports research excellence, impact, integrity, innovation and efficiency.\nKheeran has over 2 decades of experience in delivering ICT services within the higher education and research sector, including infrastructure delivery, service delivery, data management, IT & enterprise architecture and eResearch. He has a special interest in the socio-technical challenges involved in the delivery of data and information infrastructures.\nAustralia’s rapidly expanding data ecosystem faces mounting challenges in scaling effective governance amid increasing complexity driven by the digital economy, AI innovation, and shifting geopolitics emphasizing data sovereignty. Existing governance models often struggle to scale effectively, limiting the potential for secure, trustworthy, and large-scale data sharing among diverse stakeholders.\nThis presentation explores the emerging governance framework for complex data ecosystems that is based on the International Data Spaces architecture which emphasize trust-based approaches that prioritize the governance of the data ecosystem with an emphasis on data sovereignty, meaningful decision rights, stakeholder empowerment, and flexible collaboration mechanisms.\nThe presentation explores how Australia will benefit from expanding its focus from governance of data to governance of the data ecosystem. By doing so we can start to create a more holistic and adaptive data landscape that can support the dynamic needs of modern data driven research and innovation. This exploration aims to inform policy development and foster discussions about building resilient, scalable, interoperable, and sovereign data infrastructures."
    },
    {
        "day": "Thursday",
        "time": "13:40 – 14:00",
        "location": "Boulevard Room",
        "session_chair": "Kyle Hemming",
        "title": "Federated Learning for eResearch",
        "authors": "Peter Marendy",
        "url": "https://conference.eresearch.edu.au/federated-learning-for-eresearch/",
        "details": "Mr Peter Marendy1\n1Qcif Ltd, St Lucia, Australia\nPeter is currently employed as Head of Data and Software Solutions at the Queensland Cyber Infrastructure Foundation (QCIF), which provides eResearch infrastructure and services for Queensland research institutions and contributes to the National Research Infrastructure.\nIn this role, Peter is responsible for leading a team that delivers innovative, high quality, and time driven results for a wide variety of research programs from within universities, research institutes, and commercial companies. The team has expertise in workflows, specialised computing, data capture and management, and working with sensitive data.\nPrior to his role with QCIF, Peter led the Microsystems research within CSIRO’s Cybernetics research Group. During his time at CSIRO, Peter also led projects such as Bees with Backpacks, Optimising Pollination, Smart Hives, Probing Biosystems – Implantables, Brain Implants, and Smart Helmets for optimising the timing of Cranioplasty.\nPeter is also a member of the RSE-AUNZ steering committee which aims to build awareness of the diverse Research Software Engineer (RSE) roles, to connect volunteers in the RSE domain, and to help build practical solutions for the RSE Community.\nPeter brings his experience in team/capability management, project management, and customer focused collaboration and relationships. A people focused leader with broad knowledge across multiple domains with a focus on cutting edge development and applications. An excellent technology communicator to the public, academia, government, grant bodies, and industrial stakeholders.\nHe also has more than 20 years of software engineering experience across multiple domains, including Digital Agriculture, Energy, Food and Nutritional Sciences, Health, Marine Sensing, Robotics, and Visual Analytics, in research and innovation environments.\nAs data privacy regulations tighten and datasets grow increasingly siloed across institutions, Federated Learning (FL) offers a transformative approach to collaborative machine learning in the eResearch domain. This talk introduces the core principles of Federated Learning—how it enables model training across decentralised data sources without moving the data itself—and explores its relevance to research environments where data sensitivity, ownership, and locality are paramount.\nWe will outline the main types of FL, including horizontal, vertical, and federated transfer learning, and discuss their applicability to real-world research scenarios. Drawing from our own experience, we’ll share insights into evaluating and selecting FL frameworks, the practical challenges of setting up federated infrastructure, and lessons learned from early implementations.\nFinally, we’ll look ahead to where the field is going: the growing role of privacy-preserving technologies like differential privacy and secure multiparty computation, the need for standardisation, and the potential for FL to unlock new forms of cross-institutional collaboration in science and academia.\nWhether you're a researcher, data scientist, or infrastructure specialist, this session will provide a practical and forward-looking perspective on how Federated Learning can reshape data-driven research."
    },
    {
        "day": "Thursday",
        "time": "14:00 – 14:20",
        "location": "Boulevard Auditorium",
        "session_chair": "Liz Stokes",
        "title": "Delivering Spatial Ecosystem Data at Scale: TERN’s Cloud Native Approach",
        "authors": "Gerhard Weis",
        "url": "https://conference.eresearch.edu.au/delivering-spatial-ecosystem-data-at-scale-terns-cloud-native-approach/",
        "details": "Mr Gerhard Weis1, Dr. Siddeswara Guru1, Tiancheng Lan1, Enzhen Luo1\n1University of Queensland, Indooroopilly, Australia\nSiddeswara Guru is the program lead for the TERN Data Services and Analytics Platform.\nGerhard Weis is the solution architect and technical lead in the TERN Data Services and Analytics Platform.\nTiancheng Lan is a geospatial Data Engineer in the TERN Data Services and Analytics Platform.\nEnzhen Luo is a senior Software Engineer in the TERN Data Services and Analytics Platform.\nThe Terrestrial Ecosystem Research Network (TERN) is an NCRIS-funded land-based observatory that measures the environment from continental to site scales using various observation methods, including remote sensing, in situ, and human observations.\nA substantial part of TERN’s geospatial data holdings are regional and continental-scale raster datasets, complemented by high-resolution drone imagery and LiDAR datasets. Often, these datasets form a time series over multiple decades. The large volume and size of these datasets provide significant challenges in data movement, processing, publication, and visualization. The solution should be robust enough to support effective management and enable users to discover, visualise, and access data easily.\nThe presentation will showcase how TERN is developing spatial data infrastructure to support scalable and interoperable data services. The geospatial infrastructure utilises the GridFTP service provided by QRISCloud to transfer data from data providers to the Network File System (NFS) and subsequently to ARDC NeCTAR Swift storage in cloud-native file format, Cloud-optimised Geo Tiffs. After additional data validation, raster data will be published as an OGC-compliant Web Mapping Service and Web Coverage Service, accompanied by ISO 19115-3 compliant metadata that includes descriptions, citation information, platform and instrument details, methods, and additional information. In addition, the STAC catalog is created to enable users to discover, interrogate, visualise and easily access data on demand.\nThe cloud-native nature of this approach aims to accelerate scientific data discovery, foster interdisciplinary research, and drive innovation in ecosystem management and conservation efforts."
    },
    {
        "day": "Thursday",
        "time": "14:00 – 14:20",
        "location": "Boulevard B1",
        "session_chair": "Masoud Rahimi",
        "title": "Measurement-level Data Sharing Workflow – CHeBA Research Bank as a Use Case of Dementias Platform Australia (DPAU)",
        "authors": "Rory Chen",
        "url": "https://conference.eresearch.edu.au/measurement-level-data-sharing-workflow-cheba-research-bank-as-a-use-case-of-dementias-platform-australia-dpau/",
        "details": "Ms Rory Chen1, Dr Vibeke Catts1, Dr Darren Lipnicki1, Ms Naja Fragante1, Prof Perminder Sachdev1\n1Centre for Healthy Brain Ageing (CHeBA), UNSW Sydney, Australia\nRory joined the Centre for Healthy Brain Ageing (CHeBA) within the School of Psychiatry in January 2021. She is the manager of Dementias Platform Australia (DPAU).\nRory is a data scientist with an MSc in Statistics from UNSW and a BSc in Statistics from Beijing Normal University.\nShe previously worked on various clinical trials and observational studies at the National Drug and Alcohol Research Centre (NDARC) and the Chinese Center for Disease Control and Prevention (CDC).\nThe Dementias Platform Australia (DPAU), established by the Centre for Healthy Brain Ageing (CHeBA), aims to accelerate dementia research by enhancing the findability, accessibility, interoperability, and reusability (FAIR) of data. As part of this, we developed a semi-automated measurement-level data sharing workflow using Python, REDCap, and R.\nData from CHeBA’s longitudinal cohort studies: Sydney Memory and Ageing Study, Sydney Centenarian Study, and Older Australian Twins Study, have been shared on >500 occasions since 2010. The data cover 1000s of variables from >100 measurements in domains such as sociodemographic, cognition, medical history and lifestyle surveys. With the onboarding onto DPAU, opportunities for automation and efficiencies were explored.\nUsing Python, data files and variables are renamed following C-Surv Ontology, and metadata is extracted to create data dictionaries. REDCap is configured to capture study metadata and the project application process, including measurement-level data requests. Upon approval, Python extracts relevant data files for each project. R Shiny applications complement this through multiple dashboards tracking administrative workflows and provide tools for applicants to explore study metadata before submitting requests. R and Python are used to create data dictionaries and import data for REDCap.\nDetailed diagrams and real-time demonstration of the integrated workflow illustrate the reduced repetitive manual work, improved operational efficiency, streamlined reporting, and enhanced transparency of the workflow to the e-research community. The current system is highly customised and adaptation by other users may require significant effort, including a need for technical expertise. Additional work is required to ensure compliance and data security."
    },
    {
        "day": "Thursday",
        "time": "14:00 – 14:20",
        "location": "Boulevard B2",
        "session_chair": "Lesley Wyborn",
        "title": "Cybersecurity for RSEs: design patterns for eResearch web applications",
        "authors": "Sharon Tickell",
        "url": "https://conference.eresearch.edu.au/cybersecurity-for-rses-design-patterns-for-eresearch-web-applications/",
        "details": "Ms Sharon Tickell1\n1CSIRO, St Lucia, Australia\nSharon is a software engineer at CSIRO who has been involved in the design, deployment, and maintenance of several dozen eResearch web applications over the last decade and a half, from short-lived survey websites to the large and distributed eReefs web platform. She did not set out on her career with any intention of spending so much time on cybersecurity, but has had to learn some of the concepts and techniques presented in this talk out of necessity. She now hopes to help others avoid having to learning these the hard way.\nhttps://orcid.org/0000-0003-1223-9935\nAt some point in their careers, many Research Software Engineers (RSEs) will find themselves being asked to build, deploy and/or maintain a web application that delivers research data and information to other researchers or the public via the internet. Doing so safely requires a working knowledge of information technology security concepts and techniques that many of us were never trained in, or which may have changed drastically since our training days occurred.\nIn a world where cybersecurity is a career specialisation of its own and new threats emerge much faster than we can redesign research infrastructure, how can we ensure our research web applications are safe as well as functional? What difference does it make if we deploy to the cloud instead of to our own organisation’s intranet? Which bits of IT security are up to us, and when is it time to ask for help? What is all this security effort going to cost my project budget?\nI can’t promise easy answers, but this is an overview of some key concepts and techniques for mitigating cybersecurity risks at the design stage of eResearch web application developments, while still delivering on research goals."
    },
    {
        "day": "Thursday",
        "time": "14:00 – 14:20",
        "location": "Boulevard B3",
        "session_chair": "Jonathan Smillie",
        "title": "Building Australia’s Dataspace Ecosystem: Cloud-Driven Innovation for Trusted Data Sharing",
        "authors": "Robert Shen",
        "url": "https://conference.eresearch.edu.au/building-australias-dataspace-ecosystem-cloud-driven-innovation-for-trusted-data-sharing/",
        "details": "Dr Robert Shen1, Mr Kheeran Dharmawardena2, Mr Ben Chiu2, Mr Hamish Holewa2, Mr Jon Smillie2, Mr Muhammad Ali2, Mr Andrew White2, Professor Mark Easton1\n1RMIT University, Australia,2Australian Research Data Commons, Australia\nA research professional, Robert brings over 20 years of experience in higher education, digital innovation, and the research infrastructure sector. He received his PhD from the School of Information Technology, University of Sydney, in 2006. His career includes roles as a research fellow at the University of Melbourne, software lead at Australian National Data Service, and eResearch Director at Astronomy Australia. Since January 2022, Robert has been serving as the Director of RACE (RMIT AWS Cloud Supercomputing) at RMIT University.\nThe Australian Dataspaces Program, led by the ARDC in partnership with the International Data Spaces Association, is establishing infrastructure and pilot projects to adapt IDSA's secure, sovereign data-sharing standards for trusted data exchange between research, industry, and government in Australia.\nTo support ARDC’s efforts in creating a dataspace ecosystem, RMIT RACE (RMIT Advanced Cloud Ecosystem) is collaborating with ARDC to develop an Australian prototype of a dataspace testbed service, leveraging the IDSA testbed distribution. The Phase 1 of the project is expected to be completed in late 2025, with two objectives:\n1. Deploy an Australian prototype of an IDSA dataspace testbed service across multiple clouds (AWS and NeCTAR), providing capabilities for testing and demonstrating a dataspace ecosystem.\n2. Enhance the prototype by integrating Australian national research infrastructure like the Australian Access Federation (AAF), Research Data Australia (RDA), and Research Vocabularies Australia (RVA), enabling on-demand customised test dataspaces for specific research.\nThe anticipated key outcomes of the current project include:\n1. Lowering the barriers for Australian research infrastructure providers who want to establish dataspaces within Australia to test their ideas and prototypes.\n2. Establishing a demonstration capability that allows potential dataspace users and adopters to evaluate the benefits of a dataspace solution for their specific use-case.\n3. Enabling Australian dataspace operators to assess available dataspace technologies for fitness-for-purpose against their priorities.\nThis presentation will detail the advantages of adopting a multi-cloud-based dataspace testbed services and share the lessons we learned in enabling national dataspace ecosystem by leveraging cutting-edge cloud services."
    },
    {
        "day": "Thursday",
        "time": "14:20 – 14:40",
        "location": "Boulevard Auditorium",
        "session_chair": "Liz Stokes",
        "title": "Harmonizing Stable Isotope Data in Australia: The isotopes.au Platform for Enhanced Data Sharing and Collaboration",
        "authors": "Nina Welti, Lian Flick",
        "url": "https://conference.eresearch.edu.au/harmonizing-stable-isotope-data-in-australia-the-isotopes-au-platform-for-enhanced-data-sharing-and-collaboration/",
        "details": "Dr Nina Welti1, Lian Flick1, Regina Campbell1, Dr Jagoda Crawford3, Dr Geoff Fraser2, Steph Hawkins2, Dr Cath Hughes3, Dr Fabian Kohlmann5, Dr Wayne Noble5, Dr Yanfeng Shu1, Dr Tim Stobaus4, Dr Axel Suckow1, Dr Moritz Theile5, Dr Kathryn Waltenberg2\n1CSIRO, Australia,2Geoscience Australia, Australia,3Australian Nuclear Science and Technology Organisation, Australia,4National Measurement Institute, Australia,5Lithodat Pty Ltd, Australia\nDr. Nina Welti is a Senior Research Scientist within the Agriculture and Food Research Unit of CSIRO. As an environmental scientist and biogeochemist, Nina is driven by a passion for solving real-world environmental problems. Throughout her career, her research has focused on understanding and quantifying the impact of human choices and actions on ecosystem function. Her current work focuses on driving transparent and effective supply chain traceability in the agricultural industries. She leads research integrating stable isotope dynamics and agriculture management practices to develop data-driven evidence solutions.https://orcid.org/0000-0001-9966-5915\nLian Flick is a Technical Program Manager at CSIRO. She has over 20 years’ experience in bringing new technologies from concept to production in research and commercial organisations. Lian has extensive experience in managing large, multi-disciplinary, geographically dispersed teams, programs of work, and complex research and development projects. She is highly skilled in translating business requirements to digital applications to enable technology solutions for customers and internal operations. She has a proven ability to build strong relationships with partners and build consensus across multiple organisational levels and functional areas.https://orcid.org/0009-0001-8539-2319\nBackground\nThe goals of this effort were to create greater usability and availability of publicly held data, increase collaboration across research infrastructure, and realise greater value from public data. Environmental stable isotope data contributes to many disciplines and, as a result, has a high potential for re-use. Custodians of public data in Australia share a common purpose to ensure the quality, integrity, security, discoverability, accessibility, and useability of data assets.\nMethods\nThe key design criteria were a simple and easy to use tool that facilitates the harmonisation and integration of disparate stable isotope datasets while retaining data integrity and sovereignty. Using a standard flat-file structure, the ontology was co-developed by mapping each organisations data systems and identifying the common attributes.\nResults\nBy mapping from existing, disperate data systems the ontology design can be used across multiple science domains, whilst providing a defined scope of stable isotope information. The ontology uses a simple vocabulary, enabling federation of various data collections into a cohesive structure. A dedicated web-based platform, www.isotopes.au, applies this ontology, creating a single-point access to harmonised stable isotope data.\nConclusion\nThe ontology design creates a structured, formatted file that simplifies sharing data and facilitates data management. The platform and common ontology enable existing stable isotope datasets and repositories to become cohesive, interoperable and discoverable. This demonstrates how national science organisations can federate their data collections to promote interoperability, compatibility, and accessibility across multiple science domains and applications."
    },
    {
        "day": "Thursday",
        "time": "14:20 – 14:40",
        "location": "Boulevard B1",
        "session_chair": "Masoud Rahimi",
        "title": "Lost in Translation: Closing the Gap Between Knowledge and Impact",
        "authors": "Lost in Translation: Closing the Gap Between Knowledge and Impact",
        "url": "https://conference.eresearch.edu.au/lost-in-translation-closing-the-gap-between-knowledge-and-impact/",
        "details": "Dr Masoud Rahimi1, Dr Aiden Price2,1, Ms Pauline Fetaui3,4, Dr Paul Dalby5\n1Australian Urban Research Infrastructure Network (AURIN), Australia,2Centre for Data Science, Queensland University of Technology, Australia,3ACS Innovation Labs, Australia,4River City Labs, Australia,5Rozetta Institute, Australia\nMasoud is the Lead Data Scientist at AURIN, where he applies a logic-first, problem-driven approach to turning research into real-world impact. With expertise in data integration, analytics, and applied AI, he focuses on identifying industry and societal needs before designing targeted, scalable solutions. His work demonstrates how integrated digital infrastructure and data stewardship can accelerate evidence-based innovation.\nAustralia’s low Economic Complexity Index highlights a persistent national challenge: limited engagement in high-value, high-tech industries. Despite a globally competitive research sector, Australia struggles to convert knowledge into commercial and societal outcomes. Too many innovations stall at mid-stage Technology Readiness Levels (TRLs 4–6), hindered by misaligned incentives, restrictive intellectual property settings, underfunded translation pathways, and weak integration between research, industry, and government. This disconnect between discovery and deployment undermines Australia's ability to remain competitive in an increasingly knowledge-driven global economy.\neResearch, encompassing advanced digital infrastructure, data-intensive methodologies, in-silico experimentation, and AI, has the potential to reshape this landscape. With strategic investment and targeted coordination, eResearch can drive faster discovery, reduce time to translation, and enable innovation at scale. Yet, current capabilities in Australia remain fragmented and underpowered. Challenges include inconsistent access, lack of scalability, and limited focus on supporting end-to-end translation and commercialisation.\nThis Birds of a Feather session brings together voices from academia and industry to explore how technical and data professionals and digital infrastructure providers can help bridge the divide between research outputs and real-world impact. Rather than focusing on formal frameworks, we will explore practical, forward-facing questions: What enablers are essential for a research ecosystem that delivers real-world impact? What roles do data and digital professionals play in translation and innovation? What infrastructure, systems and partnerships are needed to support this system? What cultural or structural changes could foster deeper collaboration?\nParticipants will engage with real-world case studies, a 40-minute collaborative discussion, and a 20-minute co-design activity."
    },
    {
        "day": "Thursday",
        "time": "14:20 – 14:40",
        "location": "Boulevard B3",
        "session_chair": "Jonathan Smillie",
        "title": "They Are Amongst Us: Are Dataspaces Are Already Here?",
        "authors": "They Are Amongst Us: Are Dataspaces Are Already Here?",
        "url": "https://conference.eresearch.edu.au/they-are-amongst-us-are-dataspaces-are-already-here/",
        "details": "Mr Jonathan Smillie1, Dr Muhammad Ali1\n1ARDC, Australia\nJonathan has many years’ experience as a developer, systems analyst and technical lead in the eResearch sector. He now undertakes system and architecture analysis across the ARDC’s projects.\nThe Dataspaces paradigm provides a comprehensive model for secure and sovereign data sharing. The International Data Spaces Association (IDSA) Reference Architecture Model (RAM) is a conceptual framework which defines the roles, functions, interactions and system components required to implement a working dataspace. In this session we will compare the IDSA RAM with the Secure Analytics Framework for the Environment (SAFE) and the 5-SAFES framework and use these frameworks to identify existing national research infrastructure which is already implementing some, or all, of the features of Dataspaces.\nThis session is intended for designers, implementers, and users of data sharing ecosystems who are interested in how the Dataspaces paradigm may already be enhancing their data sharing activities. We will explore additional measures from the dataspaces paradigm that could be leveraged to further improve these activities, and how these have been realised by other active infrastructures.\nThe session will include an overview of how the IDSA RAM compares with the SAFE and 5-SAFES frameworks, and an exploration of the Dataspace related features of several existing national data sharing infrastructures. Participants in the session will then be invited to share the features and functions of infrastructures they are involved with, and as a group we will consider which of these are Dataspace compatible functions, and how they can collectively progress the development of a national Dataspaces ecosystem."
    },
    {
        "day": "Thursday",
        "time": "14:20 – 14:40",
        "location": "Boulevard Room",
        "session_chair": "Kyle Hemming",
        "title": "Rewriting the Rules: AI’s Impact on Digital Research Skills Training",
        "authors": "Rewriting the Rules: AI’s Impact on Digital Research Skills Training",
        "url": "https://conference.eresearch.edu.au/rewriting-the-rules-ais-impact-on-digital-research-skills-training/",
        "details": "Dr Kyle Hemming1, Dr Angus Fisk2, Mr Shaun Grady3, Dr Danny Meloncelli4, Dr Anastasios Papaioannou5, Mr Aidan Wilson6\n1University of Auckland, Auckland, Aotearoa / New Zealand,2University of Sydney, Sydney, Australia,3Hunter Medical Research Institute, New Castle, Australia,4Queensland Cyber Infrastructure Foundation, Brisbane, Australia,5University of Technology Sydney, Sydney, Australia,6Intersect Australia, Sydney, Country\nKyle Hemming supports researchers in data science, data management, and responsible AI adoption. With a decade of quantitative research experience and eight years supporting researchers, he is passionate about improving research outcomes. His interests also include reproducible research, strategic planning, and stakeholder engagement.\nAngus Fisk has a DPhil in Neuroscience, where he specialised in analysing sleep and circadian rhythms using EEG and behavioural data. His research involved building sensors, developing reproducible pipelines, and modelling complex daily rhythms. He now works as a data science trainer at the Sydney Informatics Hub, helping researchers build skills to be effective with modern tools including AI. He enjoys making complicated tools accessible and understandable, bringing data analysis pipelines into the future.\nShaun Grady is a Senior Health Informatician within the Data Sciences platform at Hunter Medical Research Institute and manages the Data Sciences Research Training Program. Shaun has been developing and delivering research training across Australia since 2019, and is now focusing these efforts on providing training opportunities for researchers in the Hunter New England area and regional NSW. Shaun is originally a Radiation Therapist by training with specific research interests in role development, clinical process improvement and utilisation of health data.\nDanny leads QCIF’s digital research training program, overseeing the development and delivery of high-impact courses that equip researchers with critical skills in coding, statistics, data management and bioinformatics. Danny has a PhD in Analytical Chemistry and a Master of Data Science. Prior to joining QCIF at the start of this year, Danny was a STEM Learning Adviser at UniSC for over a decade. Danny has a passion for simplifying complexity, up-skilling others, fostering a growth mindset, and harnessing the power of education to create lasting impact.\nAnastasios Papaioannou is a Senior Manager leading the eResearch Platforms and Services at UTS, where he oversees cloud and high-performance computing (HPC), research data storage, training, and the integration of AI in research. He actively works on the adoption of AI by collaborating with UTS colleagues to develop a comprehensive suite of AI guidelines, tools, and training programs for researchers. With a strong background in research, data science, and computational physics, he works closely with academics and HDR students to help them leverage large-scale infrastructure and digital technologies to accelerate their research.\nAidan Wilson is Intersect Australia’s Digital Research Services Manager, where he supervises a team of Digital Research Analysts, and manages Intersect’s researcher training program. Aidan has been heavily involved in training delivery and systems since he joined Intersect in 2015. Prior to joining Intersect, Aidan was a linguistic researcher working in the field of Australia’s Indigenous Languages, focusing in particular on the verb morphology of Traditional Tiwi, and other languages from the top-end, one of the most linguistically diverse regions on Earth.\nSituation\nArtificial intelligence (AI) tools are now widely available, and researchers are increasingly embedding them into research activities, from literature discovery to predictive modelling and interpreting findings. Researchers augmenting their practices with AI can realise benefits in speed, scope, and impact.\nTask\nHowever, significant challenges remain for the responsible uptake of AI. Key issues persist for researchers in understanding how AI systems work and their responsible use. For digital skills trainers, as AI tools evolve, researchers’ needs change, requiring training to adapt in content and delivery. For instance, if AI can generate code, what purpose does training ‘fundamental’ coding skills serve, and how should training practitioners respond? Should training be modified to accommodate AI-augmented practices or shift focus entirely to AI tools?\nAction\nThis Birds of a Feather (BOF) seeks to understand and address these concerns by asking the eResearch training community three questions: (1) what challenges does AI pose to current digital skills training? (2) what changes need to be made to current training? And (3) how should new training be delivered to meet evolving researcher needs?\nResult\nBy posing these questions, we hope to better prepare researcher organisations for an AI-augmented future. These insights will help guide the redevelopment and delivery style of the next generation of digital skills training across Australasia."
    },
    {
        "day": "Thursday",
        "time": "14:40 – 15:00",
        "location": "Boulevard Room",
        "session_chair": "Kyle Hemming",
        "title": "Raising the profile of in situ data within the Group on Earth Observations (GEO): it’s not all about satellite-based observations!",
        "authors": "Helen Glaves",
        "url": "https://conference.eresearch.edu.au/raising-the-profile-of-in-situ-data-within-the-group-on-earth-observations-geo-its-not-all-about-satellite-based-observations/",
        "details": "Ms Helen Glaves1, Joan Maso2, Leo Chiloane3, Paola de Salvo4\n1British Geological Survey, Nottingham, United Kingdom,2Centro de Investigación Ecológica y Aplicaciones Forestales (CREAF), Barcelona, Spain,3South African Environmental Observation Network (SAEON), Cape Town, South Africa,4Group on Earth Observations (GEO), Geneva, Switzerland\nHelen Glaves is a Senior Data Scientist at the British Geological Survey (BGS), with more than 30 years’ experience in marine geoscience and geoinformatics. Her current role focuses on the development and implementation of research infrastructures, which includes acting as Director of the Integrated Core Services (ICS-C) for the European Plate Observing System (EPOS).\nHelen Glaves is actively involved in a number of national and international initiatives addressing various aspects of open science and data stewardship, including acting as co-chair for the In Situ Data subgroup and a member of the Data and Knowledge Working Group within the Group on Earth Observations (GEO). She is also an editor for the American Geophysical Union (AGU) journal Earth & Space Science and chair of the AGU Charles S. Falkenberg Award committee.\nHelen has previously served as President of the European Geosciences Union (EGU) and as Division President for Earth and Space Science Informatics (ESSI). In 2016, she received EGU's Ian McHarg medal in recognition of her distinguished research in information technology applied to marine geoscience and, in particular, her contribution to advancing data sharing across different disciplines and organizational boundaries.\nIn situ data is vital for a range of applications such as calibration / validation of satellite-based Earth observations (EO), scientific research, and disaster response efforts. These data are usually direct, ground-based measurements made in specific and often fixed locations, which are likely to be more precise and therefore considered the “ground truth”. In contrast, satellite-based observations generally provide larger scale systematic coverage of the Earth’s surface. Together, in situ and remote sensing data form a complementary suite of observations and measurements that provide a comprehensive and detailed overview of the state of our planet.\nThe Group on Earth Observations (GEO), an intergovernmental organization addressing open sharing of data, information and knowledge, has for many years recognized the importance of in situ observations despite the widely held misconception that GEO only handles satellite-based EO data!\nTo emphasize the value of in situ data and address the challenges associated with the highly heterogeneous landscape of ground-based measurements, GEO is launching its Strategy for In Situ Data. This strategy aims to provide a framework for enhancing the sharing and reuse of in situ data, focusing on the full life cycle from acquisition to archiving. It underscores the need for collaboration from the local to the global scale alongside the adoption of common approaches, standards and best practices that are essential for integration, interoperability and reuse of in situ data across different scales, disciplines and domains. Through its In Situ Data Strategy, GEO aims to foster an approach delivering “Earth Intelligence for All!”"
    },
    {
        "day": "Thursday",
        "time": "15:00 – 15:20",
        "location": "Boulevard Room",
        "session_chair": "Kyle Hemming",
        "title": "ClimViewer: A Scalable, Interactive Platform forClimateand Weather Data Exploration",
        "authors": "Rui Yang",
        "url": "https://conference.eresearch.edu.au/climviewer-a-scalable-interactive-platform-for-climate-and-weather-data-exploration/",
        "details": "Dr Rui Yang1\n1National Computational Infrastructure, Canberra, Australia\nRui Yang is a Senior HPC Specialist at the National Computational Infrastructure (NCI), with expertise in data science, AI/ML, and high-performance computing. He specialises in optimising the performance of scientific models across climate, weather, and geoscience domains, and has extensive experience in benchmarking, profiling, and tuning workflows on large-scale HPC systems. Rui has led efforts to deploy and evaluate cutting-edge AI/ML models on HPC platforms, manage ML-ready datasets, and integrate AI techniques into scientific workflows to support domain-specific research and analysis.\nhttps://orcid.org/0009-0001-9698-5929\nThe increasing volume and complexity of high-resolution climate and weather datasets pose significant challenges in data discovery, access, and visualization. Local applications often offer powerful data processing capabilities but may lack interactivity and accessibility. Conversely, web-based platforms provide user-friendly interfaces but restrict users to predefined datasets with limited analytical flexibility. This divide highlights the need for a solution that combines the strengths of both approaches.\nWe present ClimViewer, a Python-based, scalable application for interactive climate and weather data analysis. ClimViewer integrates the Intake Earth System Model (Intake-ESM) framework to provide efficient, metadata-driven access to a wide range of datasets, including user-defined collections. By leveraging Xarray and Dask, it supports parallel processing of large-scale data and dynamic resource allocation, enabling fast image overlay rendering, statistical analysis, and animated visualizations.\nClimViewer features a web-based interface built with Voilà, turning Jupyter notebooks into interactive dashboards that require no coding experience. Users can explore 2D, 3D, and 4D datasets, apply temporal or vertical slicing, and generate location-specific time/level series plots directly through a browser.\nThe tool is integrated with the National Computational Infrastructure (NCI) Intake-ESM catalog, allowing seamless access to major datasets such as CMIP, ERA5, WeatherBench etc., while also supporting custom catalog files for user-specific datasets. This flexibility enhances discoverability, promotes wider data use, and streamlines research workflows.\nBy bridging the gap between local compute performance and interactive web accessibility, ClimViewer enables scalable, intuitive exploration and analysis of complex climate and weather data."
    },
    {
        "day": "Thursday",
        "time": "15:20 – 15:50",
        "location": "Boulevard Auditorium",
        "session_chair": "Liz Stokes",
        "title": "Afternoon Tea, Exhibition, Poster Viewing | Boulevard Foyer",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Thursday",
        "time": "15:50 – 16:30",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "Physical AI as the Next Frontier of Research and Industrial Digitization",
        "authors": "Tomasz Bednarz, Director of Strategic Researcher Engagement at NVIDIA Corporation",
        "url": "https://conference.eresearch.edu.au/physical-ai-as-the-next-frontier-of-research-and-industrial-digitization/",
        "details": "he world’s research is accelerating and everyone across academia and industry is racing to become software-defined, using the latest advancements in AI and simulation technologies. This talk will showcase examples how to leverage physical and generative AI, modern research infrastructure, open data frameworks, and advanced digital twins to unlock new possibilities across domains, from accelerated planning to a new era of agentic-AI powered autonomous operations, and more.\nTomasz is the Director of Strategic Researcher Engagement atNVIDIA Corporation. His team at NVIDIA builds active collaborations with leading researchers and premier research institutions that do compelling, computationally intense work to solve some of the world’s most challenging scientific problems.\nEarlier he was a Director and Head of Visualisation at the Expanded Perception & Interaction Centre (EPICentre) at theUNSW Art & DesignandUNSW Computer Science and Engineering. He was at the same time, a Research Team Leader atCSIRO’s Data61(leading Visual/Hybrid Analytics Team, in Software & Computational Systems research program). He also led Simulation and Modelling Cross-Cutting Capability for CSIRO’s Future Science and Technology.\nHis past roles reflect his conviction to a holistic approach to the wicked problems facing the collation, analytics and display of big data. His approach is expansive and encompasses the use of novel and emerging technologies. Over his research career, he was involved in wide range of projects in area of computational physics, immersive visualisation, human-computer interaction, computational imaging, image analysis and processing, visualisation, accelerated computing, simulation, modelling, computer graphics, computer games, computational fluid dynamics, machine learning, artificial intelligence, multi-sensors assimilation, etc.\nCurrently serving as a SIGGRAPH Asia Conference Advisory Group (SACAG) Chair, and is voting director ofACM SIGGRAPH Executive Committee. Earlier, he was Conference Chair ofACM SIGGRAPH Asia 2019(SIGGRAPH conferences are the world’s largest, most influential annual meetings and exhibitions in computer graphics and interactive techniques). Over last ten years, selected volunteering roles include Computer Animation Festival Jury (SA2004), Courses Chair (SA2017), Member of ACM SIGGRAPH International Resources Committee (2013-2017), Virtual Augmented and Mixed Reality Jury (S2018), Reviewer, Panellist, Unified Jury Committee (2017), BoF Sessions Organiser, Submitter and Presenter. In years 2022-27 he is serving asSIGGRAPH Asia Conference Advisory Group (SACAG)Chair, contributing to the future of SIGGRAPH Asia conferences and connecting global research community."
    },
    {
        "day": "Thursday",
        "time": "16:30 – 17:10",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "Critical Horizons for Australian eResearch",
        "authors": "Frankie Stevens",
        "url": "https://conference.eresearch.edu.au/critical-horizons-for-australian-eresearch/",
        "details": "Speakers: Heath Marks, Natasha Simons, Mark Stickell, Andrew Rohl.\nChair: Frankie Stevens\nHeath Marks was appointed by the Council of Australian University Directors of Information Technology (CAUDIT) in July 2009 to head a team to deliver the sustainable operations of Australia’s Trust and Identity services for Research and Education. This includes the national trust authentication framework the Australian Access Federation (AAF), and the Open Researcher and Contributor ID (ORCID) Consortium Lead for Australia. He is an IT professional with management experience in the successful delivery of transformational IT within the tertiary education and research sector supporting the National Collaborative Research Infrastructure Strategy (NCRIS).\nNatasha Simons is Director, National Coordination, for the Australian Research Data Commons (ARDC). She leads a large, talented team of Program Managers, Product Managers and Subject Matter Experts contributing to deliver ARDC’s strategic Research Data Commons initiatives. Natasha has a high international profile particularly in the area of persistent identifiers and serves on the Australian ORCID Steering Committee, the Research Data Alliance National PID Strategies Interest Group, and is on the Executive Board of DataCite. She plays a key role in the development of the Australian National PID Strategy and Roadmap.\nMark Stickells is a research executive with more than 20 years’ experience working at a senior level in innovative research and business development roles in complex, multi-stakeholder environments. Through national and international programs and joint-ventures, Mark had successfully led initiatives to accelerate the impact of research, development and education programs for Australia’s key energy, mining and agricultural sectors.\nAndrew Rohl is a leading expert in the application of supercomputing and computer simulation technologies in materials chemistry and leads the National Computational Infrastructure (NCI). He has a long history in the development and application of advanced computing technology in Australia through establishing the Pawsey Supercomputing Centre in 2012, the creation of the Curtin Institute for Data Science in 2015, and leading the ARC Training Centre for Transforming Maintenance through Data Science.\nWhat are the critical horizons for eResearch in Australia and how do we position ourselves to meet the challenges of an uncertain future. eResearch is essential for Australia’s smart economy, which aims to increase prosperity and productivity. It underpins transformative fields like AI, quantum technology, nanofabrication, personalised medicine, synthetic biology and the space sector, and promises significant economic growth and job opportunities. Australia’s world-class digital research infrastructure, led by NCRIS, is a source of global envy. It supports crucial breakthroughs in fundamental research, helps develop cutting-edge applied research, facilitates industry adoption of R&D, and provides sovereign capability to address new opportunities and challenges. Through increased efficiency, it boosts productivity through strategic investments connecting academia, research institutions, government, publicly funded agencies, and industry and supports a strengthening of Australia’s research and development ecosystem for a ‘Future Made in Australia’.What are the key challenges we need to tackle, and how to, in an uncertain environment to meet the critical horizon projects/needs?"
    },
    {
        "day": "Thursday",
        "time": "17:10 – 17:15",
        "location": "Boulevard Auditorium",
        "session_chair": "Luc Betbeder-Matibet & Natasha Simons",
        "title": "Conference CloseNatasha Simons, Luc Betbeder-Matibet, Co-Chairs, 2025 eResearch Australasia Conference",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Friday",
        "time": "08:30 – 13:00",
        "location": "N/A",
        "session_chair": "N/A",
        "title": "Registration | Boulevard 1-3 Concourse",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Friday",
        "time": "09:00 – 10:30",
        "location": "Boulevard B1",
        "session_chair": "N/A",
        "title": "Deep Dive: Hands-On Engineering with Google’s Gemini API and Gemini 2.5",
        "authors": "Mark McDonald, Google DeepMind",
        "url": "https://conference.eresearch.edu.au/deep-dive-hands-on-engineering-with-googles-gemini-api-and-gemini-2-5/",
        "details": "Mr Mark Mcdonald1\n1Google DeepMind, Australia\nMark is an Engineer & Advocate in Google DeepMind's Gemini Developer Relations team. He works on making Google's ML software platforms and APIs a smooth experience for all developers, from new to advanced.\nHe has worked on a range of Google products, including the Gemini API, PaLM API, TensorFlow, Google Maps and even Santa Tracker.\nThe rapid pace of generative AI presents transformative opportunities for research, but practical skills in using these powerful tools are often a barrier. This workshop aims to equip researchers with hands-on experience using Google's latest generative AI models to enhance their research workflows, from text generation and multi-modal data analysis to building interactive AI agents.\nThis half-day interactive workshop will guide participants from no-code prototyping through to writing complex apps for deep research, processing unstructured data and automating custom tool.\nStarting with the basics, attendees will use the Gemini Python SDK to progress through text generation, multi-turn chat, and advanced multi-modal capabilities including image, audio, video, and document processing. We'll also cover unstructured input, structured outputs, function calling with external APIs, native tools, and an introduction to the Model Context Protocol (MCP) for building agents using 3rd party tools.\nBy attending this workshop, attendees will get comfortable writing code with Google's Gemini 2.5 models for a wide range of tasks, including automated deep research, and will develop practical skills needed to use generative AI to their eResearch projects.\nThe workshop will use Python in a web browser (Colab/Jupyter notebooks), so it is suitable for anyone with a basic familiarity with Python."
    },
    {
        "day": "Friday",
        "time": "09:00 – 10:30",
        "location": "Boulevard B2",
        "session_chair": "N/A",
        "title": "Optimizing Scratch Filesystems for HPC and AI Workloads",
        "authors": "Christopher Schlipalius, Pawsey,Gin Tan, Monash UniversitySarah Walters,University of QueenslandGary Mancuso,Hitachi VantaraChris Hines,Monash University",
        "url": "https://conference.eresearch.edu.au/optimizing-scratch-filesystems-for-hpc-and-ai-workloads/",
        "details": "Mr Christopher Schlipalius1, Ms Gin Tan2, Ms Sarah Walters3, Mr Paul Hiew4, Mr Gary Mancuso5, Mr Chris Hines2\n1The Pawsey Supercomputing Centre, Perth, Australia,2Monash eResearch Centre, Melbourne, Australia,3University of Queensland, Brisbane, Australia,4NSCC, Singapore, Singapore,5Hitachi Vantara, Melbourne, Australia\nChris is an experienced presenter and Storage Manager with over 25 years of experience working and managing block storage, SANs, Tape and POSIX filesystems for large data holdings at The Curtin University of Technology and The Pawsey Supercomputing Centre.\nHe is a member of the SC25 Committee and has presented at and organised a number of workshops on Storage and Filesystems in Australia, USA, Germany and Singapore.\nHe also presented at the main Spectrum Scale Usergroup at SC18 on Improving Spark work load performance with Spectrum Conductor on Spectrum Scale (for scratch).\nHe ran the Versity User Group in Atlanta in 2025, the Spectrum Scale Usergroup in Singapore, and cities across Australia and was on the Spectrum Scale Usergroup worldwide organising committee and IBM Champion for 5 years.\nThis half-day technical workshop examines the types of scratch filesystems in supporting high-performance computing (HPC) and AI workloads.\nAttendees will gain insight into multiple scratch filesystem and storage technologies through a set of four real-world scenarios utilizing IO-intensive (challenging) HPC algorithms.\nVendors will describe their filesystem via IO 500 Benchmark, then show their results for four applications via a highlight of performance before and then after.\nHow the after result was achieved will be shown via demonstrations and led exercises.\nThe scratch technologies investigated include Lustre, DAOS, VAST, Weka, and GPFS.\nThe session will highlight strengths, use cases, and trade-offs of different technology and tuning and look at the approach of each vendor and technology in how it would address these cases and the needs of each use case.\nParticipants will walk away with a deeper understanding of scratch storage strategies aligned with domain-specific computing challenges.\nTarget Audience:\nHPC systems architects, storage administrators, AI infrastructure engineers, data scientists, and research computing professionals.\nPreparation description:\nThe plan is to provide 4 cases or such ahead of time to those demonstrating as well as the requirement to IO500 to baseline each test scratch system to frame the environment.\nThe four subject domain algorithm examples and data sets:\n1. Radio Astronomy (Pawsey reference data set example)\n2. Bioinformatics Cryo-EM or similar\n3. AI – e.g. LLM Training  –\n4. Popular choice – Forecasting model or genomic sequencing and analysis)\nThere will be co-presenters from companies and technical leaders to facilitate this workshop."
    },
    {
        "day": "Friday",
        "time": "10:30 – 11:00",
        "location": "Boulevard B1",
        "session_chair": "N/A",
        "title": "Morning Tea | Boulevard 1-3 Concourse",
        "authors": "N/A",
        "url": "N/A",
        "details": "N/A"
    },
    {
        "day": "Friday",
        "time": "11:00 – 12:30",
        "location": "Boulevard B1",
        "session_chair": "N/A",
        "title": "Deep Dive: Hands-On Engineering with Google’s Gemini API and Gemini 2.5 continued",
        "authors": "Mark McDonald, Google DeepMind",
        "url": "https://conference.eresearch.edu.au/deep-dive-hands-on-engineering-with-googles-gemini-api-and-gemini-2-5/",
        "details": "Mr Mark Mcdonald1\n1Google DeepMind, Australia\nMark is an Engineer & Advocate in Google DeepMind's Gemini Developer Relations team. He works on making Google's ML software platforms and APIs a smooth experience for all developers, from new to advanced.\nHe has worked on a range of Google products, including the Gemini API, PaLM API, TensorFlow, Google Maps and even Santa Tracker.\nThe rapid pace of generative AI presents transformative opportunities for research, but practical skills in using these powerful tools are often a barrier. This workshop aims to equip researchers with hands-on experience using Google's latest generative AI models to enhance their research workflows, from text generation and multi-modal data analysis to building interactive AI agents.\nThis half-day interactive workshop will guide participants from no-code prototyping through to writing complex apps for deep research, processing unstructured data and automating custom tool.\nStarting with the basics, attendees will use the Gemini Python SDK to progress through text generation, multi-turn chat, and advanced multi-modal capabilities including image, audio, video, and document processing. We'll also cover unstructured input, structured outputs, function calling with external APIs, native tools, and an introduction to the Model Context Protocol (MCP) for building agents using 3rd party tools.\nBy attending this workshop, attendees will get comfortable writing code with Google's Gemini 2.5 models for a wide range of tasks, including automated deep research, and will develop practical skills needed to use generative AI to their eResearch projects.\nThe workshop will use Python in a web browser (Colab/Jupyter notebooks), so it is suitable for anyone with a basic familiarity with Python."
    },
    {
        "day": "Friday",
        "time": "11:00 – 12:30",
        "location": "Boulevard B2",
        "session_chair": "N/A",
        "title": "Optimizing Scratch Filesystems for HPC and AI Workloads continued",
        "authors": "Christopher Schlipalius, Pawsey,Gin Tan, Monash UniversitySarah Walters,University of QueenslandGary Mancuso,Hitachi VantaraChris Hines,Monash University",
        "url": "https://conference.eresearch.edu.au/optimizing-scratch-filesystems-for-hpc-and-ai-workloads/",
        "details": "Mr Christopher Schlipalius1, Ms Gin Tan2, Ms Sarah Walters3, Mr Paul Hiew4, Mr Gary Mancuso5, Mr Chris Hines2\n1The Pawsey Supercomputing Centre, Perth, Australia,2Monash eResearch Centre, Melbourne, Australia,3University of Queensland, Brisbane, Australia,4NSCC, Singapore, Singapore,5Hitachi Vantara, Melbourne, Australia\nChris is an experienced presenter and Storage Manager with over 25 years of experience working and managing block storage, SANs, Tape and POSIX filesystems for large data holdings at The Curtin University of Technology and The Pawsey Supercomputing Centre.\nHe is a member of the SC25 Committee and has presented at and organised a number of workshops on Storage and Filesystems in Australia, USA, Germany and Singapore.\nHe also presented at the main Spectrum Scale Usergroup at SC18 on Improving Spark work load performance with Spectrum Conductor on Spectrum Scale (for scratch).\nHe ran the Versity User Group in Atlanta in 2025, the Spectrum Scale Usergroup in Singapore, and cities across Australia and was on the Spectrum Scale Usergroup worldwide organising committee and IBM Champion for 5 years.\nThis half-day technical workshop examines the types of scratch filesystems in supporting high-performance computing (HPC) and AI workloads.\nAttendees will gain insight into multiple scratch filesystem and storage technologies through a set of four real-world scenarios utilizing IO-intensive (challenging) HPC algorithms.\nVendors will describe their filesystem via IO 500 Benchmark, then show their results for four applications via a highlight of performance before and then after.\nHow the after result was achieved will be shown via demonstrations and led exercises.\nThe scratch technologies investigated include Lustre, DAOS, VAST, Weka, and GPFS.\nThe session will highlight strengths, use cases, and trade-offs of different technology and tuning and look at the approach of each vendor and technology in how it would address these cases and the needs of each use case.\nParticipants will walk away with a deeper understanding of scratch storage strategies aligned with domain-specific computing challenges.\nTarget Audience:\nHPC systems architects, storage administrators, AI infrastructure engineers, data scientists, and research computing professionals.\nPreparation description:\nThe plan is to provide 4 cases or such ahead of time to those demonstrating as well as the requirement to IO500 to baseline each test scratch system to frame the environment.\nThe four subject domain algorithm examples and data sets:\n1. Radio Astronomy (Pawsey reference data set example)\n2. Bioinformatics Cryo-EM or similar\n3. AI – e.g. LLM Training  –\n4. Popular choice – Forecasting model or genomic sequencing and analysis)\nThere will be co-presenters from companies and technical leaders to facilitate this workshop."
    }
]